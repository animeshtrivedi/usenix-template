%% LaTeX2e file `main.bib'
%% generated by the `filecontents' environment
%% from source `main' on 2019/03/18.
%%
%-------------------------------------------------------------------------------

@inproceedings{2023-asplos-tpp,
author = {Maruf, Hasan Al and Wang, Hao and Dhanotia, Abhishek and Weiner, Johannes and Agarwal, Niket and Bhattacharya, Pallab and Petersen, Chris and Chowdhury, Mosharaf and Kanaujia, Shobhit and Chauhan, Prakash},
title = {TPP: Transparent Page Placement for CXL-Enabled Tiered-Memory},
year = {2023},
isbn = {9781450399180},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582016.3582063},
doi = {10.1145/3582016.3582063},
abstract = {The increasing demand for memory in hyperscale applications has led to memory becoming a large portion of the overall datacenter spend. The emergence of coherent interfaces like CXL enables main memory expansion and offers an efficient solution to this problem. In such systems, the main memory can constitute different memory technologies with varied characteristics. In this paper, we characterize memory usage patterns of a wide range of datacenter applications across the server fleet of Meta. We, therefore, demonstrate the opportunities to offload colder pages to slower memory tiers for these applications. Without efficient memory management, however, such systems can significantly degrade performance. We propose a novel OS-level application-transparent page placement mechanism (TPP) for CXL-enabled memory. TPP employs a lightweight mechanism to identify and place hot/cold pages to appropriate memory tiers. It enables a proactive page demotion from local memory to CXL-Memory. This technique ensures a memory headroom for new page allocations that are often related to request processing and tend to be short-lived and hot. At the same time, TPP can promptly promote performance-critical hot pages trapped in the slow CXL-Memory to the fast local memory, while minimizing both sampling overhead and unnecessary migrations. TPP works transparently without any application-specific knowledge and can be deployed globally as a kernel release. We evaluate TPP with diverse memory-sensitive workloads in the production server fleet with early samples of new x86 CPUs with CXL 1.1 support. TPP makes a tiered memory system performant as an ideal baseline (&lt;1\% gap) that has all the memory in the local tier. It is 18\% better than today’s Linux, and 5–17\% better than existing solutions including NUMA Balancing and AutoTiering. Most of the TPP patches have been merged in the Linux v5.18 release while the remaining ones are just pending for more discussion.},
booktitle = {Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
pages = {742–755},
numpages = {14},
keywords = {CXL-Memory, Heterogeneous System, Operating Systems, Datacenters, Memory Management, Tiered-Memory},
location = {Vancouver, BC, Canada},
series = {ASPLOS 2023}
}

@inproceedings{2023-asplos-pond,
author = {Li, Huaicheng and Berger, Daniel S. and Hsu, Lisa and Ernst, Daniel and Zardoshti, Pantea and Novakovic, Stanko and Shah, Monish and Rajadnya, Samir and Lee, Scott and Agarwal, Ishwar and Hill, Mark D. and Fontoura, Marcus and Bianchini, Ricardo},
title = {Pond: CXL-Based Memory Pooling Systems for Cloud Platforms},
year = {2023},
isbn = {9781450399166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3575693.3578835},
doi = {10.1145/3575693.3578835},
abstract = {Public cloud providers seek to meet stringent performance requirements and low hardware cost. A key driver of performance and cost is main memory. Memory pooling promises to improve DRAM utilization and thereby reduce costs. However, pooling is challenging under cloud performance requirements. This paper proposes Pond, the first memory pooling system that both meets cloud performance goals and significantly reduces DRAM cost. Pond builds on the Compute Express Link (CXL) standard for load/store access to pool memory and two key insights. First, our analysis of cloud production traces shows that pooling across 8-16 sockets is enough to achieve most of the benefits. This enables a small-pool design with low access latency. Second, it is possible to create machine learning models that can accurately predict how much local and pool memory to allocate to a virtual machine (VM) to resemble same-NUMA-node memory performance. Our evaluation with 158 workloads shows that Pond reduces DRAM costs by 7\% with performance within 1-5\% of same-NUMA-node VM allocations.},
booktitle = {Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
pages = {574–587},
numpages = {14},
keywords = {CXL, datacenter, cloud computing, memory pooling, Compute Express Link, memory disaggregation},
location = {Vancouver, BC, Canada},
series = {ASPLOS 2023}
}

@inproceedings{2023-sigmod-dissg-db,
author = {Wang, Jianguo and Zhang, Qizhen},
title = {Disaggregated Database Systems},
year = {2023},
isbn = {9781450395076},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3555041.3589403},
doi = {10.1145/3555041.3589403},
abstract = {Disaggregated database systems achieve unprecedented excellence in elasticity and resource utilization at the cloud scale and have gained great momentum from both industry and academia recently. Such systems are developed in response to the emerging trend of disaggregated data centers where resources are physically separated and connected through fast data center networks. Database management systems have been traditionally built based on monolithic architectures, so disaggregation fundamentally challenges the designs. On the other hand, disaggregation offers benefits like independent scaling of compute, memory, and storage. Nonetheless, there is a lack of systematic investigation into new research challenges and opportunities in recent disaggregated database systems.To provide database researchers and practitioners with insights into different forms of resource disaggregation, we take a snapshot of state-of-the-art disaggregated database systems and related techniques and present an in-depth tutorial. The primary goal is to better understand the enabling techniques and characteristics of resource disaggregation and its implications for next-generation database systems. To that end, we survey recent work on storage disaggregation, which separates secondary storage devices (e.g., SSDs) from compute servers and is widely deployed in current cloud data centers, and memory disaggregation, which further splits compute and memory with Remote Direct Memory Access (RDMA) and is driving the transformation of clouds. In addition, we mention two techniques that bring novel perspectives to the above two paradigms: persistent memory and Compute Express Link (CXL). Finally, we identify several directions that shed light on the future development of disaggregated database systems.},
booktitle = {Companion of the 2023 International Conference on Management of Data},
pages = {37–44},
numpages = {8},
keywords = {memory disaggregation, databases, storage disaggregation},
location = {Seattle, WA, USA},
series = {SIGMOD '23}
}

@article{2023-sigops-mem-dissg-infiniswap,
author = {Al Maruf, Hasan and Chowdhury, Mosharaf},
title = {Memory Disaggregation: Advances and Open Challenges},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {1},
issn = {0163-5980},
url = {https://doi.org/10.1145/3606557.3606562},
doi = {10.1145/3606557.3606562},
abstract = {Compute and memory are tightly coupled within each server in traditional datacenters. Large-scale datacenter operators have identified this coupling as a root cause behind fleetwide resource underutilization and increasing Total Cost of Ownership (TCO). With the advent of ultra-fast networks and cache-coherent interfaces, memory disaggregation has emerged as a potential solution, whereby applications can leverage available memory even outside server boundaries.This paper summarizes the growing research landscape of memory disaggregation from a software perspective and introduces the challenges toward making it practical under current and future hardware trends. We also reflect on our seven-year journey in the SymbioticLab to build a comprehensive disaggregated memory system over ultra-fast networks. We conclude with some open challenges toward building next-generation memory disaggregation systems leveraging emerging cache-coherent interconnects.},
journal = {SIGOPS Oper. Syst. Rev.},
month = {jun},
pages = {29–37},
numpages = {9}
}

@article{2023-sigops-mem-dissg-vmware,
author = {Aguilera, Marcos K. and Amaro, Emmanuel and Amit, Nadav and Hunhoff, Erika and Yelam, Anil and Zellweger, Gerd},
title = {Memory Disaggregation: Why Now and What Are the Challenges},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {1},
issn = {0163-5980},
url = {https://doi.org/10.1145/3606557.3606563},
doi = {10.1145/3606557.3606563},
abstract = {Hardware disaggregation has emerged as one of the most fundamental shifts in how we build computer systems over the past decades. While disaggregation has been successful for several types of resources (storage, power, and others), memory disaggregation has yet to happen. We make the case that the time for memory disaggregation has arrived. We look at past successful disaggregation stories and learn that their success depended on two requirements: addressing a burning issue and being technically feasible. We examine memory disaggregation through this lens and find that both requirements are finally met. Once available, memory disaggregation will require software support to be used effectively. We discuss some of the challenges of designing an operating system that can utilize disaggregated memory for itself and its applications.},
journal = {SIGOPS Oper. Syst. Rev.},
month = {jun},
pages = {38–46},
numpages = {9}
}

@inproceedings{2023-cheops-iouring,
author = {Ren, Zebin and Trivedi, Animesh},
title = {Performance Characterization of Modern Storage Stacks: POSIX I/O, Libaio, SPDK, and Io_uring},
year = {2023},
isbn = {9798400700811},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3578353.3589545},
doi = {10.1145/3578353.3589545},
abstract = {Linux storage stack offers a variety of storage I/O stacks and APIs such as POSIX I/O, asynchronous I/O (libaio), high-performance asynchronous I/O (emerging io_uring) or SPDK, the last of which completely bypasses the kernel. Despite their availability, there has not been a systematic study of their performance and overheads. In order to aid our understanding, in this work we systematically characterize performance, scalability and microarchitectural properties of popular Linux I/O APIs on high-performance storage hardware (Intel Optane SSDs). Our characterization reveals that: (1) at low I/O loads, all APIs perform competitively with each other, with polling helping the performance by 1.7X, but consuming 2.3X CPU instructions; (2) at high-loads and scale, io_uring is more than an order of magnitude slower than SPDK; (3) at high-loads and scale, the benchmarking tool (fio) itself becomes a bottleneck; (4) state-of-practice Linux block I/O schedulers (BFQ, mq-deadline, and Kyber) introduce significant (up to 50\%) overheads, and their use of global locks hinder their scalability. All artifacts from this work are available at https://github.com/atlarge-research/Performance-Characterization-Storage-Stacks.},
booktitle = {Proceedings of the 3rd Workshop on Challenges and Opportunities of Efficient and Performant Storage Systems},
pages = {35–45},
numpages = {11},
keywords = {SPDK, io_uring, linux storage stack, measurements, efficiency},
location = {Rome, Italy},
series = {CHEOPS '23}
}

@article{2023-vldb-flash-1M-dbms,
author = {Haas, Gabriel and Leis, Viktor},
title = {What Modern NVMe Storage Can Do, and How to Exploit It: High-Performance I/O for High-Performance Storage Engines},
year = {2023},
issue_date = {May 2023},
publisher = {VLDB Endowment},
volume = {16},
number = {9},
issn = {2150-8097},
url = {https://doi.org/10.14778/3598581.3598584},
doi = {10.14778/3598581.3598584},
abstract = {NVMe SSDs based on flash are cheap and offer high throughput. Combining several of these devices into a single server enables 10 million I/O operations per second or more. Our experiments show that existing out-of-memory database systems and storage engines achieve only a fraction of this performance. In this work, we demonstrate that it is possible to close the performance gap between hardware and software through an I/O optimized storage engine design. In a heavy out-of-memory setting, where the dataset is 10 times larger than main memory, our system can achieve more than 1 million TPC-C transactions per second.},
journal = {Proc. VLDB Endow.},
month = {jul},
pages = {2090–2102},
numpages = {13}
}

@inproceedings{2022-cidr-append,
  author    = {Devashish R. Purandare and
               Peter Wilcox and
               Heiner Litz and
               Shel Finkelstein},
  title     = {Append is Near: Log-based Data Management on {ZNS} SSDs},
  booktitle = {12th Conference on Innovative Data Systems Research, {CIDR} 2022,
               Chaminade, CA, USA, January 9-12, 2022},
  publisher = {www.cidrdb.org},
  year      = {2022},
  url       = {https://www.cidrdb.org/cidr2022/papers/p93-purandare.pdf},
  timestamp = {Mon, 18 Jul 2022 17:13:00 +0200},
  biburl    = {https://dblp.org/rec/conf/cidr/PurandareWLF22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{2020-nvmsa-zns-implications,
  author    = {Hojin Shin and
               Myunghoon Oh and
               Gunhee Choi and
               Jongmoo Choi},
  title     = {Exploring Performance Characteristics of {ZNS} SSDs: Observation and
               Implication},
  booktitle = {9th Non-Volatile Memory Systems and Applications Symposium, {NVMSA}
               2020, Seoul, South Korea, August 19-21, 2020},
  pages     = {1--5},
  publisher = {{IEEE}},
  year      = {2020},
  url       = {https://doi.org/10.1109/NVMSA51238.2020.9188086},
  doi       = {10.1109/NVMSA51238.2020.9188086},
  timestamp = {Tue, 22 Sep 2020 12:18:03 +0200},
  biburl    = {https://dblp.org/rec/conf/nvmsa/ShinOCC20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{2021-atc-zns,
  author    = {Matias Bj{\o}rling and
               Abutalib Aghayev and
               Hans Holmberg and
               Aravind Ramesh and
               Damien Le Moal and
               Gregory R. Ganger and
               George Amvrosiadis},
  editor    = {Irina Calciu and
               Geoff Kuenning},
  title     = {{ZNS:} Avoiding the Block Interface Tax for Flash-based SSDs},
  booktitle = {2021 {USENIX} Annual Technical Conference, {USENIX} {ATC} 2021, July
               14-16, 2021},
  pages     = {689--703},
  publisher = {{USENIX} Association},
  year      = {2021},
  url       = {https://www.usenix.org/conference/atc21/presentation/bjorling},
  timestamp = {Thu, 12 Aug 2021 18:08:26 +0200},
  biburl    = {https://dblp.org/rec/conf/usenix/BjorlingAHRMGA21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{2021-sigcomm-gimbal,
author = {Min, Jaehong and Liu, Ming and Chugh, Tapan and Zhao, Chenxingyu and Wei, Andrew and Doh, In Hwan and Krishnamurthy, Arvind},
title = {Gimbal: Enabling Multi-Tenant Storage Disaggregation on SmartNIC JBOFs},
year = {2021},
isbn = {9781450383837},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3452296.3472940},
doi = {10.1145/3452296.3472940},
abstract = {Emerging SmartNIC-based disaggregated NVMe storage has become a promising storage infrastructure due to its competitive IO performance and low cost. These SmartNIC JBOFs are shared among multiple co-resident applications, and there is a need for the platform to ensure fairness, QoS, and high utilization. Unfortunately, given the limited computing capability of the SmartNICs and the non-deterministic nature of NVMe drives, it is challenging to provide such support on today's SmartNIC JBOFs.This paper presents Gimbal, a software storage switch that orchestrates IO traffic between Ethernet ports and NVMe drives for co-located tenants. It enables efficient multi-tenancy on SmartNIC JBOFs using the following techniques: a delay-based SSD congestion control algorithm, dynamic estimation of SSD write costs, a fair scheduler that operates at the granularity of a virtual slot, and an end-to-end credit-based flow control channel. Our prototyped system not only achieves up to x6.6 better utilization and 62.6\% less tail latency but also improves the fairness for complex workloads. It also improves a commercial key-value store performance in a multi-tenant environment with x1.7 better throughput and 35.0\% less tail latency on average.},
booktitle = {Proceedings of the 2021 ACM SIGCOMM 2021 Conference},
pages = {106–122},
numpages = {17},
keywords = {disaggregated storage, fairness, SSD, congestion control},
location = {Virtual Event, USA},
series = {SIGCOMM '21}
}

@inproceedings {2019-atc-bolted,
author = {Amin Mosayyebzadeh and Apoorve Mohan and Sahil Tikale and Mania Abdi and Nabil Schear and Trammell Hudson and Charles Munson and Larry Rudolph and Gene Cooperman and Peter Desnoyers and Orran Krieger},
title = {Supporting Security Sensitive Tenants in a {Bare-Metal} Cloud},
booktitle = {2019 USENIX Annual Technical Conference (USENIX ATC 19)},
year = {2019},
isbn = {978-1-939133-03-8},
address = {Renton, WA},
pages = {587--602},
url = {https://www.usenix.org/conference/atc19/presentation/mosayyebzadeh},
publisher = {USENIX Association},
month = jul,
}

@inproceedings{2021-hotos-skycomputing,
author = {Stoica, Ion and Shenker, Scott},
title = {From Cloud Computing to Sky Computing},
year = {2021},
isbn = {9781450384384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458336.3465301},
doi = {10.1145/3458336.3465301},
abstract = {We consider the future of cloud computing and ask how we might guide it towards a more coherent service we call sky computing. The barriers are more economic than technical, and we propose reciprocal peering as a key enabling step.},
booktitle = {Proceedings of the Workshop on Hot Topics in Operating Systems},
pages = {26–32},
numpages = {7},
location = {Ann Arbor, Michigan},
series = {HotOS '21}
}

@article{2019-berkeley-serverless,
  author    = {Eric Jonas and
               Johann Schleier{-}Smith and
               Vikram Sreekanti and
               Chia{-}che Tsai and
               Anurag Khandelwal and
               Qifan Pu and
               Vaishaal Shankar and
               Joao Carreira and
               Karl Krauth and
               Neeraja Jayant Yadwadkar and
               Joseph E. Gonzalez and
               Raluca Ada Popa and
               Ion Stoica and
               David A. Patterson},
  title     = {{Cloud Programming Simplified: {A} Berkeley View on Serverless Computing}},
  journal   = {CoRR},
  volume    = {abs/1902.03383},
  year      = {2019},
  url       = {http://arxiv.org/abs/1902.03383},
  archivePrefix = {arXiv},
  eprint    = {1902.03383},
  timestamp = {Tue, 21 May 2019 18:03:40 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1902-03383},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{2022-200zb,
  author = {Steve Morgan},  
  title        = {{The World Will Store 200 Zettabytes Of Data By 2025}},
  howpublished = {\nolinkurl{https://cybersecurityventures.com/the-world-will-store-200-zettabytes-of-data-by-2025/}},
  year = {June 2020},
  note  = {Accessed: 2022-Feb-02}  
 }
 
 
@misc{2020-vault-append,
  author = {Matias Bj{\o}rling},  
  title        = {{Zone Append: A New Way of Writing to Zoned Storage}},
  howpublished = {\nolinkurl{https://www.usenix.org/conference/vault20/presentation/bjorling}},
  year = {Feb 2020},
  note  = {Accessed: 2023-Jan-02}  
 }
 
 

@inproceedings{2022-sigcomm-luna-solar,
author = {Miao, Rui and Zhu, Lingjun and Ma, Shu and Qian, Kun and Zhuang, Shujun and Li, Bo and Cheng, Shuguang and Gao, Jiaqi and Zhuang, Yan and Zhang, Pengcheng and Liu, Rong and Shi, Chao and Fu, Binzhang and Zhu, Jiaji and Wu, Jiesheng and Cai, Dennis and Liu, Hongqiang Harry},
title = {From Luna to Solar: The Evolutions of the Compute-to-Storage Networks in Alibaba Cloud},
year = {2022},
isbn = {9781450394208},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544216.3544238},
doi = {10.1145/3544216.3544238},
abstract = {This paper presents the two generations of storage network stacks that reduced the average I/O latency of Alibaba Cloud's EBS service by 72\% in the last five years: Luna, a user-space TCP stack that corresponds the latency of network to the speed of SSD; and Solar, a storage-oriented UDP stack that enables both storage and network hardware accelerations.Luna is our first step towards a high-speed compute-to-storage network in the "storage disaggregation" architecture. Besides the tremendous performance gains and CPU savings compared with the legacy kernel TCP stack, more importantly, it teaches us the necessity of offloading both network and storage into hardware and the importance of recovering instantaneously from network failures.Solar provides a highly reliable and performant storage network running on hardware. For avoiding hardware's resource limitations and offloading storage's entire data path, Solar eliminates the superfluous complexity and the overfull states from the traditional architecture of the storage network. The core design of Solar is unifying the concepts of network packet and storage data block - each network packet is a self-contained storage data block. There are three remarkable advantages to doing so. First, it merges the packet processing and storage virtualization pipelines to bypass the CPU and PCIe; Second, since the storage processes data blocks independently, the packets in Solar become independent. Therefore, the storage (in hardware) does not need to maintain receiving buffers for assembling packets into blocks or handling packet reordering. Finally, due to the low resource requirement and the resilience to packet reordering, Solar inherently supports large-scale multi-path transport for fast failure recovery. Facing the future, Solar demonstrates that we can formalize the storage virtualization procedure into a P4-compatible packet processing pipeline. Hence, SOLAR's design perfectly applies to commodity DPUs (data processing units).},
booktitle = {Proceedings of the ACM SIGCOMM 2022 Conference},
pages = {753–766},
numpages = {14},
keywords = {storage network, in-network acceleration, data processing unit},
location = {Amsterdam, Netherlands},
series = {SIGCOMM '22}
}

@inproceedings{2015-asplos-de-vmm,
author = {Omote, Yushi and Shinagawa, Takahiro and Kato, Kazuhiko},
title = {Improving Agility and Elasticity in Bare-Metal Clouds},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2694344.2694349},
doi = {10.1145/2694344.2694349},
abstract = {Bare-metal clouds are an emerging infrastructure-as-a-service (IaaS) that leases physical machines (bare-metal instances) rather than virtual machines, allowing resource-intensive applications to have exclusive access to physical hardware. Unfortunately, bare-metal instances require time-consuming or OS-specific tasks for deployment due to the lack of virtualization layers, thereby sacrificing several beneficial features of traditional IaaS clouds such as agility, elasticity, and OS transparency. We present BMcast, an OS deployment system with a special-purpose de-virtualizable virtual machine monitor (VMM) that supports quick and OS-transparent startup of bare-metal instances. BMcast performs streaming OS deployment while allowing direct access to physical hardware from the guest OS, and then disappears after completing the deployment. Quick startup of instances improves agility and elasticity significantly, and OS transparency greatly simplifies management tasks for cloud customers. Experimental results have confirmed that BMcast initiated a bare-metal instance 8.6 times faster than image copying, and database performance on BMcast during streaming OS deployment was comparable to that on a state-of-the-art VMM without performing deployment. BMcast incurred zero overhead after de-virtualization.},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {145–159},
numpages = {15},
keywords = {bare-metal clouds, virtualization, device mediators, operating systems},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}

@INPROCEEDINGS{2018-fpl-fpga-nvme-fastpath,  author={Stratikopoulos, Athanasios and Kotselidis, Christos and Goodacre, John and Luján, Mikel},  booktitle={2018 28th International Conference on Field Programmable Logic and Applications (FPL)},   title={FastPath: Towards Wire-Speed NVMe SSDs},   year={2018},  volume={},  number={},  pages={170-1707},  doi={10.1109/FPL.2018.00036}}

@inproceedings{2021-asplos-corundum,
author = {Hoseinzadeh, Morteza and Swanson, Steven},
title = {Corundum: Statically-Enforced Persistent Memory Safety},
year = {2021},
isbn = {9781450383172},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3445814.3446710},
doi = {10.1145/3445814.3446710},
abstract = {Fast, byte-addressable, persistent main memories (PM) make it possible to build complex data structures that can survive system failures. Programming for PM is challenging, not least because it combines well-known programming challenges like locking, memory management, and pointer safety with novel PM-specific bug types. It also requires logging updates to PM to facilitate recovery after a crash. A misstep in any of these areas can corrupt data, leak resources, or prevent successful recovery after a crash. Existing PM libraries in a variety of languages -- C, C++, Java, Go -- simplify some of these problems, but they still require the programmer to learn (and flawlessly apply) complex rules to ensure correctness. Opportunities for data-destroying bugs abound. This paper presents Corundum, a Rust-based library with an idiomatic PM programming interface and leverages Rust’s type system to statically avoid most common PM programming bugs. Corundum lets programmers develop persistent data structures using familiar Rust constructs and have confidence that they will be free of those bugs. We have implemented Corundum and found its performance to be as good as or better than Intel's widely-used PMDK library, HP's Atlas, Mnemosyne, and go-pmem.},
booktitle = {Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {429–442},
numpages = {14},
keywords = {crash-consistent programming, static bug detection, non-volatile memory programming library},
location = {Virtual, USA},
series = {ASPLOS '21}
}

@article{2021-atc-twizzler,
author = {Bittman, Daniel and Alvaro, Peter and Mehra, Pankaj and Long, Darrell D. E. and Miller, Ethan L.},
title = {Twizzler: A Data-Centric OS for Non-Volatile Memory},
year = {2021},
issue_date = {May 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {2},
issn = {1553-3077},
url = {https://doi.org/10.1145/3454129},
doi = {10.1145/3454129},
abstract = {Byte-addressable, non-volatile memory (NVM) presents an opportunity to rethink the entire system stack. We present Twizzler, an operating system redesign for this near-future. Twizzler removes the kernel from the I/O path, provides programs with memory-style access to persistent data using small (64&nbsp;bit), object-relative cross-object pointers, and enables simple and efficient long-term sharing of data both between applications and between runs of an application. Twizzler provides a clean-slate programming model for persistent data, realizing the vision of Unix in a world of persistent RAM. We show that Twizzler is simpler, more extensible, and more secure than existing I/O models and implementations by building software for Twizzler and evaluating it on NVM DIMMs. Most persistent pointer operations in Twizzler impose less than 0.5&nbsp;ns added latency. Twizzler operations are up to faster than Unix, and SQLite queries are up to faster than on PMDK. YCSB workloads ran 1.1– faster on Twizzler than on native and NVM-optimized SQLite backends.},
journal = {ACM Trans. Storage},
month = {jun},
articleno = {11},
numpages = {31},
keywords = {Persistent memory, memory hierarchy, global address space, single-level store, NVM, PMEM, non-volatile memory}
}

@InProceedings{2004-cgo-llvm,
     Author  = {Chris Lattner and Vikram Adve},
     Title = {{LLVM}: A Compilation Framework for Lifelong Program Analysis and Transformation},
     Booktitle = CGO,
     Address = {San Jose, CA, USA},
     Month = {Mar},
     Year  = {2004},
     pages       = {75--88},
}

@inproceedings{2011-asplos-nvheap,
author = {Coburn, Joel and Caulfield, Adrian M. and Akel, Ameen and Grupp, Laura M. and Gupta, Rajesh K. and Jhala, Ranjit and Swanson, Steven},
title = {NV-Heaps: Making Persistent Objects Fast and Safe with next-Generation, Non-Volatile Memories},
year = {2011},
isbn = {9781450302661},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1950365.1950380},
doi = {10.1145/1950365.1950380},
abstract = {Persistent, user-defined objects present an attractive abstraction for working with non-volatile program state. However, the slow speed of persistent storage (i.e., disk) has restricted their design and limited their performance. Fast, byte-addressable, non-volatile technologies, such as phase change memory, will remove this constraint and allow programmers to build high-performance, persistent data structures in non-volatile storage that is almost as fast as DRAM. Creating these data structures requires a system that is lightweight enough to expose the performance of the underlying memories but also ensures safety in the presence of application and system failures by avoiding familiar bugs such as dangling pointers, multiple free()s, and locking errors. In addition, the system must prevent new types of hard-to-find pointer safety bugs that only arise with persistent objects. These bugs are especially dangerous since any corruption they cause will be permanent.We have implemented a lightweight, high-performance persistent object system called NV-heaps that provides transactional semantics while preventing these errors and providing a model for persistence that is easy to use and reason about. We implement search trees, hash tables, sparse graphs, and arrays using NV-heaps, BerkeleyDB, and Stasis. Our results show that NV-heap performance scales with thread count and that data structures implemented using NV-heaps out-perform BerkeleyDB and Stasis implementations by 32x and 244x, respectively, by avoiding the operating system and minimizing other software overheads. We also quantify the cost of enforcing the safety guarantees that NV-heaps provide and measure the costs of NV-heap primitive operations.},
booktitle = {Proceedings of the Sixteenth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {105–118},
numpages = {14},
keywords = {phase-change memory, memory mangement, non-volatile heap, pointer safety, spin-torque transfer memory, transactional memory, acid transactions, persistent objects},
location = {Newport Beach, California, USA},
series = {ASPLOS XVI}
}


@INPROCEEDINGS{2014-ieee-leap-mutiple-fpga,  author={Yang, Hsin Jung and Fleming, Kermin and Adler, Michael and Emer, Joel},  booktitle={2014 IEEE 22nd Annual International Symposium on Field-Programmable Custom Computing Machines},   title={LEAP Shared Memories: Automating the Construction of FPGA Coherent Memories},   year={2014},  volume={},  number={},  pages={117-124},  doi={10.1109/FCCM.2014.43}}

@inproceedings{2014-fpga-coram,
author = {Chung, Eric S. and Papamichael, Michael K. and Weisz, Gabriel and Hoe, James C. and Mai, Ken},
title = {Prototype and Evaluation of the CoRAM Memory Architecture for FPGA-Based Computing},
year = {2012},
isbn = {9781450311557},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2145694.2145717},
doi = {10.1145/2145694.2145717},
abstract = {The CoRAM memory architecture for FPGA-based computing augments traditional reconfigurable fabric with a natural and effective way for applications to interact with off-chip memory and I/O. The two central tenets of the CoRAM memory architecture are (1) the deliberate separation of concerns between computation versus data marshalling and (2) the use of a multithreaded software abstraction to replace FSM-based memory control logic. To evaluate the viability of the CoRAM memory architecture, we developed a full RTL implementation of a CoRAM microarchitecture instance that can be synthesized for standard cells or emulated on FPGAs. The results of our evaluation show that a soft emulation of the CoRAM memory architecture on current FPGAs can be impractical for memory-intensive, large-scale applications due to the high performance and area penalties incurred by the soft mechanisms. The results further show that in an envisioned FPGA built with CoRAM in mind, the introduction of hard macro blocks for data distribution can mitigate these inefficiencies---allowing applications to take advantage of the CoRAM memory architecture for ease of programmability and portability while still enjoying performance and efficiency comparable to RTL-level application development on conventional FPGAs.},
booktitle = {Proceedings of the ACM/SIGDA International Symposium on Field Programmable Gate Arrays},
pages = {139–142},
numpages = {4},
keywords = {memory architecture, fpga computing},
location = {Monterey, California, USA},
series = {FPGA '12}
}

@inproceedings{2011-fpga-leapscratch,
author = {Adler, Michael and Fleming, Kermin E. and Parashar, Angshuman and Pellauer, Michael and Emer, Joel},
title = {Leap Scratchpads: Automatic Memory and Cache Management for Reconfigurable Logic},
year = {2011},
isbn = {9781450305549},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1950413.1950421},
doi = {10.1145/1950413.1950421},
abstract = {Developers accelerating applications on FPGAs or other reconfigurable logic have nothing but raw memory devices in their standard toolkits. Each project typically includes tedious development of single-use memory management. Software developers expect a programming environment to include automatic memory management. Virtual memory provides the illusion of very large arrays and processor caches reduce access latency without explicit programmer instructions.LEAP scratchpads for reconfigurable logic dynamically allocate and manage multiple, independent, memory arrays in a large backing store. Scratchpad accesses are cached automatically in multiple levels, ranging from shared on-board, RAM-based, set-associative caches to private caches stored in FPGA RAM blocks. In the LEAP framework, scratchpads share the same interface as on-die RAM blocks and are plug-in replacements. Additional libraries support heap management within a storage set. Like software developers, accelerator authors using scratchpads may focus more on core algorithms and less on memory management.},
booktitle = {Proceedings of the 19th ACM/SIGDA International Symposium on Field Programmable Gate Arrays},
pages = {25–28},
numpages = {4},
keywords = {caches, fpga, memory management},
location = {Monterey, CA, USA},
series = {FPGA '11}
}

@article{1965-acm-segmentation,
author = {Dennis, Jack B.},
title = {Segmentation and the Design of Multiprogrammed Computer Systems},
year = {1965},
issue_date = {Oct. 1965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {4},
issn = {0004-5411},
url = {https://doi.org/10.1145/321296.321310},
doi = {10.1145/321296.321310},
journal = {J. ACM},
month = {oct},
pages = {589–602},
numpages = {14}
}

@article{1968-acm-os-segmentation-overlay,
author = {Pankhurst, R. J.},
title = {Operating Systems: Program Overlay Techniques},
year = {1968},
issue_date = {Feb. 1968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {0001-0782},
url = {https://doi.org/10.1145/362896.362923},
doi = {10.1145/362896.362923},
abstract = {The general features of program overlay systems are described. Three main types—automatic, semiautomatic and non-automatic—are classified, and the programming techniques are explained as a function of machine hardware and other system features. The implementation of a semiautomatic overlay facility in a multiprogrammed system on the CDC 6600 is described in detail, with special reference to real time applications.},
journal = {Commun. ACM},
month = {feb},
pages = {119–125},
numpages = {7},
keywords = {overlay techniques, loaders, storage allocation and segmentation, multiprogramming}
}


@inproceedings{2022-asplos-genstore,
author = {Mansouri Ghiasi, Nika and Park, Jisung and Mustafa, Harun and Kim, Jeremie and Olgun, Ataberk and Gollwitzer, Arvid and Senol Cali, Damla and Firtina, Can and Mao, Haiyu and Almadhoun Alserr, Nour and Ausavarungnirun, Rachata and Vijaykumar, Nandita and Alser, Mohammed and Mutlu, Onur},
title = {GenStore: A High-Performance in-Storage Processing System for Genome Sequence Analysis},
year = {2022},
isbn = {9781450392051},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503222.3507702},
doi = {10.1145/3503222.3507702},
abstract = {Read mapping is a fundamental step in many genomics applications. It is used to identify potential matches and differences between fragments (called reads) of a sequenced genome and an already known genome (called a reference genome). Read mapping is costly because it needs to perform approximate string matching (ASM) on large amounts of data. To address the computational challenges in genome analysis, many prior works propose various approaches such as accurate filters that select the reads within a dataset of genomic reads (called a read set) that must undergo expensive computation, efficient heuristics, and hardware acceleration. While effective at reducing the amount of expensive computation, all such approaches still require the costly movement of a large amount of data from storage to the rest of the system, which can significantly lower the end-to-end performance of read mapping in conventional and emerging genomics systems. We propose GenStore, the first in-storage processing system designed for genome sequence analysis that greatly reduces both data movement and computational overheads of genome sequence analysis by exploiting low-cost and accurate in-storage filters. GenStore leverages hardware/software co-design to address the challenges of in-storage processing, supporting reads with 1)&nbsp;different properties such as read lengths and error rates, which highly depend on the sequencing technology, and 2)&nbsp;different degrees of genetic variation compared to the reference genome, which highly depends on the genomes that are being compared. Through rigorous analysis of read mapping processes of reads with different properties and degrees of genetic variation, we meticulously design low-cost hardware accelerators and data/computation flows inside a NAND flash-based solid-state drive (SSD). Our evaluation using a wide range of real genomic datasets shows that GenStore, when implemented in three modern NAND flash-based SSDs, significantly improves the read mapping performance of state-of-the-art software (hardware) baselines by 2.07-6.05\texttimes{} (1.52-3.32\texttimes{}) for read sets with high similarity to the reference genome and 1.45-33.63\texttimes{} (2.70-19.2\texttimes{}) for read sets with low similarity to the reference genome.},
booktitle = {Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {635–654},
numpages = {20},
keywords = {Filtering, Storage, Read Mapping, Near-Data Processing, Genomics},
location = {Lausanne, Switzerland},
series = {ASPLOS '22}
}

@BOOK{Kheir1995-dh,
  title     = "Systems modeling and computer simulation, second edition",
  editor    = "Kheir, Naim A",
  publisher = "CRC Press",
  series    = "Electrical and Computer Engineering",
  edition   =  2,
  month     =  sep,
  year      =  1995,
  address   = "Boca Raton, FL"
}

@article{2020-nvm-survey,
  author    = {Haikun Liu and
               Di Chen and
               Hai Jin and
               Xiaofei Liao and
               Bingsheng He and
               Kan Hu and
               Yu Zhang},
  title     = {A Survey of Non-Volatile Main Memory Technologies: State-of-the-Arts,
               Practices, and Future Directions},
  journal   = {CoRR},
  volume    = {abs/2010.04406},
  year      = {2020},
  url       = {https://arxiv.org/abs/2010.04406},
  eprinttype = {arXiv},
  eprint    = {2010.04406},
  timestamp = {Wed, 14 Oct 2020 16:55:52 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2010-04406.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@book{book-open-science,
author = {Bezjak, Sonja and Clyburne-Sherin, April and Conzett, Philipp and Fernandes, Pedro and Görögh, Edit and Helbig, Kerstin and Kramer, Bianca and Labastida, Ignasi and Niemeyer, Kyle and Psomopoulos, Fotis and Ross-Hellauer, Tony and Schneider, René and Tennant, Jon and Verbakel, Ellen and Brinken, Helene and Heller, Lambert},
year = {2018},
month = {04},
pages = {},
title = {Open Science Training Handbook},
doi = {10.5281/zenodo.1212496}
}

@inproceedings{10.1109/CCGRID.2010.71,
author = {Kondo, Derrick and Javadi, Bahman and Iosup, Alexandru and Epema, Dick},
title = {The Failure Trace Archive: Enabling Comparative Analysis of Failures in Diverse Distributed Systems},
year = {2010},
isbn = {9780769540399},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/CCGRID.2010.71},
doi = {10.1109/CCGRID.2010.71},
abstract = {With the increasing functionality and complexity of distributed systems, resource failures are inevitable. While numerous models and algorithms for dealing with failures exist, the lack of public trace data sets and tools has prevented meaningful comparisons. To facilitate the design, validation, and comparison of fault-tolerant models and algorithms, we have created the Failure Trace Archive (FTA) as an online public repository of availability traces taken from diverse parallel and distributed systems. Our main contributions in this study are the following. First, we describe the design of the archive, in particular the rationale of the standard FTA format, and the design of a toolbox that facilitates automated analysis of trace data sets. Second, applying the toolbox, we present a uniform comparative analysis with statistics and models of failures in nine distributed systems. Third, we show how different interpretations of these data sets can result in different conclusions. This emphasizes the critical need for the public availability of trace data and methods for their analysis.},
booktitle = {Proceedings of the 2010 10th IEEE/ACM International Conference on Cluster, Cloud and Grid Computing},
pages = {398–407},
numpages = {10},
keywords = {reliability, distributed and parallel systems, fault-tolerance, failures},
series = {CCGRID '10}
}

@ARTICLE{9066946,  author={Versluis, Laurens and Mathá, Roland and Talluri, Sacheendra and Hegeman, Tim and Prodan, Radu and Deelman, Ewa and Iosup, Alexandru},  journal={IEEE Transactions on Parallel and Distributed Systems},   title={The Workflow Trace Archive: Open-Access Data From Public and Private Computing Infrastructures},   year={2020},  volume={31},  number={9},  pages={2170-2184},  doi={10.1109/TPDS.2020.2984821}}

@article{DBLP:journals/usenix-login/UtaLIMPC20,
  author    = {Alexandru Uta and
               Kristian Laursen and
               Alexandru Iosup and
               Paul Melis and
               Damian Podareanu and
               Valeriu Codreanu},
  title     = {Beneath the SURFace: An MRI-like View into the Life of a 21st-Century
               Datacenter},
  journal   = {login Usenix Mag.},
  volume    = {45},
  number    = {3},
  year      = {2020},
  url       = {https://www.usenix.org/publications/login/fall2020/uta},
  timestamp = {Fri, 02 Oct 2020 16:53:46 +0200},
  biburl    = {https://dblp.org/rec/journals/usenix-login/UtaLIMPC20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{2018-cacm-deeper-analysis,
author = {Ousterhout, John},
title = {Always Measure One Level Deeper},
year = {2018},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {61},
number = {7},
issn = {0001-0782},
url = {https://doi.org/10.1145/3213770},
doi = {10.1145/3213770},
abstract = {Performance measurements often go wrong, reporting surface-level results that are more marketing than science.},
journal = {Commun. ACM},
month = {jun},
pages = {74–83},
numpages = {10}
}

@misc{2022-bm-crimes,
  author = {Gernot Heiser},  
  title        = {{Systems Benchmarking Crimes}},
  howpublished = {\nolinkurl{https://gernot-heiser.org/benchmarking-crimes.html}},
  year = {2019},
  note         = {Accessed: 2022-Feb-02}
 }

@misc{2022-sm-energy,
  author = {SuperMicro},  
  title        = {{SuperStorage 1029P-NEL32R (Complete System Only) specification}},
  howpublished = {\nolinkurl{https://www.supermicro.com/en/products/system/1U/1029/SSG-1029P-NEL32R.cfm}},
  year = {2022},
  note         = {Accessed: 2022-Feb-02}
 } 
 
 
 
@misc{2022-camelyon,    
  title        = {{The CAMELYON17 challenge}},
  howpublished = {\nolinkurl{https://camelyon17.grand-challenge.org/}},
  year = {2022},
  note         = {Accessed: 2022-Feb-02}
 } 
 
@misc{2022-cxl,    
  title        = {{Compute Express Link (CXL)}},
  howpublished = {\nolinkurl{https://www.computeexpresslink.org/}},
  year = {2022},
  note         = {Accessed: 2022-Feb-02}
 } 


@BOOK{Jain1991-ey,
  title     = "The art of computer systems performance analysis",
  author    = "Jain, Raj",
  publisher = "John Wiley \& Sons",
  month     =  apr,
  year      =  1991,
  address   = "Nashville, TN",
  language  = "en"
}

@BOOK{Hamming2020-jk,
  title     = "The art of doing science and engineering the art of doing
               science and engineering",
  author    = "Hamming, Richard",
  publisher = "Stripe Press",
  month     =  sep,
  year      =  2020
}

@article{10.2753/MIS0742-1222240302,
author = {Peffers, Ken and Tuunanen, Tuure and Rothenberger, Marcus and Chatterjee, Samir},
title = {A Design Science Research Methodology for Information Systems Research},
year = {2007},
issue_date = {Number 3 / Winter 2007-2008},
publisher = {M. E. Sharpe, Inc.},
address = {USA},
volume = {24},
number = {3},
issn = {0742-1222},
url = {https://doi.org/10.2753/MIS0742-1222240302},
doi = {10.2753/MIS0742-1222240302},
abstract = {The paper motivates, presents, demonstrates in use, and evaluates a methodology for conducting design science (DS) research in information systems (IS). DS is of importance in a discipline oriented to the creation of successful artifacts. Several researchers have pioneered DS research in IS, yet over the past 15 years, little DS research has been done within the discipline. The lack of a methodology to serve as a commonly accepted framework for DS research and of a template for its presentation may have contributed to its slow adoption. The design science research methodology (DSRM) presented here incorporates principles, practices, and procedures required to carry out such research and meets three objectives: it is consistent with prior literature, it provides a nominal process model for doing DS research, and it provides a mental model for presenting and evaluating DS research in IS. The DS process includes six steps: problem identification and motivation, definition of the objectives for a solution, design and development, demonstration, evaluation, and communication. We demonstrate and evaluate the methodology by presenting four case studies in terms of the DSRM, including cases that present the design of a database to support health assessment methods, a software reuse measure, an Internet video telephony application, and an IS planning method. The designed methodology effectively satisfies the three objectives and has the potential to help aid the acceptance of DS research in the IS discipline.},
journal = {J. Manage. Inf. Syst.},
month = {dec},
pages = {45–77},
numpages = {33},
keywords = {Methodology, Design Science Research, Design Science, Process Model, Mental Model, Case Study, Design Theory}
}


@article{article-ls,
author = {Levy, Yair and Ellis, Timothy},
year = {2006},
month = {01},
pages = {},
title = {A Systems Approach to Conduct an Effective Literature Review in Support of Information Systems Research},
volume = {9},
journal = {International Journal of an Emerging Transdiscipline},
doi = {10.28945/479}
}

@misc{2022-dc-sheets,
  author = {{Dutch Data Center Association (DDA)}},  
  title        = {{Data Center Factsheet}},
  howpublished = {\nolinkurl{https://www.dutchdatacenters.nl/en/factsheet/}},
  year = {April 2022},
  note         = {Accessed: 2022-Feb-02}
 }
 
 @misc{2021-bsc-hongyu,
  doi = {10.48550/ARXIV.2108.01776},
  
  url = {https://arxiv.org/abs/2108.01776},
  
  author = {He, Hongyu},
  
  keywords = {Distributed, Parallel, and Cluster Computing (cs.DC), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {How Can Datacenters Join the Smart Grid to Address the Climate Crisis? Using simulation to explore power and cost effects of direct participation in the energy market},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{2022-ibm-as-400,
  author = {{IBM Corporation}},  
  title        = {{ AS/400 Machine Internal Functional Reference, Number SC41-5810-01}},  
  year = {1998},  
 }
 
@misc{2022-amd-pensando,
  author = {AMD},  
  title        = {{AMD Acquires Pensando}},
  howpublished = {\nolinkurl{https://www.amd.com/en/corporate/pensando-acquisition}},
  year = {April 2022},
  note         = {Accessed: 2022-Feb-02}
 }

@misc{2022-astron,
  author = {Astron},  
  title        = {{Institutes and NWO release 29 million for 
shared computing power for physicists and astronomers}},
  howpublished = {\nolinkurl{https://www.astron.nl/institutes-and-nwo-release-29-million-for-shared-computing-power-for-physicists-and-astronomers/}},
  year = {May 2020},
  note         = {Accessed: 2022-Feb-02}
 }

@misc{2022-intel-oneapi,
  author = {Intel}, 
  title        = {{oneAPI: A New Era of Heterogeneous Computing}},
  howpublished = {\nolinkurl{https://www.intel.com/content/www/us/en/developer/tools/oneapi/overview.html}},  
  note  = {Accessed: 2022-Feb-02}  
 }

@misc{2022-intel-roadmap2,
  title        = {{Intel's Process Roadmap to 2025: with 4nm, 3nm, 20A and 18A?!}},
  howpublished = {\nolinkurl{https://www.anandtech.com/show/16823/intel-accelerated-offensive-process-roadmap-updates-to-10nm-7nm-4nm-3nm-20a-18a-packaging-foundry-emib-foveros}},  
  note  = {Accessed: 2022-Feb-02}  
 }

@misc{2022-intel-roadmap,
author = {Intel},    
  title        = {{Intel Technology Roadmaps and Milestones}},
  howpublished = {\nolinkurl{https://www.intel.com/content/www/us/en/newsroom/news/intel-technology-roadmaps-milestones.html}},  
  note  = {Accessed: 2022-Feb-02}  
 }

@misc{2022-nvme-projection,    
  title        = {{Non-volatile Memory Express (NVMe) Market by Product (SSDs, Servers, All-flash Arrays, Adapters), Deployment Location (On-premise, Remote, Hybrid), Communication Standard (Ethernet, Fibre Channel, InfiniBand), Vertical, and Region - Global Forecast to 2025}},
  howpublished = {\nolinkurl{https://www.marketsandmarkets.com/Market-Reports/non-volatile-memory-express-market-30458978.html}},  
  note  = {Accessed: 2022-Feb-02}  
 }

@misc{2022-200zb,
  author = {Steve Morgan},  
  title        = {{The World Will Store 200 Zettabytes Of Data By 2025}},
  howpublished = {\nolinkurl{https://cybersecurityventures.com/the-world-will-store-200-zettabytes-of-data-by-2025/}},
  year = {June 2020},
  note  = {Accessed: 2022-Feb-02}  
 }

@misc{2022-daily-data,
  author = {Jason Wise},  
  title        = {{How much data is created everyday in 2022?}},
  howpublished = {\nolinkurl{https://earthweb.com/how-much-data-is-created-every-day/}},
  year = {June 2020},
  note  = {Accessed: 2022-Feb-02}  
 }

@incollection{2022-at-vu-comporg,
  author      = "Andrew S. Tanenbaum and Todd Austin",  
  booktitle   = "Structured Computer Organization",
  title       = "Computer Systems Organization (chpater 2)",
  publisher   = "Pearson",  
  year        = 2013,
}

 
@article{2018-cacm-gustavo-fpga,
author = {Alonso, Gustavo},
title = {FPGAs in Data Centers: FPGAs Are Slowly Leaving the Niche Space They Have Occupied for Decades.},
year = {2018},
issue_date = {March-April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {2},
issn = {1542-7730},
url = {https://doi-org.vu-nl.idm.oclc.org/10.1145/3212477.3231573},
doi = {10.1145/3212477.3231573},
abstract = {This installment of Research for Practice features a curated selection from Gustavo Alonso, who provides an overview of recent developments utilizing FPGAs (field-programmable gate arrays) in datacenters. As Moore’s Law has slowed and the computational overheads of datacenter workloads such as model serving and data processing have continued to rise, FPGAs offer an increasingly attractive point in the trade-off between power and performance. Gustavo’s selections highlight early successes and practical deployment considerations that inform the ongoing, high-stakes debate about the future of datacenter- and cloud-based computation substrates.},
journal = {Queue},
month = {apr},
pages = {52–57},
numpages = {6}
}


@inproceedings{2016-hotcloud-irr,
author = {\ulx{Animesh Trivedi} and Stuedi, Patrick and Pfefferle, Jonas and Stoica, Radu and Metzler, Bernard and Koltsidas, Ioannis and Ioannou, Nikolas},
title = {On the [Ir]Relevance of Network Performance for Data Processing},
year = {2016},
publisher = {USENIX Association},
address = {USA},
booktitle = {Proceedings of the 8th USENIX Conference on Hot Topics in Cloud Computing},
pages = {126–131},
numpages = {6},
location = {Denver, CO},
series = {HotCloud'16}
}

@inproceedings {2020-hotedge-griffin,
author = {\ulx{Animesh Trivedi} and Lin Wang and Henri Bal and Alexandru Iosup},
title = {Sharing and Caring of Data at the Edge},
booktitle = {3rd USENIX Workshop on Hot Topics in Edge Computing (HotEdge 20)},
year = {2020},
url = {https://www.usenix.org/conference/hotedge20/presentation/trivedi},
publisher = {USENIX Association},
month = jun,
}

@article{2016-ieee-das,
  author    = {Henri E. Bal and
               Dick H. J. Epema and
               Cees de Laat and
               Rob van Nieuwpoort and
               John W. Romein and
               Frank J. Seinstra and
               Cees Snoek and
               Harry A. G. Wijshoff},
  title     = {A Medium-Scale Distributed System for Computer Science Research: Infrastructure
               for the Long Term},
  journal   = {Computer},
  volume    = {49},
  number    = {5},
  pages     = {54--63},
  year      = {2016},
  url       = {https://doi.org/10.1109/MC.2016.127},
  doi       = {10.1109/MC.2016.127},
  timestamp = {Wed, 12 Aug 2020 10:28:54 +0200},
  biburl    = {https://dblp.org/rec/journals/computer/BalELNRSSW16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{2019-icdcs-ecosystems,
  author    = {Alexandru Iosup and
               Laurens Versluis and
               \ulx{Animesh Trivedi} and
               Erwin Van Eyk and
               Lucian Toader and
               Vincent van Beek and
               Giulia Frascaria and
               Ahmed Musaafir and
               Sacheendra Talluri},
  title     = {The AtLarge Vision on the Design of Distributed Systems and Ecosystems},
  booktitle = {39th {IEEE} International Conference on Distributed Computing Systems,
               {ICDCS} 2019, Dallas, TX, USA, July 7-10, 2019},
  pages     = {1765--1776},
  publisher = {{IEEE}},
  year      = {2019},
  url       = {https://doi.org/10.1109/ICDCS.2019.00175},
  doi       = {10.1109/ICDCS.2019.00175},
  timestamp = {Wed, 06 Nov 2019 12:26:31 +0100},
  biburl    = {https://dblp.org/rec/conf/icdcs/IosupVTETBFMT19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{2020-hotsotrage-ull-energy,
author = {Harris, Bryan and Altiparmak, Nihat},
title = {Ultra-Low Latency SSDs' Impact on Overall Energy Efficiency},
year = {2020},
publisher = {USENIX Association},
address = {USA},
abstract = {Recent technological advancements have enabled a generation of Ultra-Low Latency (ULL) SSDs that blurs the performance gap between primary and secondary storage devices. However, their power consumption characteristics are largely unknown. In addition, ULL performance in a block device is expected to put extra pressure on operating system components, significantly affecting energy efficiency of the entire system. In this work, we empirically study overall energy efficiency using a real ULL storage device, Optane SSD, a power meter, and a wide range of IO workload behaviors. We present a comparative analysis by laying out several critical observations related to idle vs. active behavior, read vs. write behavior, energy proportionality, impact on system software, as well as impact on overall energy efficiency. To the best of our knowledge, this is the first published study of a ULL SSD's impact on the system's overall power consumption, which can hopefully lead to future energy-efficient designs.},
booktitle = {Proceedings of the 12th USENIX Conference on Hot Topics in Storage and File Systems},
articleno = {2},
numpages = {1},
series = {HotStorage'20}
}

@inproceedings{2022-hotstorage-poll-energy,
author = {Harris, Bryan and Altiparmak, Nihat},
title = {When Poll is More Energy Efficient than Interrupt},
year = {2022},
isbn = {9781450393997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3538643.3539747},
doi = {10.1145/3538643.3539747},
abstract = {Polling is commonly indicated to be a more suitable IO completion mechanism than interrupt for ultra-low latency storage devices. However, polling's impact on overall energy efficiency has not been thoroughly investigated. In this paper, contrary to common belief, we show that polling can also be more energy efficient than interrupt. To do so, we systematically investigate the energy efficiency of all available Linux IO completion mechanisms, including interrupt, classic polling, and hybrid polling using a real ultra-low latency storage device, a power meter, and various workload behaviors. Our experimental results indicate that although hybrid polling provides a good trade-off in CPU utilization, it is the least energy efficient, whereas classic polling is the most energy efficient for low latency IO requests. To the best of our knowledge, this is the first paper classifying polling as more energy efficient than interrupt for a real secondary storage device, and we hope that our observations will lead to more energy efficient IO completion mechanisms for new generation storage device characteristics.},
booktitle = {Proceedings of the 14th ACM Workshop on Hot Topics in Storage and File Systems},
pages = {59–64},
numpages = {6},
keywords = {energy efficiency, IO completion},
location = {Virtual Event},
series = {HotStorage '22}
}

@inproceedings{2014-fast-mobile-energy-overheads,
author = {Li, Jing and Badam, Anirudh and Chandra, Ranveer and Swanson, Steven and Worthington, Bruce and Zhang, Qi},
title = {On the Energy Overhead of Mobile Storage Systems},
year = {2014},
isbn = {9781931971089},
publisher = {USENIX Association},
address = {USA},
abstract = {Secure digital cards and embedded multimedia cards are pervasively used as secondary storage devices in portable electronics, such as smartphones and tablets. These devices cost under 70 cents per gigabyte. They deliver more than 4000 random IOPS and 70 MBps of sequential access bandwidth. Additionally, they operate at a peak power lower than 250 milliwatts. However, software storage stack above the device level on most existing mobile platforms is not optimized to exploit the low-energy characteristics of such devices. This paper examines the energy consumption of the storage stack on mobile platforms.We conduct several experiments on mobile platforms to analyze the energy requirements of their respective storage stacks. Software storage stack consumes up to 200 times more energy when compared to storage hardware, and the security and privacy requirements of mobile apps are a major cause. A storage energy model for mobile platforms is proposed to help developers optimize the energy requirements of storage intensive applications. Finally, a few optimizations are proposed to reduce the energy consumption of storage systems on these platforms.},
booktitle = {Proceedings of the 12th USENIX Conference on File and Storage Technologies},
pages = {105–118},
numpages = {14},
location = {Santa Clara, CA},
series = {FAST'14}
}

@inproceedings{2017-hotstoage-more-energy,
author = {Mohan, Jayashree and Purohith, Dhathri and Halpern, Mathew and Chidambaram, Vijay and Reddi, Vijay Janapa},
title = {Storage on Your Smartphone Uses More Energy than You Think},
year = {2017},
publisher = {USENIX Association},
address = {USA},
abstract = {Energy consumption is a key concern for mobile devices. Prior research has focused on the screen and the network as the major sources of energy consumption. Through carefully designed measurement-based experiments, we show that for certain storage-intensive workloads, the storage subsystem on an Android smartphone consumes a significant amount of energy (36%), on par with screen energy consumption. We analyze the energy consumption of different storage primitives, such as sequential and random writes, on two popular mobile file systems, ext4 and F2FS. In addition, since most Android applications use SQLite for storage, we analyze the energy consumption of different SQLite operations. We present several interesting results from our analysis: for example, random writes consume 15\texttimes{} higher energy than sequential writes, and that F2FS consumes half the energy as ext4 for most workloads. We believe our results contribute useful design guidelines for the developers of energy-efficient mobile file systems.},
booktitle = {Proceedings of the 9th USENIX Conference on Hot Topics in Storage and File Systems},
pages = {9},
numpages = {1},
location = {Santa Clara, CA},
series = {HotStorage'17}
}

@article{2020-dutch-computing-manifesto,
  author    = {Alexandru Iosup and
               Fernando Kuipers and
               Ana Lucia Varbanescu and
               Paola Grosso and
               \ulx{Animesh Trivedi} and
               Jan S. Rellermeyer and
               Lin Wang and
               Alexandru Uta and
               Francesco Regazzoni},
  title     = {Future Computer Systems and Networking Research in the Netherlands:
               {A} Manifesto},
  journal   = {CoRR},
  volume    = {abs/2206.03259},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2206.03259},
  doi       = {10.48550/arXiv.2206.03259},
  eprinttype = {arXiv},
  eprint    = {2206.03259},
  timestamp = {Tue, 14 Jun 2022 16:41:43 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2206-03259.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{2022-itu-aging,
  author = {ITU},  
  title        = {{Digital technologies for healthy ageing}},
  howpublished = {\nolinkurl{https://www.itu.int/en/mediacentre/backgrounders/Pages/digital-technologies-for-healthy-ageing.aspx}},
  year = {May 2022},
  note         = {Accessed: 2022-Feb-02}
 }
 
@inproceedings{2021-asplos-dagger,
author = {Lazarev, Nikita and Xiang, Shaojie and Adit, Neil and Zhang, Zhiru and Delimitrou, Christina},
title = {Dagger: Efficient and Fast RPCs in Cloud Microservices with near-Memory Reconfigurable NICs},
year = {2021},
isbn = {9781450383172},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.vu-nl.idm.oclc.org/10.1145/3445814.3446696},
doi = {10.1145/3445814.3446696},
abstract = {The ongoing shift of cloud services from monolithic designs to mi- croservices creates high demand for efficient and high performance datacenter networking stacks, optimized for fine-grained work- loads. Commodity networking systems based on software stacks and peripheral NICs introduce high overheads when it comes to delivering small messages. We present Dagger, a hardware acceleration fabric for cloud RPCs based on FPGAs, where the accelerator is closely-coupled with the host processor over a configurable memory interconnect. The three key design principle of Dagger are: (1) offloading the entire RPC stack to an FPGA-based NIC, (2) leveraging memory interconnects instead of PCIe buses as the interface with the host CPU, and (3) making the acceleration fabric reconfigurable, so it can accommodate the diverse needs of microservices. We show that the combination of these principles significantly improves the efficiency and performance of cloud RPC systems while preserving their generality. Dagger achieves 1.3 − 3.8\texttimes{} higher per-core RPC throughput compared to both highly-optimized software stacks, and systems using specialized RDMA adapters. It also scales up to 84 Mrps with 8 threads on 4 CPU cores, while maintaining state-of- the-art µs-scale tail latency. We also demonstrate that large third- party applications, like memcached and MICA KVS, can be easily ported on Dagger with minimal changes to their codebase, bringing their median and tail KVS access latency down to 2.8 − 3.5 us and 5.4 − 7.8 us, respectively. Finally, we show that Dagger is beneficial for multi-tier end-to-end microservices with different threading models by evaluating it using an 8-tier application implementing a flight check-in service.},
booktitle = {Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {36–51},
numpages = {16},
keywords = {End-host networking, microservices, smartNICs, FPGAs, datacenters, cache-coherent FPGAs, cloud computing, RPC frameworks},
location = {Virtual, USA},
series = {ASPLOS '21}
}

@inproceedings{2014-sigcomm-coflow,
author = {Chowdhury, Mosharaf and Zhong, Yuan and Stoica, Ion},
title = {Efficient Coflow Scheduling with Varys},
year = {2014},
isbn = {9781450328364},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2619239.2626315},
doi = {10.1145/2619239.2626315},
abstract = {Communication in data-parallel applications often involves a collection of parallel flows. Traditional techniques to optimize flow-level metrics do not perform well in optimizing such collections, because the network is largely agnostic to application-level requirements. The recently proposed coflow abstraction bridges this gap and creates new opportunities for network scheduling. In this paper, we address inter-coflow scheduling for two different objectives: decreasing communication time of data-intensive jobs and guaranteeing predictable communication time. We introduce the concurrent open shop scheduling with coupled resources problem, analyze its complexity, and propose effective heuristics to optimize either objective. We present Varys, a system that enables data-intensive frameworks to use coflows and the proposed algorithms while maintaining high network utilization and guaranteeing starvation freedom. EC2 deployments and trace-driven simulations show that communication stages complete up to 3.16X faster on average and up to 2X more coflows meet their deadlines using Varys in comparison to per-flow mechanisms. Moreover, Varys outperforms non-preemptive coflow schedulers by more than 5X.},
booktitle = {Proceedings of the 2014 ACM Conference on SIGCOMM},
pages = {443–454},
numpages = {12},
keywords = {coflow, data-intensive applications, datacenter networks},
location = {Chicago, Illinois, USA},
series = {SIGCOMM '14}
}


@inproceedings{2012-hotnets-coflow,
author = {Chowdhury, Mosharaf and Stoica, Ion},
title = {Coflow: A Networking Abstraction for Cluster Applications},
year = {2012},
isbn = {9781450317764},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2390231.2390237},
doi = {10.1145/2390231.2390237},
abstract = {Cluster computing applications -- frameworks like MapReduce and user-facing applications like search platforms -- have application-level requirements and higher-level abstractions to express them. However, there exists no networking abstraction that can take advantage of the rich semantics readily available from these data parallel applications.We propose coflow, a networking abstraction to express the communication requirements of prevalent data parallel programming paradigms. Coflows make it easier for the applications to convey their communication semantics to the network, which in turn enables the network to better optimize common communication patterns.},
booktitle = {Proceedings of the 11th ACM Workshop on Hot Topics in Networks},
pages = {31–36},
numpages = {6},
keywords = {cloud computing, datacenter networks, coflow, cluster networking, data-intensive applications},
location = {Redmond, Washington},
series = {HotNets-XI}
}

@inproceedings{2022-asplos-molecule,
author = {Du, Dong and Liu, Qingyuan and Jiang, Xueqiang and Xia, Yubin and Zang, Binyu and Chen, Haibo},
title = {Serverless Computing on Heterogeneous Computers},
year = {2022},
isbn = {9781450392051},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.vu-nl.idm.oclc.org/10.1145/3503222.3507732},
doi = {10.1145/3503222.3507732},
abstract = {Existing serverless computing platforms are built upon homogeneous computers, limiting the function density and restricting serverless computing to limited scenarios. We introduce Molecule, the first serverless computing system utilizing heterogeneous computers. Molecule enables both general-purpose devices (e.g., Nvidia DPU) and domain-specific accelerators (e.g., FPGA and GPU) for serverless applications that significantly improve function density (50% higher) and application performance (up to 34.6x). To achieve these results, we first propose XPU-Shim, a distributed shim to bridge the gap between underlying multi-OS systems (when using general-purpose devices) and our serverless runtime (i.e., Molecule). We further introduce vectorized sandbox, a sandbox abstraction to abstract hardware heterogeneity (when using domain-specific accelerators). Moreover, we also review state-of-the-art serverless optimizations on startup and communication latency and overcome the challenges to implement them on heterogeneous computers. We have implemented Molecule on real platforms with Nvidia DPUs and Xilinx FPGAs and evaluate it using benchmarks and real-world applications.},
booktitle = {Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {797–813},
numpages = {17},
keywords = {function-as-a-service, heterogeneous computers, operating system, serverless computing, Cloud computing},
location = {Lausanne, Switzerland},
series = {ASPLOS '22}
}

@Incollection{2021-Dagstuhl-serverless,
author = {Samuel Kounev and Cristina Abad and Ian T. Foster and
Nikolas Herbst and Alexandru Iosup and Samer Al-Kiswany and
Ahmed Ali-Eldin Hassan and Bartosz Balis and Andr\'e Bauer and
Andr\'e B. Bondi and Kyle Chard and Ryan L. Chard and
Robert Chatley and Andrew A. Chien and A. Jesse Jiryu Davis and
Jesse Donkervliet and Simon Eismann and Erik Elmroth and
Nicola Ferrier and Hans-Arno Jacobsen and Pooyan Jamshidi and
Georgios Kousiouris and Philipp Leitner and Pedro Garcia Lopez and
Martina Maggio and Maciej Malawski and Bernard Metzler and
Vinod Muthusamy and Alessandro V. Papadopoulos and
Panos Patros and Guillaume Pierre and Omer F. Rana and
Robert P. Ricci and Joel Scheuner and Mina Sedaghat and
Mohammad Shahrad and Prashant Shenoy and Josef Spillner and
Davide Taibi and Douglas Thain and \ulx{Animesh Trivedi} and
Alexandru Uta and Vincent van Beek and Erwin van Eyk and
Andr\'e van Hoorn and Soam Vasani and Florian Wamser and
Guido Wirtz and Vladimir Yussupov},
title = {{Toward a Definition for Serverless Computing}},
booktitle = {{Serverless Computing (Dagstuhl Seminar 21201)}},
pages = {34--93},
journal = {Dagstuhl Reports},
year = {2021},
volume = {11},
issue = {4},
editor = {Cristina Abad and Ian T. Foster and Nikolas Herbst and
Alexandru Iosup},
publisher = {Schloss Dagstuhl -- Leibniz-Zentrum f{\"u}r Informatik},
address = {Dagstuhl, Germany},
doi = {10.4230/DagRep.11.4.34},
}

@inproceedings {2018-osdi-tvm,
author = {Tianqi Chen and Thierry Moreau and Ziheng Jiang and Lianmin Zheng and Eddie Yan and Haichen Shen and Meghan Cowan and Leyuan Wang and Yuwei Hu and Luis Ceze and Carlos Guestrin and Arvind Krishnamurthy},
title = {{TVM}: An Automated {End-to-End} Optimizing Compiler for Deep Learning},
booktitle = {13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18)},
year = {2018},
isbn = {978-1-939133-08-3},
address = {Carlsbad, CA},
pages = {578--594},
url = {https://www.usenix.org/conference/osdi18/presentation/chen},
publisher = {USENIX Association},
month = oct,
}

@inproceedings{2011-glacier,
author = {Teubner, Jens and Woods, Louis},
year = {2011},
month = {04},
pages = {738-741},
title = {Snowfall: Hardware Stream Analysis Made Easy}
}

@article{2022-acm-ipv6-roles,
author = {Piraux, Maxime and Barbette, Tom and Rybowski, Nicolas and Navarre, Louis and Alfroy, Thomas and Pelsser, Cristel and Michel, Fran\c{c}ois and Bonaventure, Olivier},
title = {The Multiple Roles That IPv6 Addresses Can Play in Today's Internet},
year = {2022},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {3},
issn = {0146-4833},
url = {https://doi-org.vu-nl.idm.oclc.org/10.1145/3561954.3561957},
doi = {10.1145/3561954.3561957},
abstract = {The Internet use IP addresses to identify and locate network interfaces of connected devices. IPv4 was introduced more than 40 years ago and specifies 32-bit addresses. As the Internet grew, available IPv4 addresses eventually became exhausted more than ten years ago. The IETF designed IPv6 with a much larger addressing space consisting of 128-bit addresses, pushing back the exhaustion problem much further in the future.In this paper, we argue that this large addressing space allows reconsidering how IP addresses are used and enables improving, simplifying and scaling the Internet. By revisiting the IPv6 addressing paradigm, we demonstrate that it opens up several research opportunities that can be investigated today. Hosts can benefit from several IPv6 addresses to improve their privacy, defeat network scanning, improve the use of several mobile access network and their mobility as well as to increase the performance of multicore servers. Network operators can solve the multihoming problem more efficiently and without putting a burden on the BGP RIB, implement Function Chaining with Segment Routing, differentiate routing inside and outside a domain given particular network metrics and offer more fine-grained multicast services.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = {sep},
pages = {10–18},
numpages = {9},
keywords = {multipath, network service, IP address, multihoming, IPv6}
}

@article{2016-acm-ipv6-migration,
author = {Nikkhah, Mehdi and Guerin, Roch and Nikkhah, Mehdi},
title = {Migrating the Internet to IPv6: An Exploration of the When and Why},
year = {2016},
issue_date = {August 2016},
publisher = {IEEE Press},
volume = {24},
number = {4},
issn = {1063-6692},
url = {https://doi-org.vu-nl.idm.oclc.org/10.1109/TNET.2015.2453338},
doi = {10.1109/TNET.2015.2453338},
abstract = {This paper documents and to some extent elucidates the progress of IPv6 across major Internet stakeholders since its introduction in the mid 1990s. IPv6 offered an early solution to a well-understood and well-documented problem IPv4 was expected to encounter. In spite of early standardization and awareness of the issue, the Internet's march to IPv6 has been anything but smooth, even if recent data point to an improvement. This paper documents this progression for several key Internet stakeholders using available measurement data, and identifies changes in the IPv6 ecosystem that may be in part responsible for how it has unfolded. The paper also develops a stylized model of IPv6 adoption across those stakeholders, and validates its qualitative predictive ability by comparing it to measurement data.},
journal = {IEEE/ACM Trans. Netw.},
month = {aug},
pages = {2291–2304},
numpages = {14}
}

@article{2019-acm-ipv6-tracking,
author = {Jia, Siyuan and Luckie, Matthew and Huffaker, Bradley and Elmokashfi, Ahmed and Aben, Emile and Claffy, Kimberly and Dhamdhere, Amogh},
title = {Tracking the Deployment of IPv6: Topology, Routing and Performance},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {165},
number = {C},
issn = {1389-1286},
url = {https://doi-org.vu-nl.idm.oclc.org/10.1016/j.comnet.2019.106947},
doi = {10.1016/j.comnet.2019.106947},
journal = {Comput. Netw.},
month = {dec},
numpages = {15},
keywords = {IPv6, Topology, Performance, BGP, Routing}
}

@techreport{ipv4,
  author = {Jon Postel},
  title = {Internet Protocol},
  howpublished = {Internet Requests for Comments},
  type = {STD},
  number = {5},
  year = {1981},
  month = {September},
  issn = {2070-1721},
  publisher = {RFC Editor},
  institution = {RFC Editor},
  url = {http://www.rfc-editor.org/rfc/rfc791.txt},
  note = {\nolinkurl{http://www.rfc-editor.org/rfc/rfc791.txt}},
}

@techreport{ipv6,
  author = {Stephen E. Deering and Robert M. Hinden},
  title = {Internet Protocol, Version 6 (IPv6) Specification},
  howpublished = {Internet Requests for Comments},
  type = {RFC},
  number = {2460},
  year = {1998},
  month = {December},
  issn = {2070-1721},
  publisher = {RFC Editor},
  institution = {RFC Editor},
  url = {http://www.rfc-editor.org/rfc/rfc2460.txt},
  note = {\nolinkurl{http://www.rfc-editor.org/rfc/rfc2460.txt}},
}

@misc{2022-bf-mackerel,
  author = {{Barrelfish project}},  
  title        = {{Mackerel User Guide}},
  howpublished = {\nolinkurl{https://barrelfish.org/publications/TN-002-Mackerel.pdf}},
  year = {2013},
  note         = {Accessed: 2022-Feb-02}
  } 
 
@misc{2022-nw-dds,  
  title        = {{Dutch Digitalization Strategy}},
  howpublished = {English translation of Nederlandse Digitaliseringsstrategie 2021 \nolinkurl{https://www.nederlanddigitaal.nl/binaries/nederlanddigitaal-nl/documenten/publicaties/2021/06/22/the-dutch-digitalisation-strategy-2021-eng/210621-min-ezk-digitaliseringstrategie-en-v03.pdf}},
  year = {April 2021},
  note         = {Accessed: 2022-Feb-02}
 }

@misc{2022-axbryd-hxdp,
  author = {{Axbryd}},
  title        = {{hXDP repo}},
  howpublished = {\nolinkurl{https://github.com/axbryd/hXDP-Artifacts}},
  note         = {Accessed: 2022-Feb-02}
 }
 
@article{2021-cacm-moores-law,
author = {Edwards, Chris},
title = {Moore's Law: What Comes Next?},
year = {2021},
issue_date = {February 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {64},
number = {2},
issn = {0001-0782},
url = {https://doi.org/10.1145/3440992},
doi = {10.1145/3440992},
abstract = {Moore's Law challenges point to changes in software.},
journal = {Commun. ACM},
month = {jan},
pages = {12–14},
numpages = {3}
}

@ARTICLE{2017-ieee-cmos-scaling,  author={Bohr, Mark T. and Young, Ian A.},  journal={IEEE Micro},   title={CMOS Scaling Trends and Beyond},   year={2017},  volume={37},  number={6},  pages={20-29},  doi={10.1109/MM.2017.4241347}}

@article{2016-cacm-exponential,
author = {Denning, Peter J. and Lewis, Ted G.},
title = {Exponential Laws of Computing Growth},
year = {2016},
issue_date = {January 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {60},
number = {1},
issn = {0001-0782},
url = {https://doi.org/10.1145/2976758},
doi = {10.1145/2976758},
abstract = {Moore's Law is one small component in an exponentially growing planetary computing ecosystem.},
journal = {Commun. ACM},
month = {dec},
pages = {54–65},
numpages = {12}
}

@article{2022-csur-fpga-programming,
author = {Del Sozzo, Emanuele and Conficconi, Davide and Zeni, Alberto and Salaris, Mirko and Sciuto, Donatella and Santambrogio, Marco D.},
title = {Pushing the Level of Abstraction of Digital System Design: A Survey on How to Program FPGAs},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {0360-0300},
url = {https://doi.org/10.1145/3532989},
doi = {10.1145/3532989},
abstract = {Field Programmable Gate Arrays (FPGAs) are spatial architectures with a heterogenous reconfigurable fabric. They are state-of-the-art for prototyping, telecommunications, embedded, and an emerging alternative for cloud-scale acceleration. However, FPGA adoption found limitations in their programmability and required knowledge. Therefore, researchers focused on FPGA abstractions and automation tools. Here, we survey three leading digital design abstractions: Hardware Description Languages (HDLs), High-Level Synthesis (HLS) tools, and Domain-Specific Languages (DSLs). We review these abstraction solutions, provide a timeline, and propose a taxonomy for each abstraction trend: programming models for HDLs; IP-based or System-based toolchains for HLS; application, architecture, and infrastructure domains for DSLs.},
note = {Just Accepted},
journal = {ACM Comput. Surv.},
month = {apr},
keywords = {Field Programmable Gate Array (FPGA), Digital Design, Hardware Description Languages (HDLs), High-Level Synthesis (HLS), Domain-Specific Languages (DSLs)}
}

@article{2021-csur-cmos,
author = {Kim, Heewoo and Amarnath, Aporva and Bagherzadeh, Javad and Talati, Nishil and Dreslinski, Ronald G.},
title = {A Survey Describing Beyond Si Transistors and Exploring Their Implications for Future Processors},
year = {2021},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {1550-4832},
url = {https://doi.org/10.1145/3453143},
doi = {10.1145/3453143},
abstract = {The advancement of Silicon CMOS technology has led information technology innovation for decades. However, scaling transistors down according to Moore’s law is almost reaching its limitations. To improve system performance, cost, and energy efficiency, vertical-optimization in multiple layers of the computing stack is required. Technological awareness in terms of devices and circuits could enable informed system-level decisions. For example, graphene is a promising material for extremely scaled high-speed transistors because of its remarkably high mobility, but it can not be used in integrated circuits as a result of the high leakage current from its zero bandgap. In this article, we discuss the fundamental physics of transistors and their ramifications on system design to assist device-level technology consideration during system design. Additionally, various emerging devices and their utilization on a vertically-optimized computing stack are introduced. This article serves as a survey of emerging device technologies that may be relevant in these areas, with an emphasis on making the descriptions approachable by system and software designers to understand the potential solutions. A basic vocabulary will be built to understand how to digest technical content, followed by a survey of devices, and finally a discussion of the implications for future processing systems.},
journal = {J. Emerg. Technol. Comput. Syst.},
month = {jun},
articleno = {27},
numpages = {44},
keywords = {NCFET, carbon-based devices, emerging devices, MoS 2, TFET, Beyond Moore, SiC, emerging non-volatile memory, III–V, Ge nanowire, system design}
}

@misc{2022-donard,
  author = {Stephen Bates},
  title        = {{Project Donard: NVM Express for Peer-2-Peer between SSDs and other PCIe Devices}},
  howpublished = {\nolinkurl{https://www.snia.org/sites/default/files/SDC15_presentations/nvme_fab/StephenBates_Donard_NVM_Express_Peer-2_Peer.pdf}},
  note         = {Accessed: 2022-Feb-02}
 }

@misc{2022-bluefield,
  author = {},
  title        = {{Mellanox BlueField SmartNIC for Ethernet}},
  howpublished = {\nolinkurl{https://www.mellanox.com/files/doc-2020/pb-bluefield-smart-nic.pdf}},
  note         = {Accessed: 2022-Feb-02}
 }

@misc{2020-arrow,
  title        = {{Apache Arrow: A cross-language development platform for in-memory data}},
  author       = {},
  howpublished = {\nolinkurl{https://arrow.apache.org/}},
  note         = {Accessed: 2020-02-15}
}

@inproceedings{2021-isca-pb-google-riscv,
author = {Karandikar, Sagar and Leary, Chris and Kennelly, Chris and Zhao, Jerry and Parimi, Dinesh and Nikolic, Borivoje and Asanovic, Krste and Ranganathan, Parthasarathy},
title = {A Hardware Accelerator for Protocol Buffers},
year = {2021},
isbn = {9781450385572},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3466752.3480051},
doi = {10.1145/3466752.3480051},
booktitle = {MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {462–478},
numpages = {17},
keywords = {deserialization, serialization, hardware-acceleration, hyperscale systems, warehouse-scale computing, profiling},
location = {Virtual Event, Greece},
series = {MICRO '21}
}

@MISC{2022-protocol-buffers,
  title={Protocol Buffers},
  author={Google},
  howpublished={\nolinkurl{https://developers.google.com/protocol-buffers/}},
  note         = {Accessed: 2022-02-15}
}

@misc{2020-plasma,
  title        = {{Plasma: A High-Performance Shared-Memory Object Store}},
  author       = {Philipp Moritz and Robert Nishihara},
  howpublished = {\nolinkurl{https://arrow.apache.org/blog/2017/08/08/plasma-in-memory-object-store/}},
  note         = {Accessed: 2020-02-15}
}

@misc{2020-cisco-efm,
  title        = {{Kinetic Edge and Fog Processing Module (EFM)}},
  author       = {Cisco},
  howpublished = {\nolinkurl{https://www.cisco.com/c/dam/en/us/solutions/collateral/internet-of-things/kinetic-datasheet-efm.pdf}},
  note         = {Accessed: 2020-02-15}
}

@misc{2020-dell-edge-gateways,
  title        = {{Edge Gateway 3003} \mbox{Specification}},
  author       = {Dell},
  howpublished = {\nolinkurl{https://topics-cdn.dell.com/pdf/dell-edge-gateway-3000-series_Specifications3_en-us.pdf}},
  note         = {Accessed: 2020-02-15}
}

@misc{2020-jc-latency,
  title        = {{Latency mitigation strategies}},
  author       = {John Carmack},
  howpublished = {\nolinkurl{https://danluu.com/latency-mitigation/} (mirror)},
  note         = {Accessed: 2020-02-15}
}
@misc{2020-aws-storage,
  title        = {{Overview of Amazon Web Services}},
  author       = {Amazon},
  howpublished = {\nolinkurl{https://docs.aws.amazon.com/whitepapers/latest/aws-overview/storage-services.html}},
  note         = {Accessed: 2020-02-15}
}

@misc{2020-azure-sqledge,
  title        = {{Azure SQL Database Edge (Preview): Small-footprint, edge-optimized data engine with built-in AI}},
  author       = {Microsoft},
  howpublished = {\nolinkurl{https://azure.microsoft.com/en-us/services/sql-database-edge/}},
  note         = {Accessed: 2020-02-15}
}

@misc{2020-aws-db,
  title        = {{ Databases on AWS}},
  author       = {Amazon},
  howpublished = {\nolinkurl{https://aws.amazon.com/products/databases/?nc2=h_ql_prod_db}},
  note         = {Accessed: 2020-02-15}
}

@misc{2020-gcp-storage,
  title        = {{Cloud storage products}},
  author       = {Google},
  howpublished = {\nolinkurl{https://cloud.google.com/products/storage}},
  note         = {Accessed: 2020-02-15}
}

@misc{2020-gcp-db,
  title        = {{Google Cloud Databases}},
  author       = {Google},
  howpublished = {\nolinkurl{https://cloud.google.com/products/databases}},
  note         = {Accessed: 2020-02-15}
}

@misc{2020-azure-stodb,
  title        = {{Azure products : Datbases and Storage categories}},
  author       = {Microsoft},
  howpublished = {\nolinkurl{https://azure.microsoft.com/en-us/services/}},
  note         = {Accessed: 2020-02-15}
}

@misc{2020-etsi-model,
  title        = {{Multi-access Edge Computing (MEC): Framework and Reference Architecture}},
  author       = {ETSI},
  year         = {2019},
  howpublished = {\nolinkurl{https://www.etsi.org/deliver/etsi_gs/MEC/001_099/003/02.01.01_60/gs_MEC003v020101p.pdf}},
  note         = {Accessed: 2020-02-15}
}

@misc{2020-aws-iot-edge,
  title        = {{AWS IoT for the Edge}},
  author       = {Amazon},
  howpublished = {\nolinkurl{https://aws.amazon.com/iot/solutions/iot-edge/}},
  note         = {Accessed: 2020-02-15}
}

@misc{2020-azure-iot,
  title        = {{Azure IoT Edge}},
  author       = {Microsoft},
  howpublished = {\nolinkurl{https://azure.microsoft.com/en-us/services/iot-edge/}},
  note         = {Accessed: 2020-02-15}
}

@misc{2020-google-iot,
  title        = {{Google Cloud IoT}},
  author       = {Google},
  howpublished = {\nolinkurl{https://cloud.google.com/solutions/iot}},
  note         = {Accessed: 2020-02-15}
}

@misc{2020-apache-cql,
  title        = {{The Cassandra Query Language (CQL)}},
  author       = {Apache},
  howpublished = {\nolinkurl{http://cassandra.apache.org/doc/latest/cql/}},
  note         = {Accessed: 2020-02-15}
}

@article{2019-ebpf-iptables,
author = {Miano, Sebastiano and Bertrone, Matteo and Risso, Fulvio and Bernal, Mauricio V\'{a}squez and Lu, Yunsong and Pi, Jianwen},
title = {Securing Linux with a Faster and Scalable Iptables},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {3},
issn = {0146-4833},
url = {https://doi.org/10.1145/3371927.3371929},
doi = {10.1145/3371927.3371929},
journal = {SIGCOMM Comput. Commun. Rev.},
month = {nov},
pages = {2–17},
numpages = {16},
keywords = {eBPF, XDP, Linux, iptables}
}

@misc{opentsdb,
  title        = {{The Scalable Time Series Database}},
  howpublished = {\nolinkurl{http://opentsdb.net}},
  note         = {Accessed: 2020-05-05}
}

@inproceedings{2020-asplos-lynx,
author = {Tork, Maroun and Maudlej, Lina and Silberstein, Mark},
title = {Lynx: A SmartNIC-Driven Accelerator-Centric Architecture for Network Servers},
year = {2020},
isbn = {9781450371025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3373376.3378528},
doi = {10.1145/3373376.3378528},
abstract = {This paper explores new opportunities afforded by the growing deployment of compute and I/O accelerators to improve the performance and efficiency of hardware-accelerated computing services in data centers.We propose Lynx, an accelerator-centric network server architecture that offloads the server data and control planes to the SmartNIC, and enables direct networking from accelerators via a lightweight hardware-friendly I/O mechanism. Lynx enables the design of hardware-accelerated network servers that run without CPU involvement, freeing CPU cores and improving performance isolation for accelerated services. It is portable across accelerator architectures and allows the management of both local and remote accelerators, seamlessly scaling beyond a single physical machine.We implement and evaluate Lynx on GPUs and the Intel Visual Compute Accelerator, as well as two SmartNIC architectures - one with an FPGA, and another with an 8-core ARM processor. Compared to a traditional host-centric approach, Lynx achieves over 4X higher throughput for a GPU-centric face verification server, where it is used for GPU communications with an external database, and 25% higher throughput for a GPU-accelerated neural network inference service. For this workload, we show that a single SmartNIC may drive 4 local and 8 remote GPUs while achieving linear performance scaling without using the host CPU.},
booktitle = {Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {117–131},
numpages = {15},
keywords = {i/o services for accelerators, hardware accelerators, server architecture, operating systems, smartnics},
location = {Lausanne, Switzerland},
series = {ASPLOS '20}
}

@article{2020-cacm-dsa,
author = {Dally, William J. and Turakhia, Yatish and Han, Song},
title = {Domain-Specific Hardware Accelerators},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {63},
number = {7},
issn = {0001-0782},
url = {https://doi.org/10.1145/3361682},
doi = {10.1145/3361682},
abstract = {DSAs gain efficiency from specialization and performance from parallelism.},
journal = {Commun. ACM},
month = {jun},
pages = {48–57},
numpages = {10}
}

@article{2020-cacm-attacks,
author = {Kocher, Paul and Horn, Jann and Fogh, Anders and Genkin, Daniel and Gruss, Daniel and Haas, Werner and Hamburg, Mike and Lipp, Moritz and Mangard, Stefan and Prescher, Thomas and Schwarz, Michael and Yarom, Yuval},
title = {Spectre Attacks: Exploiting Speculative Execution},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {63},
number = {7},
issn = {0001-0782},
url = {https://doi.org/10.1145/3399742},
doi = {10.1145/3399742},
abstract = {Modern processors use branch prediction and speculative execution to maximize performance. For example, if the destination of a branch depends on a memory value that is in the process of being read, CPUs will try to guess the destination and attempt to execute ahead. When the memory value finally arrives, the CPU either discards or commits the speculative computation. Speculative logic is unfaithful in how it executes, can access the victim's memory and registers, and can perform operations with measurable side effects.Spectre attacks involve inducing a victim to speculatively perform operations that would not occur during correct program execution and which leak the victim's confidential information via a side channel to the adversary. This paper describes practical attacks that combine methodology from side-channel attacks, fault attacks, and return-oriented programming that can read arbitrary memory from the victim's process. More broadly, the paper shows that speculative execution implementations violate the security assumptions underpinning numerous software security mechanisms, such as operating system process separation, containerization, just-in-time (JIT) compilation, and countermeasures to cache timing and side-channel attacks. These attacks represent a serious threat to actual systems because vulnerable speculative execution capabilities are found in microprocessors from Intel, AMD, and ARM that are used in billions of devices.Although makeshift processor-specific countermeasures are possible in some cases, sound solutions will require fixes to processor designs as well as updates to instruction set architectures (ISAs) to give hardware architects and software developers a common understanding as to what computation state CPU implementations are (and are not) permitted to leak.},
journal = {Commun. ACM},
month = {jun},
pages = {93–101},
numpages = {9}
}

@inbook{2020-osdi-cpu-slowdown,
author = {Behrens, Jonathan and Cao, Anton and Skeggs, Cel and Belay, Adam and Kaashoek, M. Frans and Zeldovich, Nickolai},
title = {Efficiently Mitigating Transient Execution Attacks Using the Unmapped Speculation Contract},
year = {2020},
isbn = {978-1-939133-19-9},
publisher = {USENIX Association},
address = {USA},
abstract = {Today's kernels pay a performance penalty for mitigations-- such as KPTI, retpoline, return stack stuffing, speculation barriers--to protect against transient execution side-channel attacks such as Meltdown [21] and Spectre [16].To address this performance penalty, this paper articulates the unmapped speculation contract, an observation that memory that isn't mapped in a page table cannot be leaked through transient execution. To demonstrate the value of this contract, the paper presents WARD, a new kernel design that maintains a separate kernel page table for every process. This page table contains mappings for kernel memory that is safe to expose to that process. Because a process doesn't map data of other processes, this design allows for many system calls to execute without any mitigation overhead. When a process needs access to sensitive data, WARD switches to a kernel page table that provides access to all of memory and executes with all mitigations.An evaluation of the WARD design implemented in the sv6 research kernel [8] shows that LEBench [24] can execute many system calls without mitigations. For some hardware generations, this results in performance improvement ranging from a few percent (huge page fault) to several factors (getpid), compared to a standard design with mitigations.},
booktitle = {Proceedings of the 14th USENIX Conference on Operating Systems Design and Implementation},
articleno = {64},
numpages = {16}
}

@inproceedings{2013-hotos-unify,
author = {\ulx{Animesh Trivedi} and Stuedi, Patrick and Metzler, Bernard and Pletka, Roman and Fitch, Blake G. and Gross, Thomas R.},
title = {Unified High-Performance I/O: One Stack to Rule Them All},
year = {2013},
publisher = {USENIX Association},
address = {USA},
abstract = {Fast non-volatile memories are exposing inefficiencies in traditional I/O stacks. Though there have been fragmented efforts to deal with the issues, there is a pressing need for a high-performance storage stack. Interestingly, 20 years ago, networks were faced with similar challenges, which led to the development of concepts and implementations of multiple high-performance network stacks. In this paper we draw parallels to illustrate synergies between high-performance storage requirements and concepts from the networking space. We identify common high-performance I/O properties and recent efforts in storage to achieve those properties. Instead of reinventing the performance wheel, we advocate a case for using mature high-performance networking abstractions and frameworks to meet the storage demands, and discuss opportunities and challenges that arise with this unification.},
booktitle = {Proceedings of the 14th USENIX Conference on Hot Topics in Operating Systems},
pages = {4},
numpages = {1},
location = {Santa Ana Pueblo, New Mexcio},
series = {HotOS'13}
}

@inproceedings{2022-systor-iouring,
author = {Didona, Diego and Pfefferle, Jonas and Ioannou, Nikolas and Metzler, Bernard and \ulx{Animesh Trivedi}},
title = {{Understanding Modern Storage APIs: A Systematic Study of Libaio, SPDK, and io\_uring}},
year = {2022},
isbn = {9781450393805},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534056.3534945},
doi = {10.1145/3534056.3534945},
abstract = {Recent high-performance storage devices have exposed software inefficiencies in existing storage stacks, leading to a new breed of I/O stacks. The newest storage API of the Linux kernel is io_uring. We perform one of the first in-depth studies of io_uring, and compare its performance and dis-/advantages with the established libaio and SPDK APIs. Our key findings reveal that (i) polling design significantly impacts performance; (ii) with enough CPU cores io_uring can deliver performance close to that of SPDK; and (iii) performance scalability over multiple CPU cores and devices requires careful consideration and necessitates a hybrid approach. Last, we provide design guidelines for developers of storage intensive applications.},
booktitle = {Proceedings of the 15th ACM International Conference on Systems and Storage},
pages = {120–127},
numpages = {8},
location = {Haifa, Israel},
series = {SYSTOR '22}
}

@article{2018-tos-flashnet,
author = {\ulx{Animesh Trivedi} and Ioannou, Nikolas and Metzler, Bernard and Stuedi, Patrick and Pfefferle, Jonas and Kourtis, Kornilios and Koltsidas, Ioannis and Gross, Thomas R.},
title = {FlashNet: Flash/Network Stack Co-Design},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {4},
issn = {1553-3077},
url = {https://doi-org.vu-nl.idm.oclc.org/10.1145/3239562},
doi = {10.1145/3239562},
abstract = {During the past decade, network and storage devices have undergone rapid performance improvements, delivering ultra-low latency and several Gbps of bandwidth. Nevertheless, current network and storage stacks fail to deliver this hardware performance to the applications, often due to the loss of I/O efficiency from stalled CPU performance. While many efforts attempt to address this issue solely on either the network or the storage stack, achieving high-performance for networked-storage applications requires a holistic approach that considers both.In this article, we present FlashNet, a software I/O stack that unifies high-performance network properties with flash storage access and management. FlashNet builds on RDMA principles and abstractions to provide a direct, asynchronous, end-to-end data path between a client and remote flash storage. The key insight behind FlashNet is to co-design the stack’s components (an RDMA controller, a flash controller, and a file system) to enable cross-stack optimizations and maximize I/O efficiency. In micro-benchmarks, FlashNet improves 4kB network I/O operations per second (IOPS by 38.6% to 1.22M, decreases access latency by 43.5% to 50.4μs, and prolongs the flash lifetime by 1.6-5.9\texttimes{} for writes. We illustrate the capabilities of FlashNet by building a Key-Value store and porting a distributed data store that uses RDMA on it. The use of FlashNet’s RDMA API improves the performance of KV store by 2\texttimes{} and requires minimum changes for the ported data store to access remote flash devices.},
journal = {ACM Trans. Storage},
month = {dec},
articleno = {30},
numpages = {29},
keywords = {performance, network storage, operating systems, RDMA, flash}
}

@inproceedings{2017-systor-flashnet,
author = {\ulx{Trivedi, Animesh} and Ioannou, Nikolas and Metzler, Bernard and Stuedi, Patrick and Pfefferle, Jonas and Koltsidas, Ioannis and Kourtis, Kornilios and Gross, Thomas R.},
title = {FlashNet: Flash/Network Stack Co-Design},
year = {2017},
isbn = {9781450350358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3078468.3078477},
doi = {10.1145/3078468.3078477},
abstract = {During the past decade, network and storage devices have undergone rapid performance improvements, delivering ultra-low latency and several Gbps of bandwidth. Nevertheless, current network and storage stacks fail to deliver this hardware performance to the applications, often due to the loss of IO efficiency from stalled CPU performance. While many efforts attempt to address this issue solely on either the network or the storage stack, achieving high-performance for networked-storage applications requires a holistic approach that considers both.In this paper, we present FlashNet, a software IO stack that unifies high-performance network properties with flash storage access and management. FlashNet builds on RDMA principles and abstractions to provide a direct, asynchronous, end-to-end data path between a client and remote flash storage. The key insight behind FlashNet is to co-design the stack's components (an RDMA controller, a flash controller, and a file system) to enable cross-stack optimizations and maximize IO efficiency. In micro-benchmarks, FlashNet improves 4kB network IOPS by 38.6% to 1.22M, decreases access latency by 43.5% to 50.4 µsecs, and prolongs the flash lifetime by 1.6--5.9\texttimes{} for writes. We illustrate the capabilities of FlashNet by building a Key-Value store, and porting a distributed data store that uses RDMA on it. The use of FlashNet's RDMA API improves the performance of KV store by 2\texttimes{}, and requires minimum changes for the ported data store to access remote flash devices.},
booktitle = {Proceedings of the 10th ACM International Systems and Storage Conference},
articleno = {15},
numpages = {14},
keywords = {netwoked flash, RDMA, performance, operating systems},
location = {Haifa, Israel},
series = {SYSTOR '17}
}

@article{2017-mit-cars,
  author  = {Lex Fridman and
               Daniel E. Brown and
               Michael Glazer and
               William Angell and
               Spencer Dodd and
               Benedikt Jenik and
               Jack Terwilliger and
               Julia Kindelsberger and
               Li Ding and
               Sean Seaman and
               Hillary Abraham and
               Alea Mehler and
               Andrew Sipperley and
               Anthony Pettinato and
               Bobbie Seppelt and
               Linda Angell and
               Bruce Mehler and
               Bryan Reimer},
  title   = {{MIT} Autonomous Vehicle Technology Study: Large-Scale Deep Learning Based Analysis of Driver Behavior and Interaction with Automation},
  journal = {CoRR},
  volume  = {abs/1711.06976},
  year    = {2017}
}

@inproceedings{2020-hotcloud-mc-serverless,
  author    = {Jesse Donkervliet and \ulx{Animesh Trivedi} and Alexandru Iosup},
  title     = {Towards Supporting Millions of Users in Modifiable Virtual Environments by Redesigning Minecraft-Like Games as Serverless Systems},
  booktitle = {{USENIX} Workshop on Hot Topics in Cloud Computing (HotCloud)},
  year      = {2020}
}

@inproceedings{2018-atc-albis,
  author    = {\ulx{Animesh Trivedi} and Patrick Stuedi and Jonas Pfefferle and Adrian Schuepbach and Bernard Metzler},
  title     = {Albis: High-Performance File Format for Big Data Systems},
  booktitle = {{USENIX} Annual Technical Conference ({ATC})},
  year      = {2018},
  pages     = {615--630}
}

@inproceedings {2020-osdi-panic,
author = {Jiaxin Lin and Kiran Patel and Brent E. Stephens and Anirudh Sivaraman and Aditya Akella},
title = {{PANIC}: A {High-Performance} Programmable {NIC} for Multi-tenant Networks},
booktitle = {14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20)},
year = {2020},
isbn = {978-1-939133-19-9},
pages = {243--259},
url = {https://www.usenix.org/conference/osdi20/presentation/lin},
publisher = {USENIX Association},
month = nov,
}

@misc{2022-bpf-hetero-uni-abi,
 title = {Using eBPF as a heterogeneous processing ABI},
 author = {Jakub Kicinski},
 howpublished={Linux Plumbers Conference},
 note={Accessed: 2022-Feb-02, \nolinkurl{http://vger.kernel.org/lpc_bpf2018_talks/Using_eBPF_as_a_heterogeneous_processing_ABI_LPC_2018.pdf}},
 year={2018},
}

@misc{ubpf,
 title = {Userspace eBPF VM}, 
 note={Accessed: 2022-Feb-02, \nolinkurl{https://github.com/iovisor/ubpf}},
 year={2022},
}

@article{2022-arvix-bam,
  title={BaM: A Case for Enabling Fine-grain High Throughput GPU-Orchestrated Access to Storage},
  author={Qureshi, Zaid and Mailthody, Vikram Sharma and Gelado, Isaac and Min, Seung Won and Masood, Amna and Park, Jeongmin and Xiong, Jinjun and Newburn, CJ and Vainbrand, Dmitri and Chung, I and others},
  journal={arXiv preprint arXiv:2203.04910},
  year={2022}
}

@inproceedings {2020-fast-infinicache,
author = {Ao Wang and Jingyuan Zhang and Xiaolong Ma and Ali Anwar and Lukas Rupprecht and Dimitrios Skourtis and Vasily Tarasov and Feng Yan and Yue Cheng},
title = {{InfiniCache}: Exploiting Ephemeral Serverless Functions to Build a {Cost-Effective} Memory Cache},
booktitle = {18th USENIX Conference on File and Storage Technologies (FAST 20)},
year = {2020},
isbn = {978-1-939133-12-0},
address = {Santa Clara, CA},
pages = {267--281},
url = {https://www.usenix.org/conference/fast20/presentation/wang-ao},
publisher = {USENIX Association},
month = feb,
}


@inproceedings {2020-nsdi-firecracker,
author = {Alexandru Agache and Marc Brooker and Alexandra Iordache and Anthony Liguori and Rolf Neugebauer and Phil Piwonka and Diana-Maria Popa},
title = {Firecracker: Lightweight Virtualization for Serverless Applications },
booktitle = {17th USENIX Symposium on Networked Systems Design and Implementation (NSDI 20)},
year = {2020},
isbn = {978-1-939133-13-7},
address = {Santa Clara, CA},
pages = {419--434},
url = {https://www.usenix.org/conference/nsdi20/presentation/agache},
publisher = {USENIX Association},
month = feb,
}

@inproceedings {2019-oml-serverless-ml,
author = {Anirban Bhattacharjee and Yogesh Barve and Shweta Khare and Shunxing Bao and Aniruddha Gokhale and Thomas Damiano},
title = {Stratum: A Serverless Framework for the Lifecycle Management of Machine Learning-based Data Analytics Tasks},
booktitle = {2019 USENIX Conference on Operational Machine Learning (OpML 19)},
year = {2019},
isbn = {978-1-939133-00-7},
address = {Santa Clara, CA},
pages = {59--61},
url = {https://www.usenix.org/conference/opml19/presentation/bhattacharjee},
publisher = {USENIX Association},
month = may,
}

@inproceedings {2018-atc-rapid-task,
author = {Edward Oakes and Leon Yang and Dennis Zhou and Kevin Houck and Tyler Harter and Andrea Arpaci-Dusseau and Remzi Arpaci-Dusseau},
title = {{SOCK}: Rapid Task Provisioning with {Serverless-Optimized} Containers},
booktitle = {2018 USENIX Annual Technical Conference (USENIX ATC 18)},
year = {2018},
isbn = {978-1-931971-44-7},
address = {Boston, MA},
pages = {57--70},
url = {https://www.usenix.org/conference/atc18/presentation/oakes},
publisher = {USENIX Association},
month = jul,
}

@inproceedings {2016-hotcloud-openlambda,
author = {Scott Hendrickson and Stephen Sturdevant and Tyler Harter and Venkateshwaran Venkataramani and Andrea C. Arpaci-Dusseau and Remzi H. Arpaci-Dusseau},
title = {Serverless Computation with {OpenLambda}},
booktitle = {8th USENIX Workshop on Hot Topics in Cloud Computing (HotCloud 16)},
year = {2016},
address = {Denver, CO},
url = {https://www.usenix.org/conference/hotcloud16/workshop-program/presentation/hendrickson},
publisher = {USENIX Association},
month = jun,
}

@misc{2022-xilinx-fpga-nvmf,
 author = {Deboleena Sakalley}, 
 title = {{Using FPGAs to accelerate NVMe-oF based Storage Networks}}, 
 note={Accessed: 2022-Feb-02, \nolinkurl{https://www.flashmemorysummit.com/English/Collaterals/Proceedings/2017/20170810_FW32_Sakalley.pdf}},
 year={2022},
} 

@inproceedings{2020-socc-serverless-benchmark,
author = {Yu, Tianyi and Liu, Qingyuan and Du, Dong and Xia, Yubin and Zang, Binyu and Lu, Ziqian and Yang, Pingchao and Qin, Chenggang and Chen, Haibo},
title = {Characterizing Serverless Platforms with Serverlessbench},
year = {2020},
isbn = {9781450381376},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3419111.3421280},
doi = {10.1145/3419111.3421280},
abstract = {Serverless computing promises auto-scalability and cost-efficiency (in "pay-as-you-go" manner) for high-productive software development. Because of its virtue, serverless computing has motivated increasingly new applications and services in the cloud. This, however, also presents new challenges including how to efficiently design high-performance serverless platforms and how to efficiently program on the platforms.This paper proposes ServerlessBench, an open-source benchmark suite for characterizing serverless platforms. It includes test cases exploring characteristic metrics of serverless computing, e.g., communication efficiency, startup latency, stateless overhead, and performance isolation. We have applied the benchmark suite to evaluate the most popular serverless computing platforms, including AWS Lambda, Open-Whisk, and Fn, and present new serverless implications from the study. For example, we show scenarios where decoupling an application into a composition of serverless functions can be beneficial in cost-saving and performance, and that the "stateless" property in serverless computing can hurt the execution performance of serverless functions. These implications form several design guidelines, which may help platform designers to optimize serverless platforms and application developers to design their functions best fit to the platforms.},
booktitle = {Proceedings of the 11th ACM Symposium on Cloud Computing},
pages = {30–44},
numpages = {15},
location = {Virtual Event, USA},
series = {SoCC '20}
}

@inproceedings {2020-fast-fpga-lsm,
author = {Teng Zhang and Jianying Wang and Xuntao Cheng and Hao Xu and Nanlong Yu and Gui Huang and Tieying Zhang and Dengcheng He and Feifei Li and Wei Cao and Zhongdong Huang and Jianling Sun},
title = {{FPGA-Accelerated} Compactions for {LSM-based} {Key-Value} Store},
booktitle = {18th USENIX Conference on File and Storage Technologies (FAST 20)},
year = {2020},
isbn = {978-1-939133-12-0},
address = {Santa Clara, CA},
pages = {225--237},
url = {https://www.usenix.org/conference/fast20/presentation/zhang-teng},
publisher = {USENIX Association},
month = feb,
}

@inproceedings {2021-nsdi-memcache-bpf,
author = {Yoann Ghigoff and Julien Sopena and Kahina Lazri and Antoine Blin and Gilles Muller},
title = {{BMC}: Accelerating Memcached using Safe In-kernel Caching and Pre-stack Processing},
booktitle = {18th USENIX Symposium on Networked Systems Design and Implementation (NSDI 21)},
year = {2021},
isbn = {978-1-939133-21-2},
pages = {487--501},
url = {https://www.usenix.org/conference/nsdi21/presentation/ghigoff},
publisher = {USENIX Association},
month = apr,
}

@inproceedings {2019-hotstorage-fpga-parquet,
author = {Lucas Kuhring and Eva Garcia and Zsolt Istv{\'a}n},
title = {Specialize in {Moderation{\textemdash}Building} Application-aware Storage Services using {FPGAs} in the Datacenter},
booktitle = {11th USENIX Workshop on Hot Topics in Storage and File Systems (HotStorage 19)},
year = {2019},
address = {Renton, WA},
url = {https://www.usenix.org/conference/hotstorage19/presentation/kuhring},
publisher = {USENIX Association},
month = jul,
}

@inproceedings{2019-pldi-bpf,
author = {Gershuni, Elazar and Amit, Nadav and Gurfinkel, Arie and Narodytska, Nina and Navas, Jorge A. and Rinetzky, Noam and Ryzhyk, Leonid and Sagiv, Mooly},
title = {Simple and Precise Static Analysis of Untrusted Linux Kernel Extensions},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.vu-nl.idm.oclc.org/10.1145/3314221.3314590},
doi = {10.1145/3314221.3314590},
abstract = {Extended Berkeley Packet Filter (eBPF) is a Linux subsystem that allows safely executing untrusted user-defined extensions inside the kernel. It relies on static analysis to protect the kernel against buggy and malicious extensions. As the eBPF ecosystem evolves to support more complex and diverse extensions, the limitations of its current verifier, including high rate of false positives, poor scalability, and lack of support for loops, have become a major barrier for developers.  We design a static analyzer for eBPF within the framework of abstract interpretation. Our choice of abstraction is based on common patterns found in many eBPF programs. We observed that eBPF programs manipulate memory in a rather disciplined way which permits analyzing them successfully with a scalable mixture of very-precise abstraction of certain bounded regions with coarser abstractions of other parts of the memory. We use the Zone domain, a simple domain that tracks differences between pairs of registers and offsets, to achieve precise and scalable analysis. We demonstrate that this abstraction is as precise in practice as more costly abstract domains like Octagon and Polyhedra.  Furthermore, our evaluation, based on hundreds of real-world eBPF programs, shows that the new tool generates no more false alarms than the existing Linux verifier, while it supports a wider class of programs (including programs with loops) and has better asymptotic complexity.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {1069–1084},
numpages = {16},
keywords = {static analysis, linux, kernel extensions, ebpf},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@misc{2018-kernel-bpf-verifier,
 title = {Kernel analysis using eBPF},
 author = {Daniel Thompson and Leo Yan}, 
 note={Accessed: 2022-Feb-02, \nolinkurl{https://elinux.org/images/d/dc/Kernel-Analysis-Using-eBPF-Daniel-Thompson-Linaro.pdf}},
 year={2018},
}

@article{2020-arxiv-appcode,
  author    = {Kornilios Kourtis and
               \ulx{Animesh Trivedi} and
               Nikolas Ioannou},
  title     = {Safe and Efficient Remote Application Code Execution on Disaggregated
               {NVM} Storage with eBPF},
  journal   = {CoRR},
  volume    = {abs/2002.11528},
  year      = {2020},
  url       = {https://arxiv.org/abs/2002.11528},
  eprinttype = {arXiv},
  eprint    = {2002.11528},
  timestamp = {Tue, 03 Mar 2020 14:32:13 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2002-11528.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{2018-cc-game-offloading,
  author  = {M. H. Jiang and
               Otto W. Visser and
               I. S. W. B. Prasetya and
               Alexandru Iosup},
  title   = {A mirroring architecture for sophisticated mobile games using computation-offloading},
  journal = {Concurrency and Computation Practice and Experience},
  volume  = {30},
  number  = {17},
  year    = {2018}
}

@inproceedings{2018-sec-vr-edge,
  author    = {Y. {Li} and W. {Gao}},
  booktitle = {IEEE/ACM Symposium on Edge Computing (SEC)},
  title     = {MUVR: Supporting Multi-User Mobile Virtual Reality with Resource Constrained Edge Cloud},
  year      = {2018},
  pages     = {1-16}
} 

@inproceedings{2018-sec-urban,
  author    = {Julien Gedeon and Michael Stein and Jeff Krisztinkovics and Patrick Felka and Katharina Keller and Christian Meurisch and Lin Wang and Max Mühlhäuser},
  booktitle = {IEEE/ACM Symposium on Edge Computing (SEC)},
  title     = {From Cell Towers to Smart Street Lamps: Placing Cloudlets on Existing Urban Infrastructures},
  year      = {2018},
  pages     = {187-202}
}

@inproceedings{2018-sec-iot-camera,
  author    = {Si Young Jang and Yoonhyung Lee and Byoungheon Shin and Dongman Lee},
  booktitle = {IEEE/ACM Symposium on Edge Computing (SEC)},
  title     = {Application-Aware IoT Camera Virtualization for Video Analytics Edge Computing},
  year      = {2018},
  pages     = {132-144}
} 

@inproceedings{2018-sec-drone-vid,
  author    = {Junjue Wang and Ziqiang Feng and Zhuo Chen and Shilpa Anna George and Mihir Bala and Padmanabhan Pillai and Shao-Wen Yang and Mahadev Satyanarayanan},
  booktitle = {IEEE/ACM Symposium on Edge Computing (SEC)},
  title     = {Bandwidth-Efficient Live Video Analytics for Drones Via Edge Computing},
  year      = {2018},
  pages     = {159-173}
} 

@inproceedings{2018-sec-videoedge,
  author    = {Chien-Chun Hung and Ganesh Ananthanarayanan and Peter Bodík and Leana Golubchik and Minlan Yu and Victor Bahl and Matthai Philipose},
  booktitle = {IEEE/ACM Symposium on Edge Computing (SEC)},
  title     = {VideoEdge: Processing Camera Streams using Hierarchical Clusters},
  year      = {2018},
  pages     = {115-131}
} 

@article{2018-ieee-edge-compute-offload-modeling,
  author  = {José Leal D. Neto and Se-Young Yu and Daniel F. Macedo and José Marcos S. Nogueira and Rami Langar and Stefano Secci},
  journal = {IEEE Transactions on Mobile Computing},
  title   = {ULOOF: A User Level Online Offloading Framework for Mobile Edge Computing},
  year    = {2018},
  volume  = {17},
  number  = {11},
  pages   = {2660-2674}
}

@inproceedings{2018-hotedge-eco,
  author    = {Nisha Talagala and Swaminathan Sundararaman and Vinay Sridhar and Dulcardo Arteaga and Qianmei Luo and Sriram Subramanian and Sindhu Ghanta and Lior Khermosh and Drew Roselli},
  title     = {{ECO}: Harmonizing Edge and Cloud with ML/DL Orchestration},
  booktitle = {{USENIX} Workshop on Hot Topics in Edge Computing (HotEdge)},
  year      = {2018}
}

@inproceedings{2017-chi-ss,
  author    = {Laput, Gierad and Zhang, Yang and Harrison, Chris},
  title     = {Synthetic Sensors: Towards General-Purpose Sensing},
  year      = {2017},
  booktitle = {CHI Conference on Human Factors in Computing Systems (CHI)},
  pages     = {3986–3999},
  numpages  = {14}
}

@inproceedings{Diao2014CloudCraftCD,
  author    = {Diao, Ziqiang and Wang, S. and Schallehn, E. and Saake, Gunter},
  year      = {2014},
  month     = {January},
  pages     = {71-84},
  title     = {CloudCraft: Cloud-based data management for MMORPGs},
  volume    = {270},
  booktitle = {Databases and Information Systems VIII}
}

@inproceedings{2014-osdi-param,
  author    = {Mu Li and David G. Andersen and Jun Woo Park and Alexander J. Smola and Amr Ahmed and Vanja Josifovski and James Long and Eugene J. Shekita and Bor-Yiing Su},
  title     = {Scaling Distributed Machine Learning with the Parameter Server},
  booktitle = {{USENIX} Symposium on Operating Systems Design and Implementation ({OSDI})},
  year      = {2014},
  pages     = {583--598}
}

@inproceedings{2017-edge-cloudpath,
  author    = {Mortazavi, Seyed Hossein and Salehe, Mohammad and Gomes, Carolina Simoes and Phillips, Caleb and de Lara, Eyal},
  title     = {Cloudpath: A Multi-Tier Cloud Computing Framework},
  year      = {2017},
  booktitle = {ACM/IEEE Symposium on Edge Computing (SEC)}
}

@inproceedings{2017-sec-precog,
  author    = {Drolia, Utsav and Guo, Katherine and Narasimhan, Priya},
  title     = {Precog: refetching for Image recognition Applications at the Edge},
  year      = {2017},
  booktitle = {ACM/IEEE Symposium on Edge Computing (SEC)}
}
  
@inproceedings{2017-icdcs-cachier,
  author    = {Utsav Drolia and Katherine Guo and Jiaqi Tan and Rajeev Gandhi and Priya Narasimhan},
  booktitle = {IEEE International Conference on Distributed Computing Systems (ICDCS)},
  title     = {Cachier: Edge-Caching for Recognition Applications},
  year      = {2017},
  pages     = {276-286}
}

@inproceedings{2017-mmog-ec+,
  author    = {Zhang, Wuyang and Chen, Jiachen and Zhang, Yanyong and Raychaudhuri, Dipankar},
  title     = {Towards Efficient Edge Cloud Augmentation for Virtual Reality MMOGs},
  year      = {2017},
  booktitle = {ACM/IEEE Symposium on Edge Computing (SEC)}
}

@article{2009-ieee-cloudlet,
  author  = {Mahadev Satyanarayanan and Paramvir Bahl and Ramón Cáceres and Nigel Davies},
  journal = {IEEE Pervasive Computing},
  title   = {The Case for VM-Based Cloudlets in Mobile Computing},
  year    = {2009},
  volume  = {8},
  number  = {4},
  pages   = {14-23}
}

@article{2017-ieee-satya-edge-computing,
  author     = {Satyanarayanan, Mahadev},
  title      = {The Emergence of Edge Computing},
  year       = {2017},
  issue_date = {January 2017},
  volume     = {50},
  number     = {1},
  pages      = {30–39}
}

@inproceedings{2016-acm-load-online,
  author    = {Burger, Valentin and Pajo, Jane Frances and Sanchez, Odnan Ref and Seufert, Michael and Schwartz, Christian and Wamser, Florian and Davoli, Franco and Tran-Gia, Phuoc},
  title     = {Load Dynamics of a Multiplayer Online Battle Arena and Simulative Assessment of Edge Server Placements},
  year      = {2016},
  booktitle = {ACM International Conference on Multimedia Systems (MMSys)}
}

@inproceedings{2016-icdcn-edge-storage-security,
  author    = {Esiner, Ertem and Datta, Anwitaman},
  title     = {Layered Security for Storage at the Edge: On Decentralized Multi-factor Access Control},
  booktitle = {ACM International Conference on Distributed Computing and Networking (ICDCN)},
  year      = {2016},
  pages     = {9:1--9:10}
} 

@inproceedings{2017-nsdi-farmbeats,
  author    = {Deepak Vasisht and Zerina Kapetanovic and Jongho Won and Xinxin Jin and Ranveer Chandra and Sudipta Sinha and Ashish Kapoor and Madhusudhan Sudarshan and Sean Stratman},
  title     = {FarmBeats: An IoT Platform for Data-Driven Agriculture},
  booktitle = {{USENIX} Symposium on Networked Systems Design and Implementation (NSDI)},
  year      = {2017},
  pages     = {515--529}
}

@inbook{2017-Springer-object-store,
  author    = {Confais, Bastien
and Lebre, Adrien
and Parrein, Beno{\^i}t},
  title     = {Performance Analysis of Object Store Systems in a Fog and Edge Computing Infrastructure},
  booktitle = {Transactions on Large-Scale Data- and Knowledge-Centered Systems XXXIII},
  year      = {2017},
  pages     = {40--79}
}

@inproceedings{2018-asplos-autodrive,
  author    = {Lin, Shih-Chieh and Zhang, Yunqi and Hsu, Chang-Hong and Skach, Matt and Haque, Md E. and Tang, Lingjia and Mars, Jason},
  title     = {The Architectural Implications of Autonomous Driving: Constraints and Acceleration},
  year      = {2018},
  booktitle = {International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)},
  pages     = {751–766}
}


@inproceedings{2018-hotedge-eSGD,
  author    = {Zeyi Tao and Qun Li},
  title     = {eSGD: Communication Efficient Distributed Deep Learning on the Edge},
  booktitle = {{USENIX} Workshop on Hot Topics in Edge Computing (HotEdge)},
  year      = {2018}
}

@inproceedings{2018-hotedge-modi,
  author    = {Samuel S. Ogden and Tian Guo},
  title     = {{MODI}: Mobile Deep Inference Made Efficient by Edge Computing},
  booktitle = {{USENIX} Workshop on Hot Topics in Edge Computing (HotEdge)},
  year      = {2018}
}

@ARTICLE{2016-ieee-csur-fpga-hls,  
author={Nane, Razvan and Sima, Vlad-Mihai and Pilato, Christian and Choi, Jongsok and Fort, Blair and Canis, Andrew and Chen, Yu Ting and Hsiao, Hsuan and Brown, Stephen and Ferrandi, Fabrizio and Anderson, Jason and Bertels, Koen},  
journal={IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},   
title={A Survey and Evaluation of FPGA High-Level Synthesis Tools},   year={2016},  volume={35},  number={10},  pages={1591-1604},  doi={10.1109/TCAD.2015.2513673}}

@article{2018-csur-fpga-hw-config,
author = {Vipin, Kizheppatt and Fahmy, Suhaib A.},
title = {FPGA Dynamic and Partial Reconfiguration: A Survey of Architectures, Methods, and Applications},
year = {2018},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3193827},
doi = {10.1145/3193827},
abstract = {Dynamic and partial reconfiguration are key differentiating capabilities of field programmable gate arrays (FPGAs). While they have been studied extensively in academic literature, they find limited use in deployed systems. We review FPGA reconfiguration, looking at architectures built for the purpose, and the properties of modern commercial architectures. We then investigate design flows and identify the key challenges in making reconfigurable FPGA systems easier to design. Finally, we look at applications where reconfiguration has found use, as well as proposing new areas where this capability places FPGAs in a unique position for adoption.},
journal = {ACM Comput. Surv.},
month = {jul},
articleno = {72},
numpages = {39},
keywords = {dynamic reconfiguration, Field programmable gate arrays, partial reconfiguration}
}

@article{2018-csur-edge,
  author  = {Li, Chao and Xue, Yushu and Wang, Jing and Zhang, Weigong and Li, Tao},
  title   = {Edge-Oriented Computing Paradigms: A Survey on Architecture Design and System Management},
  year    = {2018},
  volume  = {51},
  number  = {2},
  journal = {ACM Computing Surveys}
}

@inproceedings{2018-hotedge-red-wedding,
  author    = {Christopher Meiklejohn and Heather Miller and Zeeshan Lakhani},
  title     = {Towards a Solution to the Red Wedding Problem},
  booktitle = {{USENIX} Workshop on Hot Topics in Edge Computing (HotEdge)},
  year      = {2018}
}

@article{2018-xxx-stream,
  author  = {Dias de Assuno, Marcos and da Silva Veith, Alexandre and Buyya, Rajkumar},
  title   = {Distributed Data Stream Processing and Edge Computing},
  year    = {2018},
  volume  = {103},
  number  = {C},
  journal = {Journal of Network and Computer Applications},
  pages   = {1–17}
}


@article{2018-xxx-fog-iot,
  author  = {Anwar, Muhammad and Wang, Shangguang and Zia, Muhammad and Jadoon, Ahmer and Akram, Umair and Raza Naqvi, Syed Salman},
  year    = {2018},
  month   = {05},
  pages   = {1-22},
  title   = {Fog Computing: An Overview of Big IoT Data Analytics},
  volume  = {2018},
  journal = {Wireless Communications and Mobile Computing}
}

@inproceedings{2018-atc-understanding-serverless,
  author    = {Klimovic, Ana and Wang, Yawen and Kozyrakis, Christos and Stuedi, Patrick and Pfefferle, Jonas and \ulx{Trivedi, Animesh}},
  title     = {Understanding Ephemeral Storage for Serverless Analytics},
  year      = {2018},
  booktitle = {USENIX Conference on Usenix Annual Technical Conference (ATC)},
  pages     = {789–794}
}

@inproceedings{2018-osdi-pocket,
  author    = {Klimovic, Ana and Wang, Yawen and Stuedi, Patrick and \ulx{Trivedi, Animesh} and Pfefferle, Jonas and Kozyrakis, Christos},
  title     = {Pocket: Elastic Ephemeral Storage for Serverless Analytics},
  year      = {2018},
  booktitle = {USENIX Conference on Operating Systems Design and Implementation (OSDI)},
  pages     = {427–444}
}

@inproceedings{2018-hotedge-ml-datastore,
  author    = {Arun Ravindran and Anjus George},
  title     = {An Edge Datastore Architecture For Latency-Critical Distributed Machine Vision Applications},
  booktitle = {{USENIX} Workshop on Hot Topics in Edge Computing (HotEdge)},
  year      = {2018}
}

@techreport{2018-inria-techr-stream,
  title       = {{Storage and Ingestion Systems in Support of Stream Processing: A Survey}},
  author      = {Marcu, Ovidiu-Cristian and Costan, Alexandru and Antoniu, Gabriel and P{\'e}rez-Hern{\'a}ndez, Mar{\'i}a S and Tudoran, Radu and Bortoli, Stefano and Nicolae, Bogdan},
  url         = {https://hal.inria.fr/hal-01939280},
  type        = {Technical Report},
  number      = {RT-0501},
  pages       = {1-33},
  institution = {{INRIA Rennes - Bretagne Atlantique and University of Rennes 1, France}},
  year        = {2018},
  month       = Nov
}
@inproceedings{2018-hotedge-session-consistency,
  author    = {Seyed Hossein Mortazavi and Bharath Balasubramanian and Eyal de Lara and Shankaranarayanan Puzhavakath Narayanan},
  title     = {Toward Session Consistency for the Edge},
  booktitle = {{USENIX} Workshop on Hot Topics in Edge Computing (HotEdge)},
  year      = {2018}
}

@article{2019-edgedb,
  author  = {Yang Yang and Qiang Cao and Hong Jiang },
  journal = {IEEE Access},
  title   = {EdgeDB: An Efficient Time-Series Database for Edge Computing},
  year    = {2019},
  volume  = {7},
  number  = {},
  pages   = {142295-142307}
}

@inproceedings{2019-arxiv-tdb1,
  author    = {Shuai Zhang and
               Wenxi Zeng and
               I{-}Ling Yen and
               Farokh B. Bastani},
  booktitle = {IEEE International Symposium on High Assurance Systems Engineering (HASE)},
  title     = {Semantically Enhanced Time Series Databases in IoT-Edge-Cloud Infrastructure},
  year      = {2019},
  pages     = {25-32}
}

@article{2019-cacm-federated,
  author  = {Yang, Qiang and Liu, Yang and Chen, Tianjian and Tong, Yongxin},
  title   = {Federated Machine Learning: Concept and Applications},
  year    = {2019},
  volume  = {10},
  number  = {2},
  journal = {ACM Transactions on Intelligent Systems and Technology}
}

@inproceedings{2019-iotdi-hetro,
  author    = {Zhang, Daniel (Yue) and Rashid, Tahmid and Li, Xukun and Vance, Nathan and Wang, Dong},
  title     = {HeteroEdge: Taming the Heterogeneity of Edge Computing System in Social Sensing},
  year      = {2019},
  booktitle = {ACM/IEEE International Conference on Internet of Things Design and Implementation (IoTDI)},
  pages     = {37–48}
}
  
@article{2019-algocloud-egde-monitoring,
  author  = {Roger Pueyo Centelles and Mennan Selimi and Felix Freitag and Leandro Navarro},
  title   = {A Monitoring System for Distributed Edge Infrastructures with Decentralized Coordination},
  year    = {2019},
  journal = {ALGOCLOUD}
}

@article{2019-ieee-edge-game,
  author  = {Xu Zhang and Hao Chen and Zhao and Zhan Ma and Yiling Xu and Haojun Huang and Hao Yin and Dapeng Oliver Wu},
  journal = {IEEE Wireless Communications},
  title   = {Improving Cloud Gaming Experience through Mobile Edge Computing},
  year    = {2019},
  volume  = {26},
  number  = {4},
  pages   = {178-183}
}

@article{2019-cacm-serverless,
  author  = {Castro, Paul and Ishakian, Vatche and Muthusamy, Vinod and Slominski, Aleksander},
  title   = {The Rise of Serverless Computing},
  year    = {2019},
  volume  = {62},
  number  = {12},
  journal = {Communications of the ACM},
  pages   = {44–54}
}

@inproceedings{2019-hotedge-ai-serverless,
  author    = {Thomas Rausch and Waldemar Hummer and Vinod Muthusamy and Alexander Rashed and Schahram Dustdar},
  title     = {Towards a Serverless Platform for Edge {AI}},
  booktitle = {{USENIX} Workshop on Hot Topics in Edge Computing (HotEdge)},
  year      = {2019}
}

@inproceedings{2019-iotdi-serverless,
  author    = {Hall, Adam and Ramachandran, Umakishore},
  title     = {An Execution Model for Serverless Functions at the Edge},
  year      = {2019},
  booktitle = {ACM/IEEE International Conference on Internet of Things Design and Implementation (IoTDI)},
  pages     = {225–236}
}

@article{2019-arxiv-internet-of-vehicles,
  author  = {Jun Zhang and Khaled B. Letaief},
  journal = {Proceedings of the IEEE},
  title   = {Mobile Edge Intelligence and Computing for the Internet of Vehicles},
  year    = {2020},
  volume  = {108},
  number  = {2},
  pages   = {246-261}
}

@inproceedings{2019-sec-aerial-sensing,
  author    = {Boubin, Jayson G. and Babu, Naveen T. R. and Stewart, Christopher and Chumley, John and Zhang, Shiqi},
  title     = {Managing Edge Resources for Fully Autonomous Aerial Systems},
  year      = {2019},
  booktitle = {ACM/IEEE Symposium on Edge Computing (SEC)},
  pages     = {74–87}
}
 

@inproceedings{2019-sec-fcooper,
  author    = {Chen, Qi and Ma, Xu and Tang, Sihai and Guo, Jingda and Yang, Qing and Fu, Song},
  title     = {F-Cooper: Feature Based Cooperative Perception for Autonomous Vehicle Edge Computing System Using 3D Point Clouds},
  year      = {2019},
  booktitle = {ACM/IEEE Symposium on Edge Computing (SEC)},
  pages     = {88–100},
  numpages  = {13}
}

@inproceedings{2019-sec-couper,
  author    = {Hsu, Ke-Jou and Bhardwaj, Ketan and Gavrilovska, Ada},
  title     = {Couper: DNN Model Slicing for Visual Analytics Containers at the Edge},
  year      = {2019},
  booktitle = {ACM/IEEE Symposium on Edge Computing (SEC)},
  pages     = {179–194}
}
  
@article{2019-ieee-ml-survey,
  author  = { Jiasi Chen and Xukan Ran },
  journal = {Proceedings of the IEEE},
  title   = {Deep Learning With Edge Computing: A Review},
  year    = {2019},
  volume  = {107},
  number  = {8},
  pages   = {1655-1674}
}

@inproceedings{2019-sec-cec,
  author    = {Zhang, Xingzhou and Qiao, Mu and Liu, Liangkai and Xu, Yunfei and Shi, Weisong},
  title     = {Collaborative Cloud-Edge Computation for Personalized Driving Behavior Modeling},
  year      = {2019},
  booktitle = {ACM/IEEE Symposium on Edge Computing (SEC)},
  pages     = {209–221}
}

@inproceedings{2019-sec-defog,
  author    = {McChesney, Jonathan and Wang, Nan and Tanwer, Ashish and de Lara, Eyal and Varghese, Blesson},
  title     = {DeFog: Fog Computing Benchmarks},
  year      = {2019},
  booktitle = {ACM/IEEE Symposium on Edge Computing (SEC)},
  pages     = {47–58}
}

@inproceedings{2019-hotedge-iot-cooperation,
  author    = {Zach Leidall and Abhishek Chandra and Jon Weissman},
  title     = {An Edge-based Framework for Cooperation in Internet of Things Applications},
  booktitle = {{USENIX} Workshop on Hot Topics in Edge Computing (HotEdge)},
  year      = {2019}
}

@inproceedings{2019-hotedge-decafe,
  author    = {Dhruv Kumar and Aravind Alagiri Ramkumar and Rohit Sindhu and Abhishek Chandra},
  title     = {DeCaf: Iterative Collaborative Processing over the Edge},
  booktitle = {{USENIX} Workshop on Hot Topics in Edge Computing (HotEdge)},
  year      = {2019}
}

@inproceedings{2019-xxx-serverless-edge,
  author    = {Hall, Adam and Ramachandran, Umakishore},
  title     = {An Execution Model for Serverless Functions at the Edge},
  year      = {2019},
  booktitle = {ACM/IEEE International Conference on Internet of Things Design and Implementation (IoTDI)},
  pages     = {225–236}
}

@inproceedings{2019-hotedge-telepathology,
  author    = {Alessio Sacco and Flavio Esposito and Princewill Okorie and Guido Marchetto},
  title     = {LiveMicro: An Edge Computing System for Collaborative Telepathology},
  booktitle = {{USENIX} Workshop on Hot Topics in Edge Computing (HotEdge)},
  year      = {2019}
}

@inproceedings{2019-hotedge-serverless-edge,
  author    = {Thomas Rausch and Waldemar Hummer and Vinod Muthusamy and Alexander Rashed and Schahram Dustdar},
  title     = {Towards a Serverless Platform for Edge {AI}},
  booktitle = {{USENIX} Workshop on Hot Topics in Edge Computing (HotEdge)},
  year      = {2019}
}

@inproceedings{2019-hotedge-sc-training,
  author    = {Yitao Chen and Kaiqi Zhao and Baoxin Li and Ming Zhao},
  title     = {Exploring the Use of Synthetic Gradients for Distributed Deep Learning across Cloud and Edge Resources},
  booktitle = {{USENIX} Workshop on Hot Topics in Edge Computing (HotEdge)},
  year      = {2019}
}

@inproceedings{2019-hotedge-coll-learning,
  author    = {Sidi Lu and Yongtao Yao and Weisong Shi},
  title     = {Collaborative Learning on the Edges: A Case Study on Connected Vehicles},
  booktitle = {{USENIX} Workshop on Hot Topics in Edge Computing (HotEdge)},
  year      = {2019}
}
@inproceedings{2019-hotedge-deter-container,
  author    = {Matthew Furlong and Andrew Quinn and Jason Flinn},
  title     = {The Case for Determinism on the Edge},
  booktitle = {{USENIX} Workshop on Hot Topics in Edge Computing (HotEdge)},
  year      = {2019}
}

@inproceedings{2019-atc-crail,
  author    = {Stuedi, Patrick and \ulx{Trivedi, Animesh} and Pfefferle, Jonas and Klimovic, Ana and Schuepbach, Adrian and Metzler, Bernard},
  title     = {Unification of Temporary Storage in the Nodekernel Architecture},
  year      = {2019},
  booktitle = {USENIX Conference on Usenix Annual Technical Conference (ATC)},
  pages     = {767–781}
}

@article{2019-arxiv-edge-connectivity,
  author  = {Quoc{-}Viet Pham and
               Fang Fang and
               Ha{-}Nguyen Vu and
               Mai Le and
               Zhiguo Ding and
               Long Bao Le and
               Won{-}Joo Hwang},
  title   = {A Survey of Multi-Access Edge Computing in 5G and Beyond: Fundamentals, Technology Integration, and State-of-the-Art},
  journal = {CoRR},
  volume  = {abs/1906.08452},
  year    = {2019}
}


@inproceedings{2019-hotmobile-reconf-streaming,
  author    = {Tiwari, Abhishek and Ramprasad, Brian and Mortazavi, Seyed Hossein and Gabel, Moshe and Lara, Eyal de},
  title     = {Reconfigurable Streaming for the Mobile Edge},
  booktitle = {ACM International Workshop on Mobile Computing Systems and Applications (HotMobile)},
  year      = {2019},
  pages     = {153--158}
} 

@inproceedings{2018-hotedge-iot-ddos,
  author    = {Ketan Bhardwaj and Joaquin Chung Miranda and Ada Gavrilovska},
  title     = {Towards IoT-DDoS Prevention Using Edge Computing},
  booktitle = {{USENIX} Workshop on Hot Topics in Edge Computing (HotEdge)},
  year      = {2018}
}

@inproceedings{2018-osdi-ray,
  author    = {Moritz, Philipp and Nishihara, Robert and Wang, Stephanie and Tumanov, Alexey and Liaw, Richard and Liang, Eric and Elibol, Melih and Yang, Zongheng and Paul, William and Jordan, Michael I. and et al.},
  title     = {Ray: A Distributed Framework for Emerging AI Applications},
  year      = {2018},
  booktitle = {USENIX Conference on Operating Systems Design and Implementation (OSDI)},
  pages     = {561–577}
}

@inproceedings{2018-hotedge-edgecons,
  author    = {Zijiang Hao and Shanhe Yi and Qun Li},
  title     = {EdgeCons: Achieving Efficient Consensus in Edge Computing Networks},
  booktitle = {{USENIX} Workshop on Hot Topics in Edge Computing (HotEdge)},
  year      = {2018}
}

@inproceedings{2018-debs-fogstore,
  author    = {Gupta, Harshit and Ramachandran, Umakishore},
  title     = {FogStore: A Geo-Distributed Key-Value Store Guaranteeing Low Latency for Strongly Consistent Access},
  booktitle = {ACM International Conference on Distributed and Event-based Systems (DEBS)},
  year      = {2018},
  pages     = {148--159}
} 


@inproceedings{2018-mobisys-pathstore,
  author    = {Mortazavi, Seyed Hossein and Balasubramanian, Bharath and de Lara, Eyal and Narayanan, Shankaranarayanan Puzhavakath},
  title     = {Pathstore, A Data Storage Layer For The Edge},
  booktitle = {ACM Annual International Conference on Mobile Systems, Applications, and Services (MobiSys)},
  year      = {2018},
  pages     = {519--519}
} 

@inproceedings{2019-xxx-serverless-datacenter,
  title={The Serverless Data Center : Hardware Disaggregation Meets Serverless Computing},
  author={Nathan Pemberton and Johann Schleier-Smith},
  year={2019}
}

@article{2018-ieee-more-serverless,
  author  = {Erwin van Eyk and Lucian Toader and Sacheendra Talluri and Laurens Versluis and Alexandru Uță and Alexandru Iosup},
  journal = {IEEE Internet Computing},
  title   = {Serverless is More: From PaaS to Present Cloud Computing},
  year    = {2018},
  volume  = {22},
  number  = {5},
  pages   = {8-17}
}

@inproceedings{2017-ccgrid-stateless-streaming,
  author    = {Marcu, Ovidiu-Cristian and Tudoran, Radu and Nicolae, Bogdan and Costan, Alexandru and Antoniu, Gabriel and P\'{e}rez-Hern\'{a}ndez, Mar\'{\i}a S.},
  title     = {Exploring Shared State in Key-Value Store for Window-Based Multi-Pattern Streaming Analytics},
  year      = {2017},
  booktitle = {IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGrid)},
  pages     = {1044–1052}
}
  
@inproceedings{2012-osdi-spanner,
  author    = {Corbett, James C. and Dean, Jeffrey and Epstein, Michael and Fikes, Andrew and Frost, Christopher and Furman, J. J. and Ghemawat, Sanjay and Gubarev, Andrey and Heiser, Christopher and Hochschild, Peter and Hsieh, Wilson and Kanthak, Sebastian and Kogan, Eugene and Li, Hongyi and Lloyd, Alexander and Melnik, Sergey and Mwaura, David and Nagle, David and Quinlan, Sean and Rao, Rajesh and Rolig, Lindsay and Saito, Yasushi and Szymaniak, Michal and Taylor, Christopher and Wang, Ruth and Woodford, Dale},
  title     = {Spanner: Google's Globally-Distributed Database},
  year      = {2012},
  publisher = {USENIX Association},
  booktitle = {Proceedings of the 10th USENIX Conference on Operating Systems Design and Implementation},
  pages     = {251–264},
  numpages  = {14},
  address   = {Hollywood, CA, USA},
  series    = {OSDI'12}
}

@article{2012-vldb-dgl,
  author  = {Low, Yucheng and Bickson, Danny and Gonzalez, Joseph and Guestrin, Carlos and Kyrola, Aapo and Hellerstein, Joseph M.},
  title   = {Distributed GraphLab: A Framework for Machine Learning and Data Mining in the Cloud},
  year    = {2012},
  volume  = {5},
  number  = {8},
  journal = {Very Large Data Bases (VLDB)},
  pages   = {716–727}
}
  
@inproceedings{2018-nips-serverless-ml,
  author    = { Joao Carreira and Pedro Fonseca and Alexey Tumanov and Andrew Zhang and Randy Katz},
  title     = {A Case for Serverless Machine Learning},
  year      = {2018},
  booktitle = {ACM Workshop on Systems for ML and Open Source Software (MLSys)}
}

@inproceedings{2019-hotos-fastkv,
  author    = {Adya, Atul and Grandl, Robert and Myers, Daniel and Qin, Henry},
  title     = {Fast Key-Value Stores: An Idea Whose Time Has Come and Gone},
  year      = {2019},
  booktitle = {ACM Workshop on Hot Topics in Operating Systems (HotOS)},
  pages     = {113–119}
}
  
@inproceedings{2019-socc-serverless,
  author    = {Zhang, Tian and Xie, Dong and Li, Feifei and Stutsman, Ryan},
  title     = {Narrowing the Gap Between Serverless and Its State with Storage Functions},
  year      = {2019},
  booktitle = {ACM Symposium on Cloud Computing (SoCC)},
  pages     = {1–12},
  numpages  = {12}
}
  
  
@inproceedings{2019-hotedge-p2p,
  author    = {Gala Yadgar and Oleg Kolosov and Mehmet Fatih Aktas and Emina Soljanin},
  title     = {Modeling The Edge: Peer-to-Peer Reincarnated},
  booktitle = {{USENIX} Workshop on Hot Topics in Edge Computing (HotEdge)},
  year      = {2019}
}

@inproceedings {2015-nsdi-gpu-packet-processing,
author = {Anuj Kalia and Dong Zhou and Michael Kaminsky and David G. Andersen},
title = {Raising the Bar for Using GPUs in Software Packet Processing},
booktitle = {12th {USENIX} Symposium on Networked Systems Design and Implementation ({NSDI} 15)},
year = {2015},
isbn = {978-1-931971-218},
address = {Oakland, CA},
pages = {409--423},
url = {https://www.usenix.org/conference/nsdi15/technical-sessions/presentation/kalia},
publisher = {{USENIX} Association},
month = may,
}

@inproceedings{2015-wfiot-topology-analytics,
  author    = { Bin Cheng and Apostolos Papageorgiou and Flavio Cirillo and Ernoe Kovacs },
  booktitle = {IEEE World Forum on Internet of Things (WF-IoT)},
  title     = {GeeLytics: Geo-distributed edge analytics for large scale IoT systems based on dynamic topology},
  year      = {2015},
  pages     = {565-570}
}

@article{2015-sigcomm-timely,
  author  = {Mittal, Radhika and Lam, Vinh The and Dukkipati, Nandita and Blem, Emily and Wassel, Hassan and Ghobadi, Monia and Vahdat, Amin and Wang, Yaogong and Wetherall, David and Zats, David},
  title   = {TIMELY: RTT-Based Congestion Control for the Datacenter},
  year    = {2015},
  volume  = {45},
  number  = {4},
  journal = {ACM SIGCOMM Computer Communication Review},
  pages   = {537–550}
}
  
@inproceedings{2015-drive-share,
  author    = {M. {Kuderer} and S. {Gulati} and W. {Burgard}},
  booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
  title     = {Learning driving styles for autonomous vehicles from demonstration},
  year      = {2015},
  pages     = {2641-2646}
}

@inproceedings{2015-ccdb,
  author    = {Diao, Ziqiang},
  year      = {2013},
  month     = {May},
  pages     = {16-21},
  title     = {Consistency models for Cloud-based online games: The storage system's perspective},
  volume    = {1020},
  booktitle = {GI-Workshop on Foundations of Databases (Grundlagen von Daten-banken)}
}

@inproceedings{2015-wowmom-edgebuffer,
  author    = {Feixiong Zhang and
               Chenren Xu and
               Yanyong Zhang and
               K. K. Ramakrishnan and
               Shreyasee Mukherjee and
               Roy D. Yates and
               Thu D. Nguyen},
  title     = {EdgeBuffer: Caching and prefetching content at the edge in the MobilityFirst future Internet architecture},
  booktitle = {{IEEE} International Symposium on {A} World of Wireless, Mobile
               and Multimedia Networks (WoWMoM)},
  pages     = {1--9},
  year      = {2015}
}

@inproceedings{2013-sosp-pileus,
  author    = {Terry, Douglas B. and Prabhakaran, Vijayan and Kotla, Ramakrishna and Balakrishnan, Mahesh and Aguilera, Marcos K. and Abu-Libdeh, Hussam},
  title     = {Consistency-Based Service Level Agreements for Cloud Storage},
  year      = {2013},
  booktitle = {ACM Symposium on Operating Systems Principles (SOSP)},
  pages     = {309–324}
}

@inproceedings{2013-hotdep-pscloud,
  author    = {Sobir Bazarbayev and
               Matti A. Hiltunen and
               Kaustubh R. Joshi and
               William H. Sanders and
               Richard D. Schlichting},
  title     = {PSCloud: a durable context-aware personal storage cloud},
  booktitle = {ACM Workshop on Hot Topics in Dependable Systems (HotDep)},
  pages     = {9:1--9:6},
  year      = {2013}
}

@inproceedings{2010-socc-ycsb,
  author    = {Cooper, Brian F. and Silberstein, Adam and Tam, Erwin and Ramakrishnan, Raghu and Sears, Russell},
  title     = {Benchmarking Cloud Serving Systems with YCSB},
  year      = {2010},
  booktitle = {ACM Symposium on Cloud Computing (SoCC)},
  pages     = {143–154}
}
  
@inproceedings{2010-osdi-piccolo,
  author    = {Power, Russell and Li, Jinyang},
  title     = {Piccolo: Building Fast, Distributed Programs with Partitioned Tables},
  year      = {2010},
  booktitle = {USENIX Conference on Operating Systems Design and Implementation (OSDI)},
  pages     = {293–306}
}

@inproceedings{2010-iptps-amazingstore,
  author    = {Zhi Yang and
               Ben Y. Zhao and
               Yuanjian Xing and
               Song Ding and
               Feng Xiao and
               Yafei Dai},
  title     = {AmazingStore: available, low-cost online storage service using cloudlets},
  booktitle = {International Conference on Peer-to-Peer Systems (IPTPS)},
  year      = {2010}
}

@inproceedings{2018-hotedge-datafog,
  author    = {Harshit Gupta and
               Zhuangdi Xu and
               Umakishore Ramachandran},
  title     = {DataFog: Towards a Holistic Data Management Platform for the IoT Age at the Network Edge},
  booktitle = {{USENIX} Workshop on Hot Topics in Edge Computing (HotEdge)},
  year      = {2018}
}

@inproceedings{2018-sigmod-dpaxos,
  author    = {Faisal Nawab and
               Divyakant Agrawal and
               Amr {El Abbadi}},
  title     = {DPaxos: Managing Data Closer to Users for Low-Latency and Mobile Applications},
  booktitle = {ACM International Conference on Management of
               Data (SIGCOMM)},
  pages     = {1221--1236},
  year      = {2018}
}

@article{2013-vldb-f1,
  author  = {Jeff Shute and
               Radek Vingralek and
               Bart Samwel and
               Ben Handy and
               Chad Whipkey and
               Eric Rollins and
               Mircea Oancea and
               Kyle Littlefield and
               David Menestrina and
               Stephan Ellner and
               John Cieslewicz and
               Ian Rae and
               Traian Stancescu and
               Himani Apte},
  title   = {{F1:} {A} Distributed {SQL} Database That Scales},
  journal = {Very Large Data Bases (VLDB)},
  volume  = {6},
  number  = {11},
  pages   = {1068--1079},
  year    = {2013}
}

@article{2013-tocs-spanner,
  author  = {James C. Corbett and
               Jeffrey Dean and
               Michael Epstein and
               Andrew Fikes and
               Christopher Frost and
               J. J. Furman and
               Sanjay Ghemawat and
               Andrey Gubarev and
               Christopher Heiser and
               Peter Hochschild and
               Wilson C. Hsieh and
               Sebastian Kanthak and
               Eugene Kogan and
               Hongyi Li and
               Alexander Lloyd and
               Sergey Melnik and
               David Mwaura and
               David Nagle and
               Sean Quinlan and
               Rajesh Rao and
               Lindsay Rolig and
               Yasushi Saito and
               Michal Szymaniak and
               Christopher Taylor and
               Ruth Wang and
               Dale Woodford},
  title   = {Spanner: Google's Globally Distributed Database},
  journal = {{ACM} Transactions Computer Systems},
  volume  = {31},
  number  = {3},
  pages   = {8:1--8:22},
  year    = {2013}
}

@inproceedings{2013-atc-tao,
  author    = {Nathan Bronson and
               Zach Amsden and
               George Cabrera and
               Prasad Chakka and
               Peter Dimov and
               Hui Ding and
               Jack Ferris and
               Anthony Giardullo and
               Sachin Kulkarni and
               Harry C. Li and
               Mark Marchukov and
               Dmitri Petrov and
               Lovro Puzar and
               Yee Jiun Song and
               Venkateshwaran Venkataramani},
  title     = {{TAO:} Facebook's Distributed Data Store for the Social Graph},
  booktitle = {{USENIX} Annual Technical Conference (ATC)},
  pages     = {49--60},
  year      = {2013}
}

@inproceedings{2019-bigbdata-aves,
  title     = {Aves: A Framework for Energy-efficient Stream Analytics across Low-power Devices},
  author    = {{Bharath Das}, Roshan and Marc Makkes and Alexandru Uta and Lin Wang and Henri Bal},
  year      = {2019},
  month     = {12},
  booktitle = {IEEE Big Data}
}

@inproceedings{2019-nsdi-simon,
  author    = {Yilong Geng and Shiyu Liu and Zi Yin and Ashish Naik and Balaji Prabhakar and Mendel Rosenblum and Amin Vahdat},
  title     = {{SIMON}: A Simple and Scalable Method for Sensing, Inference and Measurement in Data Center Networks},
  booktitle = {{USENIX} Symposium on Networked Systems Design and Implementation ({NSDI})},
  year      = {2019},
  pages     = {549--564}
}

@article{2019-vldb-slog,
  author  = {Kun Ren and
               Dennis Li and
               Daniel J. Abadi},
  title   = {{SLOG:} Serializable, Low-latency, Geo-replicated Transactions},
  journal = {{Very Large Data Bases (VLDB)}},
  volume  = {12},
  number  = {11},
  pages   = {1747--1761},
  year    = {2019}
}

@inproceedings{2018-edbt-gplacer,
  author    = {Victor Zakhary and
               Faisal Nawab and
               Divy Agrawal and
               Amr {El Abbadi}},
  title     = {Global-Scale Placement of Transactional Data Stores},
  booktitle = {International Conference on Extending Database
               Technology (EDBT)},
  pages     = {385--396},
  year      = {2018}
}

@article{2010-osr-cassandra,
  author  = {Avinash Lakshman and
               Prashant Malik},
  title   = {Cassandra: a decentralized structured storage system},
  journal = {ACM SIGOPS Operating Systems Review},
  volume  = {44},
  number  = {2},
  pages   = {35--40},
  year    = {2010}
}

@inproceedings{2007-pdsw-rados,
  author    = {Sage A. Weil and
               Andrew W. Leung and
               Scott A. Brandt and
               Carlos Maltzahn},
  title     = {{RADOS:} a scalable, reliable storage service for petabyte-scale storage clusters},
  booktitle = {ACM International Petascale Data Storage Workshop (PDSW)},
  pages     = {35--44},
  year      = {2007}
}

@inproceedings{2018-sec-cavbench,
  author    = { Yifan Wang and Shaoshan Liu and Xiaopei Wu and Weisong Shi },
  booktitle = {IEEE/ACM Symposium on Edge Computing (SEC)},
  title     = {CAVBench: A Benchmark Suite for Connected and Autonomous Vehicles},
  year      = {2018},
  volume    = {},
  number    = {},
  pages     = {30-42}
}

@inproceedings{2018-sec-safeshareride,
  author    = {L. {Liu} and X. {Zhang} and M. {Qiao} and W. {Shi}},
  booktitle = {IEEE/ACM Symposium on Edge Computing (SEC)},
  title     = {SafeShareRide: Edge-Based Attack Detection in Ridesharing Services},
  year      = {2018},
  volume    = {},
  number    = {},
  pages     = {17-29}
}

@inproceedings{2014-icra-driving-fusion,
  author    = {Hyunggi Cho and Young-Woo Seo and B.V.K. Vijaya Kumar and Ragunathan (Raj) Rajkumar},
  booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
  title     = {A multi-sensor fusion system for moving object detection and tracking in urban driving environments},
  year      = {2014},
  volume    = {},
  number    = {},
  pages     = {1836-1843}
}

@article{2014-sprinter-hybrid-game,
  author  = {Choy, Sharon and Wong, Bernard and Simon, Gwendal and Rosenberg, Catherine},
  title   = {A Hybrid Edge-Cloud Architecture for Reducing on-Demand Gaming Latency},
  year    = {2014},
  volume  = {20},
  number  = {5},
  journal = {Multimedia Systems},
  pages   = {503–519}
}
  
@article{2014-csur-gaming,
  author  = {Yahyavi, Amir and Kemme, Bettina},
  title   = {Peer-to-Peer Architectures for Massively Multiplayer Online Games: A Survey},
  year    = {2013},
  volume  = {46},
  number  = {1},
  journal = {ACM Computing Surveys}
}
 

@article{2014-arxiv-ipfs,
  author  = {Juan Benet},
  title   = {{IPFS} - Content Addressed, Versioned, {P2P} File System},
  journal = {CoRR},
  volume  = {abs/1407.3561},
  year    = {2014}
}

@inproceedings{2011-asplos-declarative-pci,
author = {Sch\"{u}pbach, Adrian and Baumann, Andrew and Roscoe, Timothy and Peter, Simon},
title = {A Declarative Language Approach to Device Configuration},
year = {2011},
isbn = {9781450302661},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1950365.1950382},
doi = {10.1145/1950365.1950382},
abstract = {C remains the language of choice for hardware programming (device drivers, bus configuration, etc.): it is fast, allows low-level access, and is trusted by OS developers. However, the algorithms required to configure and reconfigure hardware devices and interconnects are becoming more complex and diverse, with the added burden of legacy support, quirks, and hardware bugs to work around. Even programming PCI bridges in a modern PC is a surprisingly complex problem, and is getting worse as new functionality such as hotplug appears. Existing approaches use relatively simple algorithms, hard-coded in C and closely coupled with low-level register access code, generally leading to suboptimal configurations.We investigate the merits and drawbacks of a new approach: separating hardware configuration logic (algorithms to determine configuration parameter values) from mechanism (programming device registers). The latter we keep in C, and the former we encode in a declarative programming language with constraint-satisfaction extensions. As a test case, we have implemented full PCI configuration, resource allocation, and interrupt assignment in the Barrelfish research operating system, using a concise expression of efficient algorithms in constraint logic programming. We show that the approach is tractable, and can successfully configure a wide range of PCs with competitive runtime cost. Moreover, it requires about half the code of the C-based approach in Linux while offering considerably more functionality. Additionally it easily accommodates adaptations such as hotplug, fixed regions, and quirks.},
booktitle = {Proceedings of the Sixteenth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {119–132},
numpages = {14},
keywords = {pci configuration, eclipse clp, hardware programming, constraint logic programming},
location = {Newport Beach, California, USA},
series = {ASPLOS XVI}
}

@inproceedings{2011-springer-crdt,
  author    = {Shapiro, Marc and Pregui\c{c}a, Nuno and Baquero, Carlos and Zawirski, Marek},
  title     = {Conflict-Free Replicated Data Types},
  year      = {2011},
  booktitle = {International Conference on Stabilization, Safety, and Security of Distributed Systems (SSS)},
  pages     = {386–400}
}
  
@inproceedings{2011-cidr-megastore,
  author    = {Jason Baker and
               Chris Bond and
               James C. Corbett and
               J. J. Furman and
               Andrey Khorlin and
               James Larson and
               Jean{-}Michel Leon and
               Yawei Li and
               Alexander Lloyd and
               Vadim Yushprakh},
  title     = {Megastore: Providing Scalable, Highly Available Storage for Interactive Services},
  booktitle = {Biennial Conference on Innovative Data Systems
               Research (CIDR)},
  pages     = {223--234},
  year      = {2011}
}

@inproceedings{2018-socc-kurma,
  author    = {Kirill Bogdanov and
               Waleed Reda and
               Gerald Q. Maguire Jr. and
               Dejan Kostic and
               Marco Canini},
  title     = {Fast and Accurate Load Balancing for Geo-Distributed Storage Systems},
  booktitle = {{ACM} Symposium on Cloud Computing (SoCC)},
  pages     = {386--400},
  publisher = {{ACM}},
  year      = {2018}
}

@inproceedings{2018-iot-clock-sync,
  author    = {Mani, Sathiya Kumaran and Durairajan, Ramakrishnan and Barford, Paul and Sommers, Joel},
  title     = {An Architecture for IoT Clock Synchronization},
  year      = {2018},
  booktitle = {International Conference on the Internet of Things (IOT)}
}
  
@inproceedings{2017-podc-edge-ecodes,
  author    = {Konwar, Kishori M. and Prakash, N. and Lynch, Nancy and M\'{e}dard, Muriel},
  title     = {A Layered Architecture for Erasure-Coded Consistent Distributed Storage},
  year      = {2017},
  booktitle = {ACM Symposium on Principles of Distributed Computing (PODC)},
  pages     = {63–72}
}
  
@inproceedings{2017-mecomm-connected-vehciles,
  author    = {Grewe, Dennis and Wagner, Marco and Arumaithurai, Mayutan and Psaras, Ioannis and Kutscher, Dirk},
  title     = {Information-Centric Mobile Edge Computing for Connected Vehicle Environments: Challenges and Research Directions},
  year      = {2017},
  booktitle = {ACM Workshop on Mobile Edge Communications (MECOMM)},
  pages     = {7–12}
}

@inproceedings{2017-sec-traffic,
  author    = {Kar, Gorkem and Jain, Shubham and Gruteser, Marco and Bai, Fan and Govindan, Ramesh},
  title     = {Real-Time Traffic Estimation at Vehicular Edge Nodes},
  year      = {2017},
  booktitle = {ACM/IEEE Symposium on Edge Computing (SEC)}
}
  
@inproceedings{2017-asplos-ns,
  author    = {Kang, Yiping and Hauswald, Johann and Gao, Cao and Rovinski, Austin and Mudge, Trevor and Mars, Jason and Tang, Lingjia},
  title     = {Neurosurgeon: Collaborative Intelligence Between the Cloud and Mobile Edge},
  year      = {2017},
  booktitle = {International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)},
  pages     = {615–629}
}

@article{2017-tpds-fog-gaming,
  author  = { Yuhua Lin and Haiying Shen },
  journal = {IEEE Transactions on Parallel and Distributed Systems},
  title   = {CloudFog: Leveraging Fog to Extend Cloud Gaming for Thin-Client MMOG with High Quality of Service},
  year    = {2017},
  volume  = {28},
  number  = {2},
  pages   = {431-445}
}

@inproceedings{2017-hash-hthash,
  author    = {Xuefeng Guan and
               Cheng Bo and
               Zhenqiang Li and
               Yaojin Yu},
  title     = {ST-hash: An efficient spatiotemporal index for massive trajectory
               data in a NoSQL database},
  booktitle = {International Conference on Geoinformatics, Geoinformatics},
  pages     = {1--7},
  year      = {2017}
}

@book{2012-hash-hilbert,
  title     = {Space-filling curves},
  author    = {Sagan, Hans},
  year      = {2012},
  publisher = {Springer Science \& Business Media}
}

@inproceedings{2012-sensys-tracking,
  author    = {A. B. M. Musa and
               Jakob Eriksson},
  title     = {Tracking unmodified smartphones using wi-fi monitors},
  booktitle = {{ACM} Conference on Embedded Network Sensor Systems (SenSys)},
  pages     = {281--294},
  year      = {2012}
}

@inproceedings{2014-infocom-trakcing,
  author    = {Ying Zhang},
  title     = {User mobility from the view of cellular data networks},
  booktitle = {{IEEE} Conference on Computer Communications (INFOCOM)},
  pages     = {1348--1356},
  year      = {2014}
}

@article{2013-tmc-mobility,
  author  = {Addison Chan and
               Frederick W. B. Li},
  title   = {Utilizing Massive Spatiotemporal Samples for Efficient and Accurate Trajectory Prediction},
  journal = {{IEEE} Transactions on Mobible Computing},
  volume  = {12},
  number  = {12},
  pages   = {2346--2359},
  year    = {2013}
}

@article{2013-cst-vne,
  author  = {Andreas Fischer and
               Juan Felipe Botero and
               Michael Till Beck and
               Hermann de Meer and
               Xavier Hesselbach},
  title   = {Virtual Network Embedding: {A} Survey},
  journal = {{IEEE} Communications Surveys and Tutorials},
  volume  = {15},
  number  = {4},
  pages   = {1888--1906},
  year    = {2013}
}

@article{2019-ton-vne,
  author  = {Matthias Rost and Stefan Schmid},
  title   = {Virtual Network Embedding Approximations: Leveraging Randomized Rounding},
  journal = {{IEEE/ACM} Transactions on Networking},
  volume  = {27},
  number  = {5},
  pages   = {2071--2084},
  year    = {2019}
}

@inproceedings{2019-middleware-cep,
  author    = {Manisha Luthra and
               Sebastian Hennig and
               Pratyush Agnihotri and
               Lin Wang and
               Boris Koldehofe},
  title     = {Highly Flexible Server Agnostic Complex Event Processing Operators},
  booktitle = {ACM International Middleware Conference (Middleware)},
  pages     = {11--12},
  year      = {2019}
}

@inproceedings{2018-aaai-cell,
  author    = {Sandeep Chinchali and
               Pan Hu and
               Tianshu Chu and
               Manu Sharma and
               Manu Bansal and
               Rakesh Misra and
               Marco Pavone and
               Sachin Katti},
  title     = {Cellular Network Traffic Scheduling With Deep Reinforcement Learning},
  booktitle = {{AAAI} Conference on Artificial Intelligence (AAAI)},
  pages     = {766--774},
  year      = {2018}
}

@inproceedings{2016-eurosys-posix,
author = {Atlidakis, Vaggelis and Andrus, Jeremy and Geambasu, Roxana and Mitropoulos, Dimitris and Nieh, Jason},
title = {POSIX Abstractions in Modern Operating Systems: The Old, the New, and the Missing},
year = {2016},
isbn = {9781450342407},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2901318.2901350},
doi = {10.1145/2901318.2901350},
abstract = {The POSIX standard, developed 25 years ago, comprises a set of operating system (OS) abstractions that aid application portability across UNIX-based OSes. While OSes and applications have evolved tremendously over the last 25 years, POSIX, and the basic set of abstractions it provides, has remained largely unchanged. Little has been done to measure how and to what extent traditional POSIX abstractions are being used in modern OSes, and whether new abstractions are taking form, dethroning traditional ones. We explore these questions through a study of POSIX usage in modern desktop and mobile OSes: Android, OS X, and Ubuntu. Our results show that new abstractions are taking form, replacing several prominent traditional abstractions in POSIX. While the changes are driven by common needs and are conceptually similar across the three OSes, they are not converging on any new standard, increasing fragmentation.},
booktitle = {Proceedings of the Eleventh European Conference on Computer Systems},
articleno = {19},
numpages = {17},
location = {London, United Kingdom},
series = {EuroSys '16}
}

@inproceedings{2016-debs-foglet,
  author    = {Enrique Saurez and
               Kirak Hong and
               Dave Lillethun and
               Umakishore Ramachandran and
               Beate Ottenwaelder},
  title     = {Incremental deployment and migration of geo-distributed situation
               awareness applications in the fog},
  booktitle = {{ACM} International Conference on Distributed
               and Event-based Systems (DEBS)},
  pages     = {258--269},
  year      = {2016}
}

@inproceedings{2016-can-databox,
  author    = {Richard Mortier and
               Jianxin R. Zhao and
               Jon Crowcroft and
               Liang Wang and
               Qi Li and
               Hamed Haddadi and
               Yousef Amar and
               Andy Crabtree and
               James A. Colley and
               Tom Lodge and
               Tosh Brown and
               Derek McAuley and
               Chris Greenhalgh},
  title     = {Personal Data Management with the Databox: What's Inside the Box?},
  booktitle = {{ACM} Workshop on Cloud-Assisted Networking (CAN)},
  pages     = {49--54},
  year      = {2016}
}

@article{2020-iotj-vu,
  author  = {Hui Sun and
               Weisong Shi and
               Xu Liang and
               Ying Yu},
  title   = {{VU:} Edge Computing-Enabled Video Usefulness Detection and its Application in Large-Scale Video Surveillance Systems},
  journal = {{IEEE} Internet of Things Journal},
  volume  = {7},
  number  = {2},
  pages   = {800--817},
  year    = {2020}
}

@article{2019-ieee-special-issue-video,
  author  = {Ganesh Ananthanarayanan and
               Weisong Shi},
  title   = {Live Video Analytics},
  journal = {{IEEE} Internet Computing},
  volume  = {23},
  number  = {4},
  pages   = {8--9},
  year    = {2019}
}

@inproceedings{2019-sec-quartz,
  author    = {D’souza, Sandeep and Koehler, Heiko and Joshi, Akhilesh and Vaghani, Satyam and Rajkumar, Ragunathan (Raj)},
  title     = {Quartz: Time-as-a-Service for Coordination in Geo-Distributed Systems},
  year      = {2019},
  booktitle = {ACM/IEEE Symposium on Edge Computing (SEC)},
  pages     = {264–279}
}

@inproceedings{2019-sec-app-fail,
  author    = {Nguyen, Chanh and Mehta, Amardeep and Klein, Cristian and Elmroth, Erik},
  title     = {Why Cloud Applications Are Not Ready for the Edge (Yet)},
  year      = {2019},
  booktitle = {ACM/IEEE Symposium on Edge Computing (SEC)},
  pages     = {250–263}
}
  
@article{2019-pieee-driving,
  author  = {Shaoshan Liu and
               Liangkai Liu and
               Jie Tang and
               Bo Yu and
               Yifan Wang and
               Weisong Shi},
  title   = {Edge Computing for Autonomous Driving: Opportunities and Challenges},
  journal = {Proceedings of the {IEEE}},
  volume  = {107},
  number  = {8},
  pages   = {1697--1716},
  year    = {2019}
}

@inproceedings{2018-nsdi-clock,
  author    = {Yilong Geng and Shiyu Liu and Zi Yin and Ashish Naik and Balaji Prabhakar and Mendel Rosenblum and Amin Vahdat},
  title     = {Exploiting a Natural Network Effect for Scalable, Fine-grained Clock Synchronization},
  booktitle = {{USENIX} Symposium on Networked Systems Design and Implementation (NSDI)},
  year      = {2018},
  pages     = {81--94}
}

@inproceedings{2018-sec-edgebox,
  author    = {Bing Luo and
               Sheng Tan and
               Zhifeng Yu and
               Weisong Shi},
  title     = {EdgeBox: Live Edge Video Analytics for Near Real-Time Event Detection},
  booktitle = {{IEEE/ACM} Symposium on Edge Computing (SEC)},
  pages     = {347--348},
  year      = {2018}
}

@inproceedings{2017-sec-parkmaster,
  author    = {Giulio Grassi and
               Kyle Jamieson and
               Paramvir Bahl and
               Giovanni Pau},
  title     = {Parkmaster: an in-vehicle, edge-based video analytics service for
               detecting open parking spaces in urban environments},
  booktitle = {{ACM/IEEE} Symposium on Edge Computing (SEC)},
  pages     = {16:1--16:14},
  year      = {2017}
}

@inproceedings{2018-infocom-item,
  author    = {Lin Wang and
               Lei Jiao and
               Ting He and
               Jun Li and
               Max M{\"{u}}hlh{\"{a}}user},
  title     = {Service Entity Placement for Social Virtual Reality Applications in Edge Computing},
  booktitle = {{IEEE} International Conference on Computer Communications ({INFOCOM})},
  pages     = {468--476},
  year      = {2018}
}


@inproceedings{safespec,
author = {Khasawneh, Khaled N. and Koruyeh, Esmaeil Mohammadian and Song, Chengyu and Evtyushkin, Dmitry and Ponomarev, Dmitry and Abu-Ghazaleh, Nael},
title = {{SafeSpec: Banishing the Spectre of a Meltdown with Leakage-Free Speculation}},
year = {2019},
isbn = {9781450367257},
publisher = {ACM},

url = {https://doi.org/10.1145/3316781.3317903},
doi = {10.1145/3316781.3317903},
booktitle = {Proceedings of the 56th Annual Design Automation Conference 2019},
articleno = {60},
numpages = {6},
address = {Las Vegas, NV, USA},
series = {DAC ’19}
}

@inproceedings{invisispec,
author = {Yan, Mengjia and Choi, Jiho and Skarlatos, Dimitrios and Morrison, Adam and Fletcher, Christopher W. and Torrellas, Josep},
title = {{InvisiSpec: Making Speculative Execution Invisible in the Cache Hierarchy}},
year = {2018},
isbn = {9781538662403},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO.2018.00042},
doi = {10.1109/MICRO.2018.00042},
booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {428–441},
numpages = {14},
address = {Fukuoka, Japan},
series = {MICRO-51}
}

@inproceedings{eccploit,
	title = {Exploiting {Correcting} {Codes}: {On} the {Effectiveness} of {ECC} {Memory} {Against} {Rowhammer} {Attacks}},
	NoUrl = {Paper=https://download.vusec.net/papers/eccploit_sp19.pdf Slides=https://www.ieee-security.org/TC/SP2019/SP19-Slides-pdfs/Lucian_Cojocar_Exploiting_Correcting_Codes_slides-ecc-new.pdf Web=https://www.vusec.net/projects/eccploit},
	booktitle={2019 IEEE Symposium on Security and Privacy (S\&P)},
	author = {Cojocar, Lucian and Razavi, Kaveh and Giuffrida, Cristiano and Bos, Herbert},
	month = may,
	year = {2019},
	NoNote = {Best Practical Paper Award, Pwnie Award Nomination for Most Innovative Research},
	keywords = {class\_rowhammer, type\_conf, type\_top, type\_ast, type\_award, proj\_panta, proj\_react, proj\_unicore, type\_tier1, type\_press, proj\_vici}
}

@INPROCEEDINGS {netspectre,
    author = {Michael Schwarz and Martin Schwarzl and Moritz Lipp and Jon Masters and Daniel Gruss},
    title = {{NetSpectre: Read Arbitrary Memory over Network}},
    booktitle={European Symposium on Research in Computer Security},
    year = {2019},
    series = {ESORICS'19},
}

@INPROCEEDINGS {context-ndss,
    author = {Michael Schwarz and Moritz Lipp and Claudio Canella and Robert Schilling and Florian Kargl and Daniel Gruss},
    title = {{ConTExT: A Generic Approach for Mitigating Spectre}},
    booktitle = {Network and Distributed Systems Security (NDSS) Symposium 2020},
    year={2020},
    series = {NDSS'20},
}

@INPROCEEDINGS {absynthe,
    author = {Ben Gras and Cristiano Giuffrida and Michael Kurth and Herbert Bos and Kaveh Razavi},
    title = {{ABSynthe: Automatic Blackbox Side-channel Synthesis on Commodity Microarchitectures}},
    booktitle = {Network and Distributed Systems Security (NDSS) Symposium 2020},
    year={2020},
    series = {NDSS'20},
}

@inproceedings{smotherspectre,
author = {Bhattacharyya, Atri and Sandulescu, Alexandra and Neugschwandtner, Matthias and Sorniotti, Alessandro and Falsafi, Babak and Payer, Mathias and Kurmus, Anil},
title = {{SMoTherSpectre: Exploiting Speculative Execution through Port Contention}},
year = {2019},
isbn = {9781450367479},
publisher = {ACM},

url = {https://doi.org/10.1145/3319535.3363194},
doi = {10.1145/3319535.3363194},
booktitle = {Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security},
pages = {785–800},
numpages = {16},
keywords = {speculative execution, attack, microarchitecture, side-channel, simultaneous multithreading},
address = {London, United Kingdom},
series = {CCS ’19}
}
  
@inproceedings{ridl,
title = {{RIDL}: Rogue In-flight Data Load},
booktitle={2019 IEEE Symposium on Security and Privacy (S\&P)},
author = {van Schaik, Stephan and Milburn, Alyssa and Österlund, Sebastian and Frigo, Pietro and Maisuradze, Giorgi and Razavi, Kaveh and Bos, Herbert and Giuffrida, Cristiano},
month = may,
pages={88-105},
year = {2019},
}

@inproceedings{trrespass,
	title = {{TRRespass}: {Exploiting} the {Many} {Sides} of {Target} {Row} {Refresh}},
	NoUrl = {Paper=https://download.vusec.net/papers/trrespass_sp20.pdf Web=https://www.vusec.net/projects/trrespass Code=https://github.com/vusec/trrespass},	
	booktitle={2020 IEEE Symposium on Security and Privacy (S\&P)},
	author = {Frigo, Pietro and Vannacci, Emanuele and Hassan, Hasan and van der Veen, Victor and Mutlu, Onur and Giuffrida, Cristiano and Bos, Herbert and Razavi, Kaveh},
	month = may,
	year = {2020},
	keywords = {class\_rowhammer, type\_conf, type\_top, proj\_panta, proj\_react, proj\_unicore, type\_tier1, type\_press, proj\_vici}
}

@inproceedings{vanbulck2018foreshadow,
    author = {Van Bulck, Jo and Minkin, Marina and Weisse, Ofir and Genkin, Daniel and Kasikci, Baris and
              Piessens, Frank and Silberstein, Mark and Wenisch, Thomas F. and Yarom, Yuval and Strackx, Raoul},
    title = {{Foreshadow: Extracting the Keys to the {Intel SGX} Kingdom with Transient Out-of-Order Execution}},
    booktitle = {Proceedings of the 27th {USENIX} Security Symposium},
    year = {2018},
    month = {August},
    
}

@inproceedings{resource_containers,
author = {Banga, Gaurav and Druschel, Peter and Mogul, Jeffrey C.},
title = {{Resource Containers: A New Facility for Resource Management in Server Systems}},
year = {1999},
isbn = {1880446391},

booktitle = {Proceedings of the Third Symposium on Operating Systems Design and Implementation},
pages = {45–58},
numpages = {14},
address = {New Orleans, Louisiana, USA},
series = {OSDI ’99}
}
 

@phdthesis{2012-skb-thesis,
    title    = {Tackling OS Complexity with Declarative Techniques},
    school   = {ETH Zurich},
    author   = { Sch\"upbach, Adrian L.},
    year     = {2012},
    note = {\nolinkurl{https://www.research-collection.ethz.ch/handle/20.500.11850/61055}}, 
}

@INPROCEEDINGS {portsmash,    
    author = {Alejandro Cabrera Aldaya and Billy Bob Brumley and  Sohaib ul Hassan and Cesar Pereida García and Nicola Tuveri},  
    title = {{Port Contention for Fun and Profit}},    
    booktitle={2019 IEEE Symposium on Security and Privacy (S\&P)},
    year={2019},
  pages={870-887},
}

@inproceedings{2019-asplos-ubench,
author = {Gan, Yu and Zhang, Yanqi and Cheng, Dailun and Shetty, Ankitha and Rathi, Priyal and Katarki, Nayan and Bruno, Ariana and Hu, Justin and Ritchken, Brian and Jackson, Brendon and Hu, Kelvin and Pancholi, Meghna and He, Yuan and Clancy, Brett and Colen, Chris and Wen, Fukang and Leung, Catherine and Wang, Siyuan and Zaruvinsky, Leon and Espinosa, Mateo and Lin, Rick and Liu, Zhongling and Padilla, Jake and Delimitrou, Christina},
title = {{An Open-Source Benchmark Suite for Microservices and Their Hardware-Software Implications for Cloud \& Edge Systems}},
year = {2019},
isbn = {9781450362405},
publisher = {ACM},

url = {https://doi.org/10.1145/3297858.3304013},
doi = {10.1145/3297858.3304013},
booktitle = {Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {3–18},
numpages = {16},
keywords = {microservices, cluster management, datacenters, cloud computing, acceleration, qos, serverless, fpga},
address = {Providence, RI, USA},
series = {ASPLOS ’19}
}
  
@inproceedings{2019-hotos-inc-armed,
author = {Benson, Theophilus A.},
title = {{In-Network Compute: Considered Armed and Dangerous}},
year = {2019},
isbn = {9781450367271},
publisher = {ACM},

url = {https://doi.org/10.1145/3317550.3321436},
doi = {10.1145/3317550.3321436},
booktitle = {Proceedings of the Workshop on Hot Topics in Operating Systems},
pages = {216–224},
numpages = {9},
keywords = {in-network computing, programmable network devices},
address = {Bertinoro, Italy},
series = {HotOS ’19}
}
  
@inproceedings{2019-atc-zanzibar,
author = {Pang, Ruoming and C\'{a}ceres, Ram\'{o}n and Burrows, Mike and Chen, Zhifeng and Dave, Pratik and Germer, Nathan and Golynski, Alexander and Graney, Kevin and Kang, Nina and Kissner, Lea and et al.},
title = {{Zanzibar: Google’s Consistent, Global Authorization System}},
year = {2019},
isbn = {9781939133038},

booktitle = {Proceedings of the 2019 USENIX Conference on Usenix Annual Technical Conference},
pages = {33–46},
numpages = {14},
address = {Renton, WA, USA},
series = {USENIX ATC ’19}
}

@INPROCEEDINGS{ye2014coloris,
  author={Ye, Ying and West, Richard and Cheng, Zhuoqun and Li, Ye},  
  booktitle={2014 23rd International Conference on Parallel Architecture and Compilation Techniques (PACT)},
  title={{COLORIS: A Dynamic Cache Partitioning System Using Page Coloring}},   
  series={PACT'14},  
  year={2014},
  pages={381-392},
}

@article{opennebula,
author = {Milojicic, Dejan and Llorente, Ignacio M. and Montero, Ruben S.},
title = {OpenNebula: A Cloud Management Tool},
year = {2011},
issue_date = {March 2011},
publisher = {IEEE Educational Activities Department},
address = {USA},
volume = {15},
number = {2},
issn = {1089-7801},
url = {https://doi.org/10.1109/MIC.2011.44},
doi = {10.1109/MIC.2011.44},
journal = {IEEE Internet Computing},
month = mar,
pages = {11–14},
numpages = {4},
keywords = {OpenNebula, cloud computing},
}

@article{Li2019-dc-energy,
  doi = {10.1007/s11227-019-03036-9},
  url = {https://doi.org/10.1007/s11227-019-03036-9},
  year = {2019},
  month = oct,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {76},
  number = {1},
  pages = {481--498},
  author = {Hongjian Li and Yuyan Zhao and Shuyong Fang},
  title = {{CSL}-driven and energy-efficient resource scheduling in cloud data center},
  journal = {The Journal of Supercomputing}
}

@article{dc-growth,
author = {Eric Masanet  and Arman Shehabi  and Nuoa Lei  and Sarah Smith  and Jonathan Koomey },
title = {Recalibrating global data center energy-use estimates},
journal = {Science},
volume = {367},
number = {6481},
pages = {984-986},
year = {2020},
doi = {10.1126/science.aba3758},
URL = {https://www.science.org/doi/abs/10.1126/science.aba3758},
eprint = {https://www.science.org/doi/pdf/10.1126/science.aba3758},
abstract = {Growth in energy use has slowed owing to efficiency gains that smart policies can help maintain in the near term Data centers represent the information backbone of an increasingly digitalized world. Demand for their services has been rising rapidly (1), and data-intensive technologies such as artificial intelligence, smart and connected energy systems, distributed manufacturing systems, and autonomous vehicles promise to increase demand further (2). Given that data centers are energy-intensive enterprises, estimated to account for around 1\% of worldwide electricity use, these trends have clear implications for global energy demand and must be analyzed rigorously. Several oft-cited yet simplistic analyses claim that the energy used by the world's data centers has doubled over the past decade and that their energy use will triple or even quadruple within the next decade (3–5). Such estimates contribute to a conventional wisdom (5, 6) that as demand for data center services rises rapidly, so too must their global energy use. But such extrapolations based on recent service demand growth indicators overlook strong countervailing energy efficiency trends that have occurred in parallel (see the first figure). Here, we integrate new data from different sources that have emerged recently and suggest more modest growth in global data center energy use (see the second figure). This provides policy-makers and energy analysts a recalibrated understanding of global data center energy use, its drivers, and near-term efficiency potential.}
}


@misc{ddio_overview2012, 
	title={{Intel Data Direct I/O Technology Overview}},
	author={Intel.},
	year={2012}, howpublished={\nolinkurl{https://www.intel.co.jp/content/dam/www/public/us/en/documents/white-papers/data-direct-i-o-technology-overview-paper.pdf}}, 
	note={Accessed: 2019-05-24}
}

@misc{2019-checklist, 
	title={{A Checklist Manifesto for Empirical Evaluation: A Preemptive Strike Against a Replication Crisis in Computer Science}},
	author={Emery D. Berger and Stephen M. Blackburn and Matthias Hauswirth and Michael W. Hicks},
	year={2019}, 
	howpublished={\nolinkurl{https://blog.sigplan.org/2019/08/28/a-checklist-manifesto-for-empirical-evaluation-a-preemptive-strike-against-a-replication-crisis-in-computer-science/}}, 
	note={Accessed: 2019-05-24}
}

@article{2016-fairdata,
  doi = {10.1038/sdata.2016.18},
  url = {https://doi.org/10.1038/sdata.2016.18},
  year = {2016},
  month = mar,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {3},
  number = {1},
  author = {Mark D. Wilkinson and Michel Dumontier and IJsbrand Jan Aalbersberg and Gabrielle Appleton and Myles Axton and Arie Baak and Niklas Blomberg and Jan-Willem Boiten and Luiz Bonino da Silva Santos and Philip E. Bourne and Jildau Bouwman and Anthony J. Brookes and Tim Clark and Merc{\`{e}} Crosas and Ingrid Dillo and Olivier Dumon and Scott Edmunds and Chris T. Evelo and Richard Finkers and Alejandra Gonzalez-Beltran and Alasdair J.G. Gray and Paul Groth and Carole Goble and Jeffrey S. Grethe and Jaap Heringa and Peter A.C 't Hoen and Rob Hooft and Tobias Kuhn and Ruben Kok and Joost Kok and Scott J. Lusher and Maryann E. Martone and Albert Mons and Abel L. Packer and Bengt Persson and Philippe Rocca-Serra and Marco Roos and Rene van Schaik and Susanna-Assunta Sansone and Erik Schultes and Thierry Sengstag and Ted Slater and George Strawn and Morris A. Swertz and Mark Thompson and Johan van der Lei and Erik van Mulligen and Jan Velterop and Andra Waagmeester and Peter Wittenburg and Katherine Wolstencroft and Jun Zhao and Barend Mons},
  title = {The {FAIR} Guiding Principles for scientific data management and stewardship},
  journal = {Scientific Data}
}

@ARTICLE{1996-ieee-nemesis, 
author={I. M. {Leslie} and D. {McAuley} and R. {Black} and T. {Roscoe} and P. {Barham} and D. {Evers} and R. {Fairbairns} and E. {Hyden}}, 
journal={IEEE Journal on Selected Areas in Communications}, 
title={The design and implementation of an operating system to support distributed multimedia applications}, 
year={1996}, 
volume={14}, 
number={7}, 
pages={1280-1297}, 
keywords={multimedia communication;multimedia computing;network operating systems;distributed processing;operating system;distributed multimedia applications;multimedia applications;general purpose computing platforms;resources multiplexing;CPU;design;hardware platforms;Operating systems;Multimedia computing;Hardware}, 
doi={10.1109/49.536480}, 
ISSN={0733-8716}, 
month={Sep.},}

@inproceedings{1997-rtas-os-cache,
 author = {Liedtke, Jochen and Haertig, Hermann and Hohmuth, Michael},
 title = {OS-Controlled Cache Predictability for Real-Time Systems},
 booktitle = {Proceedings of the 3rd IEEE Real-Time Technology and Applications Symposium (RTAS '97)},
 series = {RTAS '97},
 year = {1997},
 isbn = {0-8186-8016-4},
 pages = {213--},
 url = {http://dl.acm.org/citation.cfm?id=523983.828369},
 acmid = {828369},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{1998-asplos-isolation,
 author = {Verghese, Ben and Gupta, Anoop and Rosenblum, Mendel},
 title = {Performance Isolation: Sharing and Isolation in Shared-memory Multiprocessors},
 booktitle = {Proceedings of the Eighth International Conference on Architectural Support for Programming Languages and Operating Systems},
 series = {ASPLOS VIII},
 year = {1998},
 isbn = {1-58113-107-0},
 address = {San Jose, California, USA},
 pages = {181--192},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/291069.291044},
 doi = {10.1145/291069.291044},
 acmid = {291044},
 publisher = {ACM},
 
} 

@article{1999-sccr-prog-net,
 author = {Campbell, Andrew T. and De Meer, Herman G. and Kounavis, Michael E. and Miki, Kazuho and Vicente, John B. and Villela, Daniel},
 title = {A Survey of Programmable Networks},
 journal = {SIGCOMM Comput. Commun. Rev.},
 issue_date = {April 1999},
 volume = {29},
 number = {2},
 month = {April},
 year = {1999},
 issn = {0146-4833},
 pages = {7--23},
 numpages = {17},
 url = {http://doi.acm.org/10.1145/505733.505735},
 doi = {10.1145/505733.505735},
 acmid = {505735},
 publisher = {ACM},
 
} 

@inproceedings{2000-osdi-kaffeeos,
 author = {Back, Godmar and Hsieh, Wilson C. and Lepreau, Jay},
 title = {Processes in KaffeOS: Isolation, Resource Management, and Sharing in Java},
 booktitle = {Proceedings of the 4th Conference on Symposium on Operating System Design \& Implementation - Volume 4},
 series = {OSDI'00},
 year = {2000},
 address = {San Diego, California},
 articleno = {23},
 url = {http://dl.acm.org/citation.cfm?id=1251229.1251252},
 acmid = {1251252},
 
 
} 

@inproceedings{2003-sosp-infokernel,
 author = {Arpaci-Dusseau, Andrea C. and Arpaci-Dusseau, Remzi H. and Burnett, Nathan C. and Denehy, Timothy E. and Engle, Thomas J. and Gunawi, Haryadi S. and Nugent, James A. and Popovici, Florentina I.},
 title = {Transforming Policies into Mechanisms with Infokernel},
 booktitle = {Proceedings of the Nineteenth ACM Symposium on Operating Systems Principles},
 series = {SOSP '03},
 year = {2003},
 isbn = {1-58113-757-5},
 address = {Bolton Landing, NY, USA},
 pages = {90--105},
 numpages = {16},
 url = {http://doi.acm.org/10.1145/945445.945455},
 doi = {10.1145/945445.945455},
 acmid = {945455},
 publisher = {ACM},
 
 keywords = {information, mechanism, policy},
} 

@InProceedings{2003-ieee-des-cache,
author="Tsunoo, Yukiyasu
and Saito, Teruo
and Suzaki, Tomoyasu
and Shigeri, Maki
and Miyauchi, Hiroshi",
editor="Walter, Colin D.
and Ko{\c{c}}, {\c{C}}etin K.
and Paar, Christof",
title="Cryptanalysis of DES Implemented on Computers with Cache",
booktitle="Cryptographic Hardware and Embedded Systems - CHES 2003",
year="2003",
publisher="Springer Berlin Heidelberg",
address="Cologne, Germany",
pages="62--76",
abstract="This paper presents the results of applying an attack against the Data Encryption Standard (DES) implemented in some applications, using side-channel information based on CPU delay as proposed in [11]. This cryptanalysis technique uses side-channel information on encryption processing to select and collect effective plaintexts for cryptanalysis, and infers the information on the expanded key from the collected plaintexts. On applying this attack, we found that the cipher can be broken with 223 known plaintexts and 224 calculations at a success rate > 90{\%}, using a personal computer with 600-MHz Pentium III.",
isbn="978-3-540-45238-6"
}

@inproceedings{2006-ctrsa-aes,
 author = {Osvik, Dag Arne and Shamir, Adi and Tromer, Eran},
 title = {{Cache Attacks and Countermeasures: The Case of AES}},
 booktitle = {Proceedings of the 2006 The Cryptographers' Track at the RSA Conference on Topics in Cryptology},
 series = {CT-RSA'06},
 year = {2006},
 isbn = {3-540-31033-9, 978-3-540-31033-4},
 address = {San Jose, CA},
 pages = {1--20},
 numpages = {20},
 url = {http://dx.doi.org/10.1007/11605805_1},
 doi = {10.1007/11605805_1},
 acmid = {2117741},
 publisher = {Springer-Verlag},
 keywords = {AES, cache, cryptanalysis, memory access, side-channel attack},
} 

@inproceedings{2006-micro-cpart-utility,
 author = {Qureshi, Moinuddin K. and Patt, Yale N.},
 title = {Utility-Based Cache Partitioning: A Low-Overhead, High-Performance, Runtime Mechanism to Partition Shared Caches},
 booktitle = {Proceedings of the 39th Annual IEEE/ACM International Symposium on Microarchitecture},
 series = {MICRO 39},
 year = {2006},
 isbn = {0-7695-2732-9},
 pages = {423--432},
 numpages = {10},
 url = {https://doi.org/10.1109/MICRO.2006.49},
 doi = {10.1109/MICRO.2006.49},
 acmid = {1194855},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{2007-nsdi-vm,
 author = {Wood, Timothy and Shenoy, Prashant and Venkataramani, Arun and Yousif, Mazin},
 title = {Black-box and Gray-box Strategies for Virtual Machine Migration},
 booktitle = {Proceedings of the 4th USENIX Conference on Networked Systems Design \&\#38; Implementation},
 series = {NSDI'07},
 year = {2007},
 address = {Cambridge, MA},
 pages = {17--17},
 numpages = {1},
 url = {http://dl.acm.org/citation.cfm?id=1973430.1973447},
 acmid = {1973447},
 
 
} 

@inproceedings{2008-asplos-hydra,
 author = {Weinsberg, Yaron and Dolev, Danny and Anker, Tal and Ben-Yehuda, Muli and Wyckoff, Pete},
 title = {{Tapping into the Fountain of CPUs: On Operating System Support for Programmable Devices}},
 booktitle = {Proceedings of the 13th International Conference on Architectural Support for Programming Languages and Operating Systems},
 series = {ASPLOS XIII},
 year = {2008},
 isbn = {978-1-59593-958-6},
 address = {Seattle, WA, USA},
 pages = {179--188},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1346281.1346304},
 doi = {10.1145/1346281.1346304},
 acmid = {1346304},
 publisher = {ACM},
 
 keywords = {offloading, operating systems, programming model},
} 

@Article{2008-xxx-declarative-infra,
author="Narain, Sanjai
and Levin, Gary
and Malik, Sharad
and Kaul, Vikram",
title="{Declarative Infrastructure Configuration Synthesis and Debugging}",
journal="Journal of Network and Systems Management",
year="2008",
month="Sep",
day="01",
volume="16",
number="3",
pages="235--258",
abstract="There is a large conceptual gap between end-to-end infrastructure requirements and detailed component configuration implementing those requirements. Today, this gap is manually bridged so large numbers of configuration errors are made. Their adverse effects on infrastructure security, availability, and cost of ownership are well documented. This paper presents ConfigAssure to help automatically bridge the above gap. It proposes solutions to four fundamental problems: specification, configuration synthesis, configuration error diagnosis, and configuration error repair. Central to ConfigAssure is a Requirement Solver. It takes as input a configuration database containing variables, and a requirement as a first-order logic constraint in finite domains. The Solver tries to compute as output, values for variables that make the requirement true of the database when instantiated with these values. If unable to do so, it computes a proof of unsolvability. The Requirement Solver is used in different ways to solve the above problems. The Requirement Solver is implemented with Kodkod, a SAT-based model finder for first-order logic. While any requirement can be directly encoded in Kodkod, parts of it can often be solved much more efficiently by non model-finding methods using information available in the configuration database. Solving these parts and simplifying can yield a reduced constraint that truly requires the power of model-finding. To implement this plan, a quantifier-free form, QFF, is defined. A QFF is a Boolean combination of simple arithmetic constraints on integers. A requirement is specified by defining a partial evaluator that transforms it into an equivalent QFF. This QFF is efficiently solved by Kodkod. The partial evaluator is implemented in Prolog. ConfigAssure is shown to be natural and scalable in the context of a realistic, secure and fault-tolerant datacenter.",
issn="1573-7705",
doi="10.1007/s10922-008-9108-y",
url="https://doi.org/10.1007/s10922-008-9108-y"
}

@inproceedings{2016-ross-gpurdma,
author = {Daoud, Feras and Watad, Amir and Silberstein, Mark},
title = {GPUrdma: GPU-Side Library for High Performance Networking from GPU Kernels},
year = {2016},
isbn = {9781450343879},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.vu-nl.idm.oclc.org/10.1145/2931088.2931091},
doi = {10.1145/2931088.2931091},
abstract = {We present GPUrdma, a GPU-side library for performing Remote Direct Memory Accesses (RDMA) across the network directly from GPU kernels. The library executes no code on CPU, directly accessing the Host Channel Adapter (HCA) Infiniband hardware for both control and data. Slow single-thread GPU performance and the intricacies of the GPU-to-network adapter interaction pose a significant challenge. We describe several design options and analyze their performance implications in detail.We achieve 5μsec one-way communication latency and up to 50Gbit/sec transfer bandwidth for messages from 16KB and larger between K40c NVIDIA GPUs across the network. Moreover, GPUrdma outperforms the CPU RDMA for smaller packets ranging from 2 to 1024 bytes by factor of 4.5x thanks to greater parallelism of transfer requests enabled by highly parallel GPU hardware.We use GPUrdma to implement a subset of the global address space programming interface (GPI) for point-to-point asynchronous RDMA messaging. We demonstrate our preliminary results using two simple applications -- ping-pong and a multi-matrix-vector product with constant matrix and multiple vectors -- each running on two different machines connected by Infiniband. Our basic ping-pong implementation achieves 5%higher performance than the baseline using GPI-2. The improved ping-pong implementation with per-threadblock communication overlap enables further 20% improvement. The multi-matrix-vector product is up to 4.5x faster thanks to higher throughput for small messages and the ability to keep the matrix in fast GPU shared memory while receiving new inputs.GPUrdma prototype is not yet suitable for production systems due to hardware constraints in the current generation of NVIDIA GPUs which we discuss in detail. However, our results highlight the great potential of GPU-side native networking, and encourage further research toward scalable, high-performance, heterogeneous networking infrastructure.},
booktitle = {Proceedings of the 6th International Workshop on Runtime and Operating Systems for Supercomputers},
articleno = {6},
numpages = {8},
keywords = {Networking, accelerators, Operating Systems Design, GPGPUs},
location = {Kyoto, Japan},
series = {ROSS '16}
}

@inproceedings {2014-osdi-gpunet,
author = {Sangman Kim and Seonggu Huh and Xinya Zhang and Yige Hu and Amir Wated and Emmett Witchel and Mark Silberstein},
title = {{GPUnet}: Networking Abstractions for {GPU} Programs},
booktitle = {11th USENIX Symposium on Operating Systems Design and Implementation (OSDI 14)},
year = {2014},
isbn = { 978-1-931971-16-4},
address = {Broomfield, CO},
pages = {201--216},
url = {https://www.usenix.org/conference/osdi14/technical-sessions/presentation/kim},
publisher = {USENIX Association},
month = oct,
}

@inproceedings {2018-nsdi-azure,
author = {Daniel Firestone and Andrew Putnam and Sambhrama Mundkur and Derek Chiou and Alireza Dabagh and Mike Andrewartha and Hari Angepat and Vivek Bhanu and Adrian Caulfield and Eric Chung and Harish Kumar Chandrappa and Somesh Chaturmohta and Matt Humphrey and Jack Lavier and Norman Lam and Fengfen Liu and Kalin Ovtcharov and Jitu Padhye and Gautham Popuri and Shachar Raindel and Tejas Sapre and Mark Shaw and Gabriel Silva and Madhan Sivakumar and Nisheeth Srivastava and Anshuman Verma and Qasim Zuhair and Deepak Bansal and Doug Burger and Kushagra Vaid and David A. Maltz and Albert Greenberg},
title = {Azure Accelerated Networking: {SmartNICs} in the Public Cloud},
booktitle = {15th USENIX Symposium on Networked Systems Design and Implementation (NSDI 18)},
year = {2018},
isbn = {978-1-939133-01-4},
address = {Renton, WA},
pages = {51--66},
url = {https://www.usenix.org/conference/nsdi18/presentation/firestone},
publisher = {USENIX Association},
month = apr,
}

@inproceedings {2019-atc-nica,
author = {Haggai Eran and Lior Zeno and Maroun Tork and Gabi Malka and Mark Silberstein},
title = {{NICA}: An Infrastructure for Inline Acceleration of Network Applications},
booktitle = {2019 USENIX Annual Technical Conference (USENIX ATC 19)},
year = {2019},
isbn = {978-1-939133-03-8},
address = {Renton, WA},
pages = {345--362},
url = {https://www.usenix.org/conference/atc19/presentation/eran},
publisher = {USENIX Association},
month = jul,
}

@article{2017-tos-spin,
author = {Bergman, Shai and Brokhman, Tanya and Cohen, Tzachi and Silberstein, Mark},
title = {SPIN: Seamless Operating System Integration of Peer-to-Peer DMA Between SSDs and GPUs},
year = {2019},
issue_date = {May 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {2},
issn = {0734-2071},
url = {https://doi.org/10.1145/3309987},
doi = {10.1145/3309987},
abstract = {Recent GPUs enable Peer-to-Peer Direct Memory Access (p2p) from fast peripheral devices like NVMe SSDs to exclude the CPU from the data path between them for efficiency. Unfortunately, using p2p to access files is challenging because of the subtleties of low-level non-standard interfaces, which bypass the OS file I/O layers and may hurt system performance. Developers must possess intimate knowledge of low-level interfaces to manually handle the subtleties of data consistency and misaligned accesses.We present SPIN, which integrates p2p into the standard OS file I/O stack, dynamically activating p2p where appropriate, transparently to the user. It combines p2p with page cache accesses, re-enables read-ahead for sequential reads, all while maintaining standard POSIX FS consistency, portability across GPUs and SSDs, and compatibility with virtual block devices such as software RAID.We evaluate SPIN on NVIDIA and AMD GPUs using standard file I/O benchmarks, application traces, and end-to-end experiments. SPIN achieves significant performance speedups across a wide range of workloads, exceeding p2p throughput by up to an order of magnitude. It also boosts the performance of an aerial imagery rendering application by 2.6\texttimes{} by dynamically adapting to its input-dependent file access pattern, enables 3.3\texttimes{} higher throughput for a GPU-accelerated log server, and enables 29% faster execution for the highly optimized GPU-accelerated image collage with only 30 changed lines of code.},
journal = {ACM Trans. Comput. Syst.},
month = {apr},
articleno = {5},
numpages = {26},
keywords = {GPU, I/O subsystem, Accelerators, file systems, operating systems}
}

@inproceedings{2017-isca-tpu,
author = {Jouppi, Norman P. and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and Boyle, Rick and Cantin, Pierre-luc and Chao, Clifford and Clark, Chris and Coriell, Jeremy and Daley, Mike and Dau, Matt and Dean, Jeffrey and Gelb, Ben and Ghaemmaghami, Tara Vazir and Gottipati, Rajendra and Gulland, William and Hagmann, Robert and Ho, C. Richard and Hogberg, Doug and Hu, John and Hundt, Robert and Hurt, Dan and Ibarz, Julian and Jaffey, Aaron and Jaworski, Alek and Kaplan, Alexander and Khaitan, Harshit and Killebrew, Daniel and Koch, Andy and Kumar, Naveen and Lacy, Steve and Laudon, James and Law, James and Le, Diemthu and Leary, Chris and Liu, Zhuyuan and Lucke, Kyle and Lundin, Alan and MacKean, Gordon and Maggiore, Adriana and Mahony, Maire and Miller, Kieran and Nagarajan, Rahul and Narayanaswami, Ravi and Ni, Ray and Nix, Kathy and Norrie, Thomas and Omernick, Mark and Penukonda, Narayana and Phelps, Andy and Ross, Jonathan and Ross, Matt and Salek, Amir and Samadiani, Emad and Severn, Chris and Sizikov, Gregory and Snelham, Matthew and Souter, Jed and Steinberg, Dan and Swing, Andy and Tan, Mercedes and Thorson, Gregory and Tian, Bo and Toma, Horia and Tuttle, Erick and Vasudevan, Vijay and Walter, Richard and Wang, Walter and Wilcox, Eric and Yoon, Doe Hyun},
title = {In-Datacenter Performance Analysis of a Tensor Processing Unit},
year = {2017},
isbn = {9781450348928},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3079856.3080246},
doi = {10.1145/3079856.3080246},
abstract = {Many architects believe that major improvements in cost-energy-performance must now come from domain-specific hardware. This paper evaluates a custom ASIC---called a Tensor Processing Unit (TPU) --- deployed in datacenters since 2015 that accelerates the inference phase of neural networks (NN). The heart of the TPU is a 65,536 8-bit MAC matrix multiply unit that offers a peak throughput of 92 TeraOps/second (TOPS) and a large (28 MiB) software-managed on-chip memory. The TPU's deterministic execution model is a better match to the 99th-percentile response-time requirement of our NN applications than are the time-varying optimizations of CPUs and GPUs that help average throughput more than guaranteed latency. The lack of such features helps explain why, despite having myriad MACs and a big memory, the TPU is relatively small and low power. We compare the TPU to a server-class Intel Haswell CPU and an Nvidia K80 GPU, which are contemporaries deployed in the same datacenters. Our workload, written in the high-level TensorFlow framework, uses production NN applications (MLPs, CNNs, and LSTMs) that represent 95% of our datacenters' NN inference demand. Despite low utilization for some applications, the TPU is on average about 15X -- 30X faster than its contemporary GPU or CPU, with TOPS/Watt about 30X -- 80X higher. Moreover, using the CPU's GDDR5 memory in the TPU would triple achieved TOPS and raise TOPS/Watt to nearly 70X the GPU and 200X the CPU.},
booktitle = {Proceedings of the 44th Annual International Symposium on Computer Architecture},
pages = {1–12},
numpages = {12},
keywords = {domain-specific architecture, neural network, MLP, RNN, accelerator, deep learning, LSTM, GPU, CNN, TPU, DNN, TensorFlow},
location = {Toronto, ON, Canada},
series = {ISCA '17}
}

@inproceedings{2008-atc-ssd-tradeoff,
 author = {Agrawal, Nitin and Prabhakaran, Vijayan and Wobber, Ted and Davis, John D. and Manasse, Mark and Panigrahy, Rina},
 title = {{Design Tradeoffs for SSD Performance}},
 booktitle = {Proceedings of the USENIX 2008 Annual Technical Conference},
 series = {ATC'08},
 year = {2008},
 address = {Boston, Massachusetts},
 pages = {57--70},
 numpages = {14},
 url = {http://dl.acm.org/citation.cfm?id=1404014.1404019},
 acmid = {1404019},
 
 
} 

@article{10.1145/2644865.2541959,
author = {Ouyang, Jian and Lin, Shiding and Jiang, Song and Hou, Zhenyu and Wang, Yong and Wang, Yuanzheng},
title = {SDF: Software-Defined Flash for Web-Scale Internet Storage Systems},
year = {2014},
issue_date = {April 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/2644865.2541959},
doi = {10.1145/2644865.2541959},
abstract = {In the last several years hundreds of thousands of SSDs have been deployed in the data centers of Baidu, China's largest Internet search company. Currently only 40% or less of the raw bandwidth of the flash memory in the SSDs is delivered by the storage system to the applications. Moreover, because of space over-provisioning in the SSD to accommodate non-sequential or random writes, and additionally, parity coding across flash channels, typically only 50-70% of the raw capacity of a commodity SSD can be used for user data. Given the large scale of Baidu's data center, making the most effective use of its SSDs is of great importance. Specifically, we seek to maximize both bandwidth and usable capacity.To achieve this goal we propose {em software-defined flash} (SDF), a hardware/software co-designed storage system to maximally exploit the performance characteristics of flash memory in the context of our workloads. SDF exposes individual flash channels to the host software and eliminates space over-provisioning. The host software, given direct access to the raw flash channels of the SSD, can effectively organize its data and schedule its data access to better realize the SSD's raw performance potential.Currently more than 3000 SDFs have been deployed in Baidu's storage system that supports its web page and image repository services. Our measurements show that SDF can deliver approximately 95% of the raw flash bandwidth and provide 99% of the flash capacity for user data. SDF increases I/O bandwidth by 300% and reduces per-GB hardware cost by 50% on average compared with the commodity-SSD-based system used at Baidu.},
journal = {SIGPLAN Not.},
month = {feb},
pages = {471–484},
numpages = {14},
keywords = {flash memory, data center, solid-state drive(ssd)}
}

@inproceedings {2000-osdi-devil,
author = {Fabrice M{\'e}rillon and Laurent R{\'e}veill{\`e}re and Charles Consel and Renaud Marlet and Gilles Muller},
title = {Devil: An {IDL} for Hardware Programming},
booktitle = {Fourth Symposium on Operating Systems Design and Implementation (OSDI 2000)},
year = {2000},
address = {San Diego, CA },
url = {https://www.usenix.org/conference/osdi-2000/devil-idl-hardware-programming},
publisher = {USENIX Association},
month = oct,
}


@inproceedings{2014-eurosys-ocssd-kv,
author = {Wang, Peng and Sun, Guangyu and Jiang, Song and Ouyang, Jian and Lin, Shiding and Zhang, Chen and Cong, Jason},
title = {An Efficient Design and Implementation of LSM-Tree Based Key-Value Store on Open-Channel SSD},
year = {2014},
isbn = {9781450327046},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2592798.2592804},
doi = {10.1145/2592798.2592804},
abstract = {Various key-value (KV) stores are widely employed for data management to support Internet services as they offer higher efficiency, scalability, and availability than relational database systems. The log-structured merge tree (LSM-tree) based KV stores have attracted growing attention because they can eliminate random writes and maintain acceptable read performance. Recently, as the price per unit capacity of NAND flash decreases, solid state disks (SSDs) have been extensively adopted in enterprise-scale data centers to provide high I/O bandwidth and low access latency. However, it is inefficient to naively combine LSM-tree-based KV stores with SSDs, as the high parallelism enabled within the SSD cannot be fully exploited. Current LSM-tree-based KV stores are designed without assuming SSD's multi-channel architecture.To address this inadequacy, we propose LOCS, a system equipped with a customized SSD design, which exposes its internal flash channels to applications, to work with the LSM-tree-based KV store, specifically LevelDB in this work. We extend LevelDB to explicitly leverage the multiple channels of an SSD to exploit its abundant parallelism. In addition, we optimize scheduling and dispatching polices for concurrent I/O requests to further improve the efficiency of data access. Compared with the scenario where a stock LevelDB runs on a conventional SSD, the throughput of storage system can be improved by more than 4X after applying all proposed optimization techniques.},
booktitle = {Proceedings of the Ninth European Conference on Computer Systems},
articleno = {16},
numpages = {14},
keywords = {solid state disk, log-structured merge tree, flash, key-value store},
location = {Amsterdam, The Netherlands},
series = {EuroSys '14}
}

@inproceedings {2017-fast-flashbox,
author = {Jian Huang and Anirudh Badam and Laura Caulfield and Suman Nath and Sudipta Sengupta and Bikash Sharma and Moinuddin K. Qureshi},
title = {{FlashBlox}: Achieving Both Performance Isolation and Uniform Lifetime for Virtualized {SSDs}},
booktitle = {15th USENIX Conference on File and Storage Technologies (FAST 17)},
year = {2017},
isbn = {978-1-931971-36-2},
address = {Santa Clara, CA},
pages = {375--390},
url = {https://www.usenix.org/conference/fast17/technical-sessions/presentation/huang},
publisher = {USENIX Association},
month = feb,
}

@inproceedings{2017-ieee-cgra,
title = "Coarse grained reconfigurable architectures in the past 25 years: overview and classification",
abstract = "Reconfigurable architectures become more popular now general purpose compute performance does not increase as rapidly as before. Field programmable gate arrays are slowly moving into the direction of Coarse Grain Reconfigurable Architectures (CGRA) by adding DSP and other coarse grained IP blocks, general purpose processors become more heterogeneous and include sub-word parallelism and even some reconfigurable logic. In the past 25 years, several CGRAs have been published. In this paper an overview and classification of these architectures is presented. This work also provides a clear definition of CGRAs and identifies topics for future research which are key to unlock the full potential of CGRAs.",
keywords = "CGRA, Classification, Coarse Grain Reconfigurable Architecture",
author = "M. Wijtvliet and L. Waeijen and H. Corporaal",
year = "2017",
month = jan,
day = "13",
doi = "10.1109/SAMOS.2016.7818353",
language = "English",
pages = "235--244",
booktitle = "Proceedings - 2016 16th International Conference on Embedded Computer Systems: Architectures, Modeling and Simulation, SAMOS 2016",
publisher = "Institute of Electrical and Electronics Engineers",
address = "United States",
note = "16th International Conference on Embedded Computer Systems: Architectures, Modeling and Simulation (SAMOS 2016), 18-21 July 2016, Samos, Greece, SAMOS2016 ; Conference date: 18-07-2016 Through 21-07-2016",
url = "http://www.wikicfp.com/cfp/servlet/event.showcfp?eventid=50851&copyownerid=84389",
}


@inproceedings{2020-ieee-fletcher-arrow-fpga,
  author    = {Johan Peltenburg and
               Lars T. J. van Leeuwen and
               Joost Hoozemans and
               Jian Fang and
               Zaid Al{-}Ars and
               H. Peter Hofstee},
  title     = {Battling the {CPU} Bottleneck in Apache Parquet to Arrow Conversion
               Using {FPGA}},
  booktitle = {International Conference on Field-Programmable Technology, {(IC)FPT}
               2020, Maui, HI, USA, December 9-11, 2020},
  pages     = {281--286},
  publisher = {{IEEE}},
  year      = {2020},
  url       = {https://doi.org/10.1109/ICFPT51103.2020.00048},
  doi       = {10.1109/ICFPT51103.2020.00048},
  timestamp = {Tue, 11 May 2021 10:41:53 +0200},
  biburl    = {https://dblp.org/rec/conf/icfpt/PeltenburgLH0AH20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings {2018-fast-spiffy,
author = {Kuei Sun and Daniel Fryer and Joseph Chu and Matthew Lakier and Angela Demke Brown and Ashvin Goel},
title = {Spiffy: Enabling {File-System} Aware Storage Applications},
booktitle = {16th USENIX Conference on File and Storage Technologies (FAST 18)},
year = {2018},
isbn = {978-1-931971-42-3},
address = {Oakland, CA},
pages = {91--104},
url = {https://www.usenix.org/conference/fast18/presentation/sun},
publisher = {USENIX Association},
month = feb,
}

@inproceedings{2017-sosp-strata,
author = {Kwon, Youngjin and Fingler, Henrique and Hunt, Tyler and Peter, Simon and Witchel, Emmett and Anderson, Thomas},
title = {Strata: A Cross Media File System},
year = {2017},
isbn = {9781450350853},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132747.3132770},
doi = {10.1145/3132747.3132770},
abstract = {Current hardware and application storage trends put immense pressure on the operating system's storage subsystem. On the hardware side, the market for storage devices has diversified to a multi-layer storage topology spanning multiple orders of magnitude in cost and performance. Above the file system, applications increasingly need to process small, random IO on vast data sets with low latency, high throughput, and simple crash consistency. File systems designed for a single storage layer cannot support all of these demands together.We present Strata, a cross-media file system that leverages the strengths of one storage media to compensate for weaknesses of another. In doing so, Strata provides performance, capacity, and a simple, synchronous IO model all at once, while having a simpler design than that of file systems constrained by a single storage device. At its heart, Strata uses a log-structured approach with a novel split of responsibilities among user mode, kernel, and storage layers that separates the concerns of scalable, high-performance persistence from storage layer management. We quantify the performance benefits of Strata using a 3-layer storage hierarchy of emulated NVM, a flash-based SSD, and a high-density HDD. Strata has 20-30% better latency and throughput, across several unmodified applications, compared to file systems purpose-built for each layer, while providing synchronous and unified access to the entire storage hierarchy. Finally, Strata achieves up to 2.8x better throughput than a block-based 2-layer cache provided by Linux's logical volume manager.},
booktitle = {Proceedings of the 26th Symposium on Operating Systems Principles},
pages = {460–477},
numpages = {18},
keywords = {Multi-layer storage, Non-volatile memory, File system},
location = {Shanghai, China},
series = {SOSP '17}
}

@inproceedings{2016-eurosys-guided-placement,
author = {Dulloor, Subramanya R. and Roy, Amitabha and Zhao, Zheguang and Sundaram, Narayanan and Satish, Nadathur and Sankaran, Rajesh and Jackson, Jeff and Schwan, Karsten},
title = {Data Tiering in Heterogeneous Memory Systems},
year = {2016},
isbn = {9781450342407},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2901318.2901344},
doi = {10.1145/2901318.2901344},
abstract = {Memory-based data center applications require increasingly large memory capacities, but face the challenges posed by the inherent difficulties in scaling DRAM and also the cost of DRAM. Future systems are attempting to address these demands with heterogeneous memory architectures coupling DRAM with high capacity, low cost, but also lower performance, non-volatile memories (NVM) such as PCM and RRAM. A key usage model intended for NVM is as cheaper high capacity volatile memory. Data center operators are bound to ask whether this model for the usage of NVM to replace the majority of DRAM memory leads to a large slowdown in their applications? It is crucial to answer this question because a large performance impact will be an impediment to the adoption of such systems.This paper presents a thorough study of representative applications -- including a key-value store (MemC3), an in-memory database (VoltDB), and a graph analytics framework (GraphMat) -- on a platform that is capable of emulating a mix of memory technologies. Our conclusions are that it is indeed possible to use a mix of a small amount of fast DRAM and large amounts of slower NVM without a proportional impact to an application's performance. The caveat is that this result can only be achieved through careful placement of data structures. The contribution of this paper is the design and implementation of a set of libraries and automatic tools that enables programmers to achieve optimal data placement with minimal effort on their part.With such guided placement and with DRAM constituting only 6% of the total memory footprint for GraphMat and 25% for VoltDB and MemC3 (remaining memory is NVM with 4x higher latency and 8x lower bandwidth than DRAM), we show that our target applications demonstrate only a 13% to 40% slowdown. Without guided placement, these applications see, in the worst case, 1.5x to 5.9x slowdown on the same configuration. Based on a realistic assumption that NVM will be 5x cheaper (per bit) than DRAM, this hybrid solution also results in 2x to 2.8x better performance/$ than a DRAM-only system.},
booktitle = {Proceedings of the Eleventh European Conference on Computer Systems},
articleno = {15},
numpages = {16},
location = {London, United Kingdom},
series = {EuroSys '16}
}

@article{2021-cacm-serverless-future,
author = {Schleier-Smith, Johann and Sreekanti, Vikram and Khandelwal, Anurag and Carreira, Joao and Yadwadkar, Neeraja J. and Popa, Raluca Ada and Gonzalez, Joseph E. and Stoica, Ion and Patterson, David A.},
title = {What Serverless Computing is and Should Become: The next Phase of Cloud Computing},
year = {2021},
issue_date = {May 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {64},
number = {5},
issn = {0001-0782},
url = {https://doi.org/10.1145/3406011},
doi = {10.1145/3406011},
abstract = {The evolution that serverless computing represents, the economic forces that shape it, why it could fail, and how it might fulfill its potential.},
journal = {Commun. ACM},
month = {apr},
pages = {76–84},
numpages = {9}
}

@article{2020-sigmetrics-optimal-placement,
author = {Zhang, Lei and Karimi, Reza and Ahmad, Irfan and Vigfusson, Ymir},
title = {Optimal Data Placement for Heterogeneous Cache, Memory, and Storage Systems},
year = {2020},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
url = {https://doi.org/10.1145/3379472},
doi = {10.1145/3379472},
journal = {Proc. ACM Meas. Anal. Comput. Syst.},
month = {may},
articleno = {06},
numpages = {27},
keywords = {cost-aware cache replacement, spatial sampling, offline optimal analysis, memory hierarchy, non-volatile memory, data placement}
}

@inproceedings {2022-osdi-xrp,
author = {Yuhong Zhong and Haoyu Li and Yu Jian Wu and Ioannis Zarkadas and Jeffrey Tao and Evan Mesterhazy and Michael Makris and Junfeng Yang and Amy Tai and Ryan Stutsman and Asaf Cidon},
title = {{XRP}: {In-Kernel} Storage Functions with {eBPF}},
booktitle = {16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)},
year = {2022},
isbn = {978-1-939133-28-1},
address = {Carlsbad, CA},
pages = {375--393},
url = {https://www.usenix.org/conference/osdi22/presentation/zhong},
publisher = {USENIX Association},
month = jul,
}

@inproceedings {2021-fast-orthus,
author = {Kan Wu and Zhihan Guo and Guanzhou Hu and Kaiwei Tu and Ramnatthan Alagappan and Rathijit Sen and Kwanghyun Park and Andrea C. Arpaci-Dusseau and Remzi H. Arpaci-Dusseau},
title = {The Storage Hierarchy is Not a Hierarchy: Optimizing Caching on Modern Storage Devices with Orthus},
booktitle = {19th USENIX Conference on File and Storage Technologies (FAST 21)},
year = {2021},
isbn = {978-1-939133-20-5},
pages = {307--323},
url = {https://www.usenix.org/conference/fast21/presentation/wu-kan},
publisher = {USENIX Association},
month = feb,
}

@inproceedings{2018-sc-dc-scheduling,
author = {Andreadis, Georgios and Versluis, Laurens and Mastenbroek, Fabian and Iosup, Alexandru},
title = {A Reference Architecture for Datacenter Scheduling: Design, Validation, and Experiments},
year = {2018},
publisher = {IEEE Press},
abstract = {Datacenters act as cloud-infrastructure to stakeholders across industry, government, and academia. To meet growing demand yet operate efficiently, datacenter operators employ increasingly more sophisticated scheduling systems, mechanisms, and policies. Although many scheduling techniques already exist, relatively little research has gone into the abstraction of the scheduling process itself, hampering design, tuning, and comparison of existing techniques. In this work, we propose a reference architecture for datacenter schedulers. The architecture follows five design principles: components with clearly distinct responsibilities, grouping of related components where possible, separation of mechanism from policy, scheduling as complex workflow, and hierarchical multi-scheduler structure. To demonstrate the validity of the reference architecture, we map to it state-of-the-art datacenter schedulers. We find scheduler-stages are commonly underspecified in peer-reviewed publications. Through trace-based simulation and real-world experiments, we show underspecification of scheduler-stages can lead to significant variations in performance.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage, and Analysis},
articleno = {37},
numpages = {15},
keywords = {scheduling, reference architecture, datacenter},
location = {Dallas, Texas},
series = {SC '18}
}

@inproceedings{2017-eurosys-unwritten,
author = {He, Jun and Kannan, Sudarsun and Arpaci-Dusseau, Andrea C. and Arpaci-Dusseau, Remzi H.},
title = {The Unwritten Contract of Solid State Drives},
year = {2017},
isbn = {9781450349383},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.vu-nl.idm.oclc.org/10.1145/3064176.3064187},
doi = {10.1145/3064176.3064187},
abstract = {We perform a detailed vertical analysis of application performance atop a range of modern file systems and SSD FTLs. We formalize the "unwritten contract" that clients of SSDs should follow to obtain high performance, and conduct our analysis to uncover application and file system designs that violate the contract. Our analysis, which utilizes a highly detailed SSD simulation underneath traces taken from real workloads and file systems, provides insight into how to better construct applications, file systems, and FTLs to realize robust and sustainable performance.},
booktitle = {Proceedings of the Twelfth European Conference on Computer Systems},
pages = {127–144},
numpages = {18},
location = {Belgrade, Serbia},
series = {EuroSys '17}
}

@inproceedings {2021-atc-zns,
author = {Matias Bj{\o}rling and Abutalib Aghayev and Hans Holmberg and Aravind Ramesh and Damien Le Moal and Gregory R. Ganger and George Amvrosiadis},
title = {{ZNS}: Avoiding the Block Interface Tax for Flash-based {SSDs}},
booktitle = {2021 USENIX Annual Technical Conference (USENIX ATC 21)},
year = {2021},
isbn = {978-1-939133-23-6},
pages = {689--703},
url = {https://www.usenix.org/conference/atc21/presentation/bjorling},
publisher = {USENIX Association},
month = jul,
}

@inproceedings{2009-xxx-uflip,
  author    = {Luc Bouganim and
               Bj{\"{o}}rn {\TH}{\'{o}}r J{\'{o}}nsson and
               Philippe Bonnet},
  title     = {uFLIP: Understanding Flash {IO} Patterns},
  booktitle = {Fourth Biennial Conference on Innovative Data Systems Research, {CIDR}
               2009, Asilomar, CA, USA, January 4-7, 2009, Online Proceedings},
  publisher = {www.cidrdb.org},
  year      = {2009},
  url       = {http://www-db.cs.wisc.edu/cidr/cidr2009/Paper\_102.pdf},
  timestamp = {Mon, 18 Jul 2022 17:13:00 +0200},
  biburl    = {https://dblp.org/rec/conf/cidr/BouganimJB09.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{2022-cidr-daphne,
  author    = {Patrick Damme and
               Marius Birkenbach and
               Constantinos Bitsakos and
               Matthias Boehm and
               Philippe Bonnet and
               Florina M. Ciorba and
               Mark Dokter and
               Pawel Dowgiallo and
               Ahmed Eleliemy and
               Christian Faerber and
               Georgios I. Goumas and
               Dirk Habich and
               Niclas Hedam and
               Marlies Hofer and
               Wenjun Huang and
               Kevin Innerebner and
               Vasileios Karakostas and
               Roman Kern and
               Tomaz Kosar and
               Alexander Krause and
               Daniel Krems and
               Andreas Laber and
               Wolfgang Lehner and
               Eric Mier and
               Marcus Paradies and
               Bernhard Peischl and
               Gabrielle Poerwawinata and
               Stratos Psomadakis and
               Tilmann Rabl and
               Piotr Ratuszniak and
               Pedro Silva and
               Nikolai Skuppin and
               Andreas Starzacher and
               Benjamin Steinwender and
               Ilin Tolovski and
               Pinar T{\"{o}}z{\"{u}}n and
               Wojciech Ulatowski and
               Yuanyuan Wang and
               Izajasz P. Wrosz and
               Ales Zamuda and
               Ce Zhang and
               Xiaoxiang Zhu},
  title     = {{DAPHNE:} An Open and Extensible System Infrastructure for Integrated
               Data Analysis Pipelines},
  booktitle = {12th Conference on Innovative Data Systems Research, {CIDR} 2022,
               Chaminade, CA, USA, January 9-12, 2022},
  publisher = {www.cidrdb.org},
  year      = {2022},
  url       = {https://www.cidrdb.org/cidr2022/papers/p4-damme.pdf},
  timestamp = {Mon, 18 Jul 2022 17:13:00 +0200},
  biburl    = {https://dblp.org/rec/conf/cidr/DammeBB0BCDDEFG22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{2014-asplos-sdf,
author = {Ouyang, Jian and Lin, Shiding and Jiang, Song and Hou, Zhenyu and Wang, Yong and Wang, Yuanzheng},
title = {SDF: Software-Defined Flash for Web-Scale Internet Storage Systems},
year = {2014},
isbn = {9781450323055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541940.2541959},
doi = {10.1145/2541940.2541959},
abstract = {In the last several years hundreds of thousands of SSDs have been deployed in the data centers of Baidu, China's largest Internet search company. Currently only 40% or less of the raw bandwidth of the flash memory in the SSDs is delivered by the storage system to the applications. Moreover, because of space over-provisioning in the SSD to accommodate non-sequential or random writes, and additionally, parity coding across flash channels, typically only 50-70% of the raw capacity of a commodity SSD can be used for user data. Given the large scale of Baidu's data center, making the most effective use of its SSDs is of great importance. Specifically, we seek to maximize both bandwidth and usable capacity.To achieve this goal we propose {em software-defined flash} (SDF), a hardware/software co-designed storage system to maximally exploit the performance characteristics of flash memory in the context of our workloads. SDF exposes individual flash channels to the host software and eliminates space over-provisioning. The host software, given direct access to the raw flash channels of the SSD, can effectively organize its data and schedule its data access to better realize the SSD's raw performance potential.Currently more than 3000 SDFs have been deployed in Baidu's storage system that supports its web page and image repository services. Our measurements show that SDF can deliver approximately 95% of the raw flash bandwidth and provide 99% of the flash capacity for user data. SDF increases I/O bandwidth by 300% and reduces per-GB hardware cost by 50% on average compared with the commodity-SSD-based system used at Baidu.},
booktitle = {Proceedings of the 19th International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {471–484},
numpages = {14},
keywords = {data center, flash memory, solid-state drive(ssd)},
location = {Salt Lake City, Utah, USA},
series = {ASPLOS '14}
}


@inproceedings{2009-sosp-helios,
 author = {Nightingale, Edmund B. and Hodson, Orion and McIlroy, Ross and Hawblitzel, Chris and Hunt, Galen},
 title = {{Helios: Heterogeneous Multiprocessing with Satellite Kernels}},
 booktitle = {Proceedings of the ACM SIGOPS 22Nd Symposium on Operating Systems Principles},
 series = {SOSP '09},
 year = {2009},
 isbn = {978-1-60558-752-3},
 address = {Big Sky, Montana, USA},
 pages = {221--234},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1629575.1629597},
 doi = {10.1145/1629575.1629597},
 acmid = {1629597},
 publisher = {ACM},
 
 keywords = {heterogeneous computing, operating systems},
} 

@inproceedings{2009-sosp-bf,
 author = {Baumann, Andrew and Barham, Paul and Dagand, Pierre-Evariste and Harris, Tim and Isaacs, Rebecca and Peter, Simon and Roscoe, Timothy and Sch\"{u}pbach, Adrian and Singhania, Akhilesh},
 title = {The Multikernel: A New OS Architecture for Scalable Multicore Systems},
 booktitle = {Proceedings of the ACM SIGOPS 22Nd Symposium on Operating Systems Principles},
 series = {SOSP '09},
 year = {2009},
 isbn = {978-1-60558-752-3},
 address = {Big Sky, Montana, USA},
 pages = {29--44},
 numpages = {16},
 url = {http://doi.acm.org/10.1145/1629575.1629579},
 doi = {10.1145/1629575.1629579},
 acmid = {1629579},
 publisher = {ACM},
 
 keywords = {message passing, multicore processors, scalability},
} 

@inproceedings{2009-hotos-dist-os,
 author = {Baumann, Andrew and Peter, Simon and Schupbach, Adrian and Singhania, Akhilesh and Roscoe, Timothy and Barham, Paul and Isaacs, Rebecca},
 title = {Your Computer is Already a Distributed System. Why Isn'T Your OS?},
 booktitle = {Proceedings of the 12th Conference on Hot Topics in Operating Systems},
 series = {HotOS'09},
 year = {2009},
 address = {Monte Verit\&\#224;, Switzerland},
 pages = {12--12},
 numpages = {1},
 url = {http://dl.acm.org/citation.cfm?id=1855568.1855580},
 acmid = {1855580},
 
 
} 

@inproceedings{2009-eurosys-snowflock,
 author = {Lagar-Cavilla, Horacio Andr{\'e}s and Whitney, Joseph Andrew and Scannell, Adin Matthew and Patchin, Philip and Rumble, Stephen M. and de Lara, Eyal and Brudno, Michael and Satyanarayanan, Mahadev},
 title = {SnowFlock: Rapid Virtual Machine Cloning for Cloud Computing},
 booktitle = {Proceedings of the 4th ACM European Conference on Computer Systems},
 series = {EuroSys '09},
 year = {2009},
 isbn = {978-1-60558-482-9},
 address = {Nuremberg, Germany},
 pages = {1--12},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1519065.1519067},
 doi = {10.1145/1519065.1519067},
 acmid = {1519067},
 publisher = {ACM},
 
 keywords = {cloud computing, virtualization},
} 

@inproceedings{2009-ccs-infoleak,
 author = {Ristenpart, Thomas and Tromer, Eran and Shacham, Hovav and Savage, Stefan},
 title = {Hey, You, Get off of My Cloud: Exploring Information Leakage in Third-party Compute Clouds},
 booktitle = {Proceedings of the 16th ACM Conference on Computer and Communications Security},
 series = {CCS '09},
 year = {2009},
 isbn = {978-1-60558-894-0},
 address = {Chicago, Illinois, USA},
 pages = {199--212},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1653662.1653687},
 doi = {10.1145/1653662.1653687},
 acmid = {1653687},
 publisher = {ACM},
 
 keywords = {cloud computing, side channels, virtual machine security},
} 

@inproceedings{2009-middleware-rhizoma,
 author = {Yin, Qin and Sch\"{u}pbach, Adrian and Cappos, Justin and Baumann, Andrew and Roscoe, Timothy},
 title = {{Rhizoma: A Runtime for Self-deploying, Self-managing Overlays}},
 booktitle = {Proceedings of the 10th ACM/IFIP/USENIX International Conference on Middleware},
 series = {Middleware '09},
 year = {2009},
 address = {Urbanna, Illinois},
 pages = {10:1--10:20},
 articleno = {10},
 numpages = {20},
 url = {http://dl.acm.org/citation.cfm?id=1656980.1656994},
 acmid = {1656994},
 publisher = {Springer-Verlag}, 
} 

@inproceedings{2011-hotos-mind-gap,
author = {Mogul, Jeffrey C. and Baumann, Andrew and Roscoe, Timothy and Soares, Livio},
title = {{Mind the Gap: Reconnecting Architecture and OS Research}},
year = {2011},

booktitle = {Proceedings of the 13th USENIX Conference on Hot Topics in Operating Systems},
pages = {1},
numpages = {1},
address = {Napa, California},
series = {HotOS’13}
}
  
@inproceedings{2011-socc-cope,
 author = {Liu, Changbin and Loo, Boon Thau and Mao, Yun},
 title = {{Declarative Automated Cloud Resource Orchestration}},
 booktitle = {Proceedings of the 2nd ACM Symposium on Cloud Computing},
 series = {SOCC '11},
 year = {2011},
 isbn = {978-1-4503-0976-9},
 address = {Cascais, Portugal},
 pages = {26:1--26:8},
 articleno = {26},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/2038916.2038942},
 doi = {10.1145/2038916.2038942},
 acmid = {2038942},
 publisher = {ACM},
 
 keywords = {cloud computing, declarative queries, distributed optimizations, resource orchestration},
} 

@inproceedings {2020-hotstorage-acc-sto-nec,
author = {Shinichi Awamoto and Erich Focht and Michio Honda},
title = {Designing a Storage Software Stack for Accelerators},
booktitle = {12th USENIX Workshop on Hot Topics in Storage and File Systems (HotStorage 20)},
year = {2020},
url = {https://www.usenix.org/conference/hotstorage20/presentation/awamoto},
publisher = {USENIX Association},
month = jul,
}


@inproceedings{2020-eurosys-metalfs,
author = {Schmid, Robert and Plauth, Max and Wenzel, Lukas and Eberhardt, Felix and Polze, Andreas},
title = {Accessible Near-Storage Computing with FPGAs},
year = {2020},
isbn = {9781450368827},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.vu-nl.idm.oclc.org/10.1145/3342195.3387557},
doi = {10.1145/3342195.3387557},
abstract = {Data transfers impose a major bottleneck in heterogenous system architectures. As a mitigation strategy, compute resources can be introduced in places where data occurs naturally. The increased diversity of compute resources in turn affects programming models and practicalities of software development for near-data compute kernels and raises the question of how those resources can be made accessible to users and applications.We introduce the Metal FS framework to improve the accessibility of FPGA-based near-storage accelerators: Firstly, we present a near-storage-compute-aware file system that enables self-contained, reusable compute kernels to operate on the granularity of file data streams. Secondly, we provide an integrated build process for FPGA overlay images that starts with the acquisition of compute kernels through a package manager and finally allows to dynamically configure near-storage compute pipelines consisting of them. Thirdly, we integrate the framework into Linux as a file system driver and repurpose Unix Pipes as a well-known operating system primitive to orchestrate near-storage compute pipelines.},
booktitle = {Proceedings of the Fifteenth European Conference on Computer Systems},
articleno = {28},
numpages = {12},
keywords = {FPGA, data-flow paradigm, near-storage computing, near-data processing, programming model},
location = {Heraklion, Greece},
series = {EuroSys '20}
}

@inproceedings{2015-sigcomm-netfpga,
author = {Zilberman, Noa and Audzevich, Yury and Kalogeridou, Georgina and Manihatty-Bojan, Neelakandan and Zhang, Jingyun and Moore, Andrew},
title = {NetFPGA: Rapid Prototyping of Networking Devices in Open Source},
year = {2015},
isbn = {9781450335423},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.vu-nl.idm.oclc.org/10.1145/2785956.2790029},
doi = {10.1145/2785956.2790029},
abstract = {The demand-led growth of datacenter networks has meant that many constituent technologies are beyond the budget of the wider community. In order to make and validate timely and relevant new contributions, the wider community requires accessible evaluation, experimentation and demonstration environments with specification comparable to the subsystems of the most massive datacenter networks. We demonstrate NetFPGA, an open-source platform for rapid prototyping of networking devices with I/O capabilities up to 100Gbps. NetFPGA offers an integrated environment that enables networking research by users from a wide range of disciplines: from hardware-centric research to formal methods.},
booktitle = {Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication},
pages = {363–364},
numpages = {2},
keywords = {high-speed, netFPGA, programmable hardware, networking},
location = {London, United Kingdom},
series = {SIGCOMM '15}
}

@inproceedings{2017-sosr-p4fpga,
author = {Wang, Han and Soul\'{e}, Robert and Dang, Huynh Tu and Lee, Ki Suh and Shrivastav, Vishal and Foster, Nate and Weatherspoon, Hakim},
title = {P4FPGA: A Rapid Prototyping Framework for P4},
year = {2017},
isbn = {9781450349475},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.vu-nl.idm.oclc.org/10.1145/3050220.3050234},
doi = {10.1145/3050220.3050234},
abstract = {This paper presents P4FPGA, a new tool for developing and evaluating data plane applications. P4FPGA is an open-source compiler and runtime. The compiler extends the P4.org reference compiler with a custom backend that generates FPGA code. P4FPGA supports different architecture configurations, depending on the needs of the particular application.We have benchmarked several representative P4 programs, and our experiments show that code generated by P4FPGA runs at line-rate at all packet sizes with latencies comparable to commercial ASICs. By combining high-level programming abstractions offered by P4 with a flexible and powerful hardware target, P4FPGA allows developers to rapidly prototype and deploy new data plane applications.},
booktitle = {Proceedings of the Symposium on SDN Research},
pages = {122–135},
numpages = {14},
keywords = {P4, FPGA, High-level synthesis},
location = {Santa Clara, CA, USA},
series = {SOSR '17}
}

@inproceedings {2020-osdi-hxdp,
author = {Marco Spaziani Brunella and Giacomo Belocchi and Marco Bonola and Salvatore Pontarelli and Giuseppe Siracusano and Giuseppe Bianchi and Aniello Cammarano and Alessandro Palumbo and Luca Petrucci and Roberto Bifulco},
title = {{hXDP}: Efficient Software Packet Processing on {FPGA} {NICs}},
booktitle = {14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20)},
year = {2020},
isbn = {978-1-939133-19-9},
pages = {973--990},
url = {https://www.usenix.org/conference/osdi20/presentation/brunella},
publisher = {USENIX Association},
month = nov,
}

@article{2021-arxiv-zcsd,
  author    = {Corne Lukken and
               Giulia Frascaria and
               \ulx{Animesh Trivedi}},
  title     = {{ZCSD:} a Computational Storage Device over Zoned Namespaces {(ZNS)}
               SSDs},
  journal   = {CoRR},
  volume    = {abs/2112.00142},
  year      = {2021},
  url       = {https://arxiv.org/abs/2112.00142},
  eprinttype = {arXiv},
  eprint    = {2112.00142},
  timestamp = {Tue, 07 Dec 2021 12:15:54 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2112-00142.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{2014-osdi-willow,
author = {Seshadri, Sudharsan and Gahagan, Mark and Bhaskaran, Sundaram and Bunker, Trevor and De, Arup and Jin, Yanqin and Liu, Yang and Swanson, Steven},
title = {Willow: A User-Programmable SSD},
year = {2014},
isbn = {9781931971164},
publisher = {USENIX Association},
address = {USA},
abstract = {We explore the potential of making programmability a central feature of the SSD interface. Our prototype system, called Willow, allows programmers to augment and extend the semantics of an SSD with application-specific features without compromising file system protections. The SSD Apps running on Willow give applications low-latency, high-bandwidth access to the SSD's contents while reducing the load that IO processing places on the host processor. The programming model for SSD Apps provides great flexibility, supports the concurrent execution of multiple SSD Apps in Willow, and supports the execution of trusted code in Willow.We demonstrate the effectiveness and flexibility of Willow by implementing six SSD Apps and measuring their performance. We find that defining SSD semantics in software is easy and beneficial, and that Willow makes it feasible for a wide range of IO-intensive applications to benefit from a customized SSD interface.},
booktitle = {Proceedings of the 11th USENIX Conference on Operating Systems Design and Implementation},
pages = {67–80},
numpages = {14},
location = {Broomfield, CO},
series = {OSDI'14}
}

@inproceedings{2011-sosp-ptask,
 author = {Rossbach, Christopher J. and Currey, Jon and Silberstein, Mark and Ray, Baishakhi and Witchel, Emmett},
 title = {{PTask: Operating System Abstractions to Manage GPUs As Compute Devices}},
 booktitle = {Proceedings of the Twenty-Third ACM Symposium on Operating Systems Principles},
 series = {SOSP '11},
 year = {2011},
 isbn = {978-1-4503-0977-6},
 address = {Cascais, Portugal},
 pages = {233--248},
 numpages = {16},
 url = {http://doi.acm.org/10.1145/2043556.2043579},
 doi = {10.1145/2043556.2043579},
 acmid = {2043579},
 publisher = {ACM},
 
 keywords = {GPGPU, GPUs, OS design, accelerators, dataflow, gestural interface, operating systems},
} 

@inproceedings{2012-hotcloud-llc-vm-scheduling,
 author = {Ahn, Jeongseob and Kim, Changdae and Han, Jaeung and Choi, Young-Ri and Huh, Jaehyuk},
 title = {{Dynamic Virtual Machine Scheduling in Clouds for Architectural Shared Resources}},
 booktitle = {Proceedings of the 4th USENIX Conference on Hot Topics in Cloud Computing},
 series = {HotCloud'12},
 year = {2012},
 address = {Boston, MA},
 pages = {19--19},
 numpages = {1},
 url = {http://dl.acm.org/citation.cfm?id=2342763.2342782},
 acmid = {2342782},
 
 
} 

@article{2012-micro-vantage,
 author = {Sanchez, Daniel and Kozyrakis, Christos},
 title = {Scalable and Efficient Fine-Grained Cache Partitioning with Vantage},
 journal = {IEEE Micro},
 issue_date = {May 2012},
 volume = {32},
 number = {3},
 month = may,
 year = {2012},
 issn = {0272-1732},
 pages = {26--37},
 numpages = {12},
 url = {https://doi.org/10.1109/MM.2012.19},
 doi = {10.1109/MM.2012.19},
 acmid = {2311819},
 publisher = {IEEE Computer Society Press},
 address = {Los Alamitos, CA, USA},
 keywords = {cache memories, design styles, memory structures, hardware, parallel architectures, processor architectures, computer systems organization, memory hierarchy, microarchitecture implementation considerations, processor architectures, computer systems organization, Vantage, cache partitioning, CMP, QoS},
} 

@inproceedings{2012-hotcloud-uVM,
 author = {Ahn, Jeongseob and Kim, Changdae and Han, Jaeung and Choi, Young-Ri and Huh, Jaehyuk},
 title = {Dynamic Virtual Machine Scheduling in Clouds for Architectural Shared Resources},
 booktitle = {Proceedings of the 4th USENIX Conference on Hot Topics in Cloud Computing},
 series = {HotCloud'12},
 year = {2012},
 address = {Boston, MA},
 pages = {19--19},
 numpages = {1},
 url = {http://dl.acm.org/citation.cfm?id=2342763.2342782},
 acmid = {2342782},
 
 
} 

@inproceedings{2012-asplos-clearning-cloud,
 author = {Ferdman, Michael and Adileh, Almutaz and Kocberber, Onur and Volos, Stavros and Alisafaee, Mohammad and Jevdjic, Djordje and Kaynak, Cansu and Popescu, Adrian Daniel and Ailamaki, Anastasia and Falsafi, Babak},
 title = {{Clearing the Clouds: A Study of Emerging Scale-out Workloads on Modern Hardware}},
 booktitle = {Proceedings of the Seventeenth International Conference on Architectural Support for Programming Languages and Operating Systems},
 series = {ASPLOS XVII},
 year = {2012},
 isbn = {978-1-4503-0759-8},
 address = {London, England, UK},
 pages = {37--48},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/2150976.2150982},
 doi = {10.1145/2150976.2150982},
 acmid = {2150982},
 publisher = {ACM},
 
 keywords = {architectural evaluation, cloud computing, design insights, workload characterization},
} 

@inproceedings{2013-asplos-paragon,
 author = {Delimitrou, Christina and Kozyrakis, Christos},
 title = {{Paragon: QoS-aware Scheduling for Heterogeneous Datacenters}},
 booktitle = {Proceedings of the Eighteenth International Conference on Architectural Support for Programming Languages and Operating Systems},
 series = {ASPLOS '13},
 year = {2013},
 isbn = {978-1-4503-1870-9},
 address = {Houston, Texas, USA},
 pages = {77--88},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/2451116.2451125},
 doi = {10.1145/2451116.2451125},
 acmid = {2451125},
 publisher = {ACM},
 
 keywords = {cloud computing, datacenter, heterogeneity, interference, qos, scheduling},
} 

@inproceedings{2013-fast-openssd1,
 author = {Lu, Youyou and Shu, Jiwu and Zheng, Weimin},
 title = {{Extending the Lifetime of Flash-based Storage Through Reducing Write Amplification from File Systems}},
 booktitle = {Proceedings of the 11th USENIX Conference on File and Storage Technologies},
 series = {FAST'13},
 year = {2013},
 address = {San Jose, CA},
 pages = {257--270},
 numpages = {14},
 url = {http://dl.acm.org/citation.cfm?id=2591272.2591299},
 acmid = {2591299},
 
 
} 

@inproceedings{2014-sec-flushreload,
 author = {Yarom, Yuval and Falkner, Katrina},
 title = {{FLUSH+RELOAD: A High Resolution, Low Noise, L3 Cache Side-channel Attack}},
 booktitle = {Proceedings of the 23rd USENIX Conference on Security Symposium},
 series = {SEC'14},
 year = {2014},
 isbn = {978-1-931971-15-7},
 address = {San Diego, CA},
 pages = {719--732},
 numpages = {14},
 url = {http://dl.acm.org/citation.cfm?id=2671225.2671271},
 acmid = {2671271},
 
 
} 

@inproceedings {2002-atc-eros,
author = {Jonathan S. Shapiro and Jonathan Adams},
title = {Design Evolution of the {EROS} {Single-Level} Store},
booktitle = {2002 USENIX Annual Technical Conference (USENIX ATC 02)},
year = {2002},
address = {Monterey, CA},
url = {https://www.usenix.org/conference/2002-usenix-annual-technical-conference/design-evolution-eros-single-level-store},
publisher = {USENIX Association},
month = jun,
}

@inproceedings{2014-isca-catapult,
 author = {Putnam, Andrew and Caulfield, Adrian M. and Chung, Eric S. and Chiou, Derek and Constantinides, Kypros and Demme, John and Esmaeilzadeh, Hadi and Fowers, Jeremy and Gopal, Gopi Prashanth and Gray, Jan and Haselman, Michael and Hauck, Scott and Heil, Stephen and Hormati, Amir and Kim, Joo-Young and Lanka, Sitaram and Larus, James and Peterson, Eric and Pope, Simon and Smith, Aaron and Thong, Jason and Xiao, Phillip Yi and Burger, Doug},
 title = {{A Reconfigurable Fabric for Accelerating Large-scale Datacenter Services}},
 booktitle = {Proceeding of the 41st Annual International Symposium on Computer Architecuture},
 series = {ISCA '14},
 year = {2014},
 isbn = {978-1-4799-4394-4},
 address = {Minneapolis, Minnesota, USA},
 pages = {13--24},
 numpages = {12},
 url = {http://dl.acm.org/citation.cfm?id=2665671.2665678},
 acmid = {2665678},
 publisher = {IEEE Press}, 
} 

@inproceedings{2014-nsdi-farm,
 author = {Dragojevi\'{c}, Aleksandar and Narayanan, Dushyanth and Hodson, Orion and Castro, Miguel},
 title = {{FaRM: Fast Remote Memory}},
 booktitle = {Proceedings of the 11th USENIX Conference on Networked Systems Design and Implementation},
 series = {NSDI'14},
 year = {2014},
 isbn = {978-1-931971-09-6},
 address = {Seattle, WA},
 pages = {401--414},
 numpages = {14},
 url = {http://dl.acm.org/citation.cfm?id=2616448.2616486},
 acmid = {2616486},
 
 
} 

@inproceedings{2014-nsdi-mica,
 author = {Lim, Hyeontaek and Han, Dongsu and Andersen, David G. and Kaminsky, Michael},
 title = {MICA: A Holistic Approach to Fast In-memory Key-value Storage},
 booktitle = {Proceedings of the 11th USENIX Conference on Networked Systems Design and Implementation},
 series = {NSDI'14},
 year = {2014},
 isbn = {978-1-931971-09-6},
 address = {Seattle, WA},
 pages = {429--444},
 numpages = {16},
 url = {http://dl.acm.org/citation.cfm?id=2616448.2616488},
 acmid = {2616488},
 
 
} 

@misc{2022-nvidia-gpudirect,
  author = {NVIDIA},
  title = {{Developing a Linux Kernel Module using GPUDirect RDMA}},
  howpublished = {\nolinkurl{https://docs.nvidia.com/cuda/gpudirect-rdma/index.html}},
  note = {Accessed: 2022-Feb-02}
}

@misc{2019-open-channel-ssd,
  author = {Matias Bj{\o}rling},
  title = {{Open-Channel Solid State Drives}},
  howpublished = {\nolinkurl{https://events.static.linuxfound.org/sites/events/files/slides/LightNVM-Vault2015.pdf}},
  note = {Accessed: 2019-10-24}
}

@inproceedings{2015-vee-hyv,
author = {Pfefferle, Jonas and Stuedi, Patrick and \ulx{Trivedi, Animesh} and Metzler, Bernard and Koltsidas, Ionnis and Gross, Thomas R.},
title = {{A Hybrid I/O Virtualization Framework for RDMA-Capable Network Interfaces}},
year = {2015},
isbn = {9781450334501},
publisher = {ACM},

url = {https://doi.org/10.1145/2731186.2731200},
doi = {10.1145/2731186.2731200},
booktitle = {Proceedings of the 11th ACM SIGPLAN/SIGOPS International Conference on Virtual Execution Environments},
pages = {17–30},
numpages = {14},
keywords = {virtualization, rdma},
address = {Istanbul, Turkey},
series = {VEE ’15}
}

@inproceedings{2015-sigcomm-condor,
 author = {Schlinker, Brandon and Mysore, Radhika Niranjan and Smith, Sean and Mogul, Jeffrey C. and Vahdat, Amin and Yu, Minlan and Katz-Bassett, Ethan and Rubin, Michael},
 title = {{Condor: Better Topologies Through Declarative Design}},
 booktitle = {Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication},
 series = {SIGCOMM '15},
 year = {2015},
 isbn = {978-1-4503-3542-3},
 address = {London, United Kingdom},
 pages = {449--463},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/2785956.2787476},
 doi = {10.1145/2785956.2787476},
 acmid = {2787476},
 publisher = {ACM},
 
 keywords = {expandable topologies, slo compliance, topology design},
} 

@inproceedings{2015-isca-heracles,
 author = {Lo, David and Cheng, Liqun and Govindaraju, Rama and Ranganathan, Parthasarathy and Kozyrakis, Christos},
 title = {Heracles: Improving Resource Efficiency at Scale},
 booktitle = {Proceedings of the 42Nd Annual International Symposium on Computer Architecture},
 series = {ISCA '15},
 year = {2015},
 isbn = {978-1-4503-3402-0},
 address = {Portland, Oregon},
 pages = {450--462},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/2749469.2749475},
 doi = {10.1145/2749469.2749475},
 acmid = {2749475},
 publisher = {ACM},
 
} 

@inproceedings{2015-ieee-uarch-security,
 author = {Lefray, Arnaud and Caron, Eddy and Rouzaud-Cornabas, Jonathan and Toinard, Christian},
 title = {{Microarchitecture-Aware Virtual Machine Placement Under Information Leakage Constraints}},
 booktitle = {Proceedings of the 2015 IEEE 8th International Conference on Cloud Computing},
 series = {CLOUD '15},
 year = {2015},
 isbn = {978-1-4673-7287-9},
 pages = {588--595},
 numpages = {8},
 url = {https://doi.org/10.1109/CLOUD.2015.84},
 doi = {10.1109/CLOUD.2015.84},
 acmid = {2859141},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
 keywords = {Security, Cloud, Virtual Machine Placement, Isolation, Microarchitecture, Covert channels},
} 

@inproceedings{2015-vee-adrm,
 author = {Wang, Hui and Isci, Canturk and Subramanian, Lavanya and Choi, Jongmoo and Qian, Depei and Mutlu, Onur},
 title = {{A-DRM: Architecture-aware Distributed Resource Management of Virtualized Clusters}},
 booktitle = {Proceedings of the 11th ACM SIGPLAN/SIGOPS International Conference on Virtual Execution Environments},
 series = {VEE '15},
 year = {2015},
 isbn = {978-1-4503-3450-1},
 address = {Istanbul, Turkey},
 pages = {93--106},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/2731186.2731202},
 doi = {10.1145/2731186.2731202},
 acmid = {2731202},
 publisher = {ACM},
 
 keywords = {live migration, microarchitecture, performance counters, resource management, virtualization},
} 

@inproceedings{2015-atc-mercury,
 author = {Karanasos, Konstantinos and Rao, Sriram and Curino, Carlo and Douglas, Chris and Chaliparambil, Kishore and Fumarola, Giovanni Matteo and Heddaya, Solom and Ramakrishnan, Raghu and Sakalanaga, Sarvesh},
 title = {Mercury: Hybrid Centralized and Distributed Scheduling in Large Shared Clusters},
 booktitle = {Proceedings of the 2015 USENIX Conference on Usenix Annual Technical Conference},
 series = {USENIX ATC '15},
 year = {2015},
 isbn = {978-1-931971-225},
 address = {Santa Clara, CA},
 pages = {485--497},
 numpages = {13},
 url = {http://dl.acm.org/citation.cfm?id=2813767.2813803},
 acmid = {2813803},
 
 
} 

@inproceedings{2015-eurosys-borg,
 author = {Verma, Abhishek and Pedrosa, Luis and Korupolu, Madhukar and Oppenheimer, David and Tune, Eric and Wilkes, John},
 title = {Large-scale Cluster Management at Google with Borg},
 booktitle = {Proceedings of the Tenth European Conference on Computer Systems},
 series = {EuroSys '15},
 year = {2015},
 isbn = {978-1-4503-3238-5},
 address = {Bordeaux, France},
 pages = {18:1--18:17},
 articleno = {18},
 numpages = {17},
 url = {http://doi.acm.org/10.1145/2741948.2741964},
 doi = {10.1145/2741948.2741964},
 acmid = {2741964},
 publisher = {ACM},
 
} 

@inproceedings{2015-socc-migration-modeling,
 author = {Nathan, Senthil and Bellur, Umesh and Kulkarni, Purushottam},
 title = {Towards a Comprehensive Performance Model of Virtual Machine Live Migration},
 booktitle = {Proceedings of the Sixth ACM Symposium on Cloud Computing},
 series = {SoCC '15},
 year = {2015},
 isbn = {978-1-4503-3651-2},
 address = {Kohala Coast, Hawaii},
 pages = {288--301},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/2806777.2806838},
 doi = {10.1145/2806777.2806838},
 acmid = {2806838},
 publisher = {ACM},
 
}
 
@inproceedings{2015-sec-ctemplate,
 author = {Gruss, Daniel and Spreitzer, Raphael and Mangard, Stefan},
 title = {Cache Template Attacks: Automating Attacks on Inclusive Last-level Caches},
 booktitle = {Proceedings of the 24th USENIX Conference on Security Symposium},
 series = {SEC'15},
 year = {2015},
 isbn = {978-1-931971-232},
 address = {Washington, D.C.},
 pages = {897--912},
 numpages = {16},
 url = {http://dl.acm.org/citation.cfm?id=2831143.2831200},
 acmid = {2831200},
 
 
} 
 
@inproceedings{2015-sp-pp,
 author = {Liu, Fangfei and Yarom, Yuval and Ge, Qian and Heiser, Gernot and Lee, Ruby B.},
 title = {Last-Level Cache Side-Channel Attacks Are Practical},
 booktitle = {Proceedings of the 2015 IEEE Symposium on Security and Privacy},
 series = {SP '15},
 year = {2015},
 isbn = {978-1-4673-6949-7},
 pages = {605--622},
 numpages = {18},
 url = {https://doi.org/10.1109/SP.2015.43},
 doi = {10.1109/SP.2015.43},
 acmid = {2867673},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
 keywords = {side-channel attack, cross-VM side channel, covert channel, last-level cache, ElGamal},
} 

@inproceedings{2015-isca-1-billion,
 author = {Li, Sheng and Lim, Hyeontaek and Lee, Victor W. and Ahn, Jung Ho and Kalia, Anuj and Kaminsky, Michael and Andersen, David G. and Seongil, O. and Lee, Sukhan and Dubey, Pradeep},
 title = {Architecting to Achieve a Billion Requests Per Second Throughput on a Single Key-value Store Server Platform},
 booktitle = {Proceedings of the 42Nd Annual International Symposium on Computer Architecture},
 series = {ISCA '15},
 year = {2015},
 isbn = {978-1-4503-3402-0},
 address = {Portland, Oregon},
 pages = {476--488},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/2749469.2750416},
 doi = {10.1145/2749469.2750416},
 acmid = {2750416},
 publisher = {ACM},
 
} 

@inproceedings{2016-socc-yarn-fpga,
 author = {Huang, Muhuan and Wu, Di and Yu, Cody Hao and Fang, Zhenman and Interlandi, Matteo and Condie, Tyson and Cong, Jason},
 title = {Programming and Runtime Support to Blaze FPGA Accelerator Deployment at Datacenter Scale},
 booktitle = {Proceedings of the Seventh ACM Symposium on Cloud Computing},
 series = {SoCC '16},
 year = {2016},
 isbn = {978-1-4503-4525-5},
 address = {Santa Clara, CA, USA},
 pages = {456--469},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/2987550.2987569},
 doi = {10.1145/2987550.2987569},
 acmid = {2987569},
 publisher = {ACM},
 
 keywords = {FPGA-as-a-service, heterogeneous datacenter},
} 

@inproceedings {flipsflops,
 author = {Yuan Xiao and Xiaokuan Zhang and Yinqian Zhang and Radu Teodorescu},
 title = {{One Bit Flips, One Cloud Flops: Cross-VM Row Hammer Attacks and Privilege Escalation}},
 booktitle = {Proceedings of the 25th USENIX Conference on Security Symposium},
 series = {SEC'16},
 year = {2016},
 address = {Austin, TX, USA},
 
 
}

@inproceedings{2016-sec-ffs,
 author = {Razavi, Kaveh and Gras, Ben and Bosman, Erik and Preneel, Bart and Giuffrida, Cristiano and Bos, Herbert},
 title = {{Flip Feng Shui: Hammering a Needle in the Software Stack}},
 booktitle = {Proceedings of the 25th USENIX Conference on Security Symposium},
 series = {SEC'16},
 year = {2016},
 isbn = {978-1-931971-32-4},
 address = {Austin, TX, USA},
 pages = {1--18},
 numpages = {18},
 url = {http://dl.acm.org/citation.cfm?id=3241094.3241096},
 acmid = {3241096},
 
 
} 

@article{2019-xxx-fpga-survey,
author = {Skhiri, Rym and Fresse, Virginie and Jamont, Jean Paul and Suffran, Benoit and Malek, Jihene and Margala, Martin},
title = {From FPGA to Support Cloud to Cloud of FPGA: State of the Art},
year = {2019},
issue_date = {2019},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2019},
issn = {1687-7195},
url = {https://doi-org.vu-nl.idm.oclc.org/10.1155/2019/8085461},
doi = {10.1155/2019/8085461},
journal = {Int. J. Reconfig. Comput.},
month = {jan},
numpages = {17}
}

@inproceedings{2016-micro-azure-cloud,
 author = {Caulfield, Adrian M. and Chung, Eric S. and Putnam, Andrew and Angepat, Hari and Fowers, Jeremy and Haselman, Michael and Heil, Stephen and Humphrey, Matt and Kaur, Puneet and Kim, Joo-Young and Lo, Daniel and Massengill, Todd and Ovtcharov, Kalin and Papamichael, Michael and Woods, Lisa and Lanka, Sitaram and Chiou, Derek and Burger, Doug},
 title = {{A Cloud-scale Acceleration Architecture}},
 booktitle = {The 49th Annual IEEE/ACM International Symposium on Microarchitecture},
 series = {MICRO-49},
 year = {2016},
 address = {Taipei, Taiwan},
 pages = {7:1--7:13},
 articleno = {7},
 numpages = {13},
 url = {http://dl.acm.org/citation.cfm?id=3195638.3195647},
 acmid = {3195647},
 publisher = {IEEE Press}, 
} 

@inproceedings {2016-atc-guidelines,
author = {Anuj Kalia and Michael Kaminsky and David G. Andersen},
title = {{Design Guidelines for High Performance {RDMA} Systems}},
booktitle = {2016 {USENIX} Annual Technical Conference ({USENIX} {ATC} 16)},
year = {2016},
isbn = {978-1-931971-30-0},
address = {Denver, CO},
pages = {437--450},
url = {https://www.usenix.org/conference/atc16/technical-sessions/presentation/kalia},

}

@inproceedings {2016-atc-parafs,
author = {Jiacheng Zhang and Jiwu Shu and Youyou Lu},
title = {ParaFS: A Log-Structured File System to Exploit the Internal Parallelism of Flash Devices},
booktitle = {2016 {USENIX} Annual Technical Conference ({USENIX} {ATC} 16)},
year = {2016},
isbn = {978-1-931971-30-0},
address = {Denver, CO},
pages = {87--100},
url = {https://www.usenix.org/conference/atc16/technical-sessions/presentation/zhang},

}

@inproceedings {2016-atc-ginseng,
author = {Liran Funaro and Orna Agmon Ben-Yehuda and Assaf Schuster},
title = {Ginseng: Market-Driven {LLC} Allocation},
booktitle = {2016 {USENIX} Annual Technical Conference ({USENIX} {ATC} 16)},
year = {2016},
isbn = {978-1-931971-30-0},
address = {Denver, CO},
pages = {295--308},
url = {https://www.usenix.org/conference/atc16/technical-sessions/presentation/funaro},

}

@inproceedings{2016-osdi-no-paxos,
 author = {Li, Jialin and Michael, Ellis and Sharma, Naveen Kr. and Szekeres, Adriana and Ports, Dan R. K.},
 title = {Just Say No to Paxos Overhead: Replacing Consensus with Network Ordering},
 booktitle = {Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation},
 series = {OSDI'16},
 year = {2016},
 isbn = {978-1-931971-33-1},
 address = {Savannah, GA, USA},
 pages = {467--483},
 numpages = {17},
 url = {http://dl.acm.org/citation.cfm?id=3026877.3026914},
 acmid = {3026914},
 
 
} 

@inproceedings{2016-osdi-firmament,
 author = {Gog, Ionel and Schwarzkopf, Malte and Gleave, Adam and Watson, Robert N. M. and Hand, Steven},
 title = {Firmament: Fast, Centralized Cluster Scheduling at Scale},
 booktitle = {Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation},
 series = {OSDI'16},
 year = {2016},
 isbn = {978-1-931971-33-1},
 address = {Savannah, GA, USA},
 pages = {99--115},
 numpages = {17},
 url = {http://dl.acm.org/citation.cfm?id=3026877.3026886},
 acmid = {3026886},
 
 
} 

@inproceedings {2016-usenix-sec-ffs,
author = {Kaveh Razavi and Ben Gras and Erik Bosman and Bart Preneel and Cristiano Giuffrida and Herbert Bos},
title = {Flip Feng Shui: Hammering a Needle in the Software Stack},
booktitle = {25th {USENIX} Security Symposium ({USENIX} Security 16)},
year = {2016},
isbn = {978-1-931971-32-4},
address = {Austin, TX},
pages = {1--18},
url = {https://www.usenix.org/conference/usenixsecurity16/technical-sessions/presentation/razavi},

}

@inproceedings{2016-osdi-ingens,
 author = {Kwon, Youngjin and Yu, Hangchen and Peter, Simon and Rossbach, Christopher J. and Witchel, Emmett},
 title = {Coordinated and Efficient Huge Page Management with Ingens},
 booktitle = {Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation},
 series = {OSDI'16},
 year = {2016},
 isbn = {978-1-931971-33-1},
 address = {Savannah, GA, USA},
 pages = {705--721},
 numpages = {17},
 url = {http://dl.acm.org/citation.cfm?id=3026877.3026931},
 acmid = {3026931},
 
 
} 

@inproceedings{2016-asplos-flexnic,
 author = {Kaufmann, Antoine and Peter, SImon and Sharma, Naveen Kr. and Anderson, Thomas and Krishnamurthy, Arvind},
 title = {{High Performance Packet Processing with FlexNIC}},
 booktitle = {Proceedings of the Twenty-First International Conference on Architectural Support for Programming Languages and Operating Systems},
 series = {ASPLOS '16},
 year = {2016},
 isbn = {978-1-4503-4091-5},
 address = {Atlanta, Georgia, USA},
 pages = {67--81},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/2872362.2872367},
 doi = {10.1145/2872362.2872367},
 acmid = {2872367},
 publisher = {ACM},
 
 keywords = {DMA, flexible network processing, match-and-action processing, network interface card},
} 

@inproceedings{2016-socc-rackout,
 author = {Novakovic, Stanko and Daglis, Alexandros and Bugnion, Edouard and Falsafi, Babak and Grot, Boris},
 title = {The Case for RackOut: Scalable Data Serving Using Rack-Scale Systems},
 booktitle = {Proceedings of the Seventh ACM Symposium on Cloud Computing},
 series = {SoCC '16},
 year = {2016},
 isbn = {978-1-4503-4525-5},
 address = {Santa Clara, CA, USA},
 pages = {182--195},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/2987550.2987577},
 doi = {10.1145/2987550.2987577},
 acmid = {2987577},
 publisher = {ACM},
 
 keywords = {RDMA, Rack-scale systems, data skew},
} 

@inproceedings{2017-kbnet-rdma-anamoly,
author = {Zhang, Yiwen and Gu, Juncheng and Lee, Youngmoon and Chowdhury, Mosharaf and Shin, Kang G.},
title = {{Performance Isolation Anomalies in RDMA}},
year = {2017},
isbn = {9781450350532},
publisher = {ACM},

url = {https://doi.org/10.1145/3098583.3098591},
doi = {10.1145/3098583.3098591},
booktitle = {Proceedings of the Workshop on Kernel-Bypass Networks},
pages = {43–48},
numpages = {6},
keywords = {fairness, RDMA, performance isolation},
address = {Los Angeles, CA, USA},
series = {KBNets ’17}
}
  
@article{2017-open-ssd-isolation,
title = {{Multi-Tenant I/O Isolation with Open-Channel SSDs}},
author = {Javier González and Matias Bjørling},
journal= {Nonvolatile Memory Workshop (NVMW)},
year = {2017},
}

@article{2017-tpds-gpu,
 author = {Mei, Xinxin and Chu, Xiaowen},
 title = {{Dissecting GPU Memory Hierarchy Through Microbenchmarking}},
 journal = {IEEE Transactions on Parallel and Distributed Systems},
 issue_date = {January 2017},
 volume = {28},
 number = {1},
 month = jan,
 year = {2017},
 issn = {1045-9219},
 pages = {72--86},
 numpages = {15},
 url = {https://doi.org/10.1109/TPDS.2016.2549523},
 doi = {10.1109/TPDS.2016.2549523},
 acmid = {3028945},
 publisher = {IEEE Press},
 address = {Piscataway, NJ, USA},
} 

@inproceedings{2017-micro-numa-gpus,
 author = {Milic, Ugljesa and Villa, Oreste and Bolotin, Evgeny and Arunkumar, Akhil and Ebrahimi, Eiman and Jaleel, Aamer and Ramirez, Alex and Nellans, David},
 title = {Beyond the Socket: NUMA-aware GPUs},
 booktitle = {Proceedings of the 50th Annual IEEE/ACM International Symposium on Microarchitecture},
 series = {MICRO-50 '17},
 year = {2017},
 isbn = {978-1-4503-4952-9},
 address = {Cambridge, Massachusetts},
 pages = {123--135},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/3123939.3124534},
 doi = {10.1145/3123939.3124534},
 acmid = {3124534},
 publisher = {ACM},
 
 keywords = {NUMA systems, graphics processing units, multi-socket GPUs},
} 

@inproceedings{2017-hotos-neardata-os,
 author = {Barbalace, Antonio and Iliopoulos, Anthony and Rauchfuss, Holm and Brasche, Goetz},
 title = {It's Time to Think About an Operating System for Near Data Processing Architectures},
 booktitle = {Proceedings of the 16th Workshop on Hot Topics in Operating Systems},
 series = {HotOS '17},
 year = {2017},
 isbn = {978-1-4503-5068-6},
 address = {Whistler, BC, Canada},
 pages = {56--61},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/3102980.3102990},
 doi = {10.1145/3102980.3102990},
 acmid = {3102990},
 publisher = {ACM},
 
 keywords = {Near data processing, decentralized resource control, multiple kernels OS, single protection domain},
} 

@inproceedings{2017-asplos-incbrick,
 author = {Liu, Ming and Luo, Liang and Nelson, Jacob and Ceze, Luis and Krishnamurthy, Arvind and Atreya, Kishore},
 title = {IncBricks: Toward In-Network Computation with an In-Network Cache},
 booktitle = {Proceedings of the Twenty-Second International Conference on Architectural Support for Programming Languages and Operating Systems},
 series = {ASPLOS '17},
 year = {2017},
 isbn = {978-1-4503-4465-4},
 address = {Xi'an, China},
 pages = {795--809},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/3037697.3037731},
 doi = {10.1145/3037697.3037731},
 acmid = {3037731},
 publisher = {ACM},
 
 keywords = {in-network caching, programmable network devices},
} 

@inproceedings{2017-asplos-bolt,
 author = {Delimitrou, Christina and Kozyrakis, Christos},
 title = {{Bolt: I Know What You Did Last Summer... In The Cloud}},
 booktitle = {Proceedings of the Twenty-Second International Conference on Architectural Support for Programming Languages and Operating Systems},
 series = {ASPLOS '17},
 year = {2017},
 isbn = {978-1-4503-4465-4},
 address = {Xi'an, China},
 pages = {599--613},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/3037697.3037703},
 doi = {10.1145/3037697.3037703},
 acmid = {3037703},
 publisher = {ACM},
 
 keywords = {cloud computing, data mining, datacenter, denial of service attack, interference, isolation, latency, security},
} 

@inproceedings{2017-asplos-reflex,
 author = {Klimovic, Ana and Litz, Heiner and Kozyrakis, Christos},
 title = {{ReFlex: Remote Flash = Local Flash}},
 booktitle = {Proceedings of the Twenty-Second International Conference on Architectural Support for Programming Languages and Operating Systems},
 series = {ASPLOS '17},
 year = {2017},
 isbn = {978-1-4503-4465-4},
 address = {Xi'an, China},
 pages = {345--359},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/3037697.3037732},
 doi = {10.1145/3037697.3037732},
 acmid = {3037732},
 publisher = {ACM},
 
 keywords = {datacenter storage, flash, i/o scheduling, network storage, qos},
} 

@inproceedings {2017-fast-didacache,
author = {Zhaoyan Shen and Feng Chen and Yichen Jia and Zili Shao},
title = {DIDACache: A Deep Integration of Device and Application for Flash Based Key-Value Caching},
booktitle = {15th {USENIX} Conference on File and Storage Technologies ({FAST} 17)},
year = {2017},
isbn = {978-1-931971-36-2},
address = {Santa Clara, CA},
pages = {391--405},
url = {https://www.usenix.org/conference/fast17/technical-sessions/presentation/shen},

}

@inproceedings{2017-fast-openchannel-ssd,
author = {Bj\o{}rling, Matias and Gonz\'{a}lez, Javier and Bonnet, Philippe},
title = {{LightNVM: The Linux Open-Channel SSD Subsystem}},
year = {2017},
isbn = {9781931971362},

booktitle = {Proceedings of the 15th Usenix Conference on File and Storage Technologies},
pages = {359–373},
numpages = {15},
address = {Santa clara, CA, USA},
series = {FAST’17}
}
 
@inproceedings{2017-ppopp-gpuarch,
 author = {Zhang, Xiuxia and Tan, Guangming and Xue, Shuangbai and Li, Jiajia and Zhou, Keren and Chen, Mingyu},
 title = {{Understanding the GPU Microarchitecture to Achieve Bare-Metal Performance Tuning}},
 booktitle = {Proceedings of the 22Nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
 series = {PPoPP '17},
 year = {2017},
 isbn = {978-1-4503-4493-7},
 address = {Austin, Texas, USA},
 pages = {31--43},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/3018743.3018755},
 doi = {10.1145/3018743.3018755},
 acmid = {3018755},
 publisher = {ACM},
 
 keywords = {assembler, convolution, gpu, performance, reverse-engineering gpu isa encoding, sgemm},
} 

@inproceedings{2017-nsdi-decibel,
author = {Nanavati, Mihir and Wires, Jake and Warfield, Andrew},
title = {{Decibel: Isolation and Sharing in Disaggregated Rack-Scale Storage}},
year = {2017},
isbn = {9781931971379},

booktitle = {Proceedings of the 14th USENIX Conference on Networked Systems Design and Implementation},
pages = {17–33},
numpages = {17},
address = {Boston, MA, USA},
series = {NSDI’17}
}
  
@inproceedings {2017-nsdi-excamera,
author = {Sadjad Fouladi and Riad S. Wahby and Brennan Shacklett and Karthikeyan Vasuki Balasubramaniam and William Zeng and Rahul Bhalerao and Anirudh Sivaraman and George Porter and Keith Winstein},
title = {{Encoding, Fast and Slow: Low-Latency Video Processing Using Thousands of Tiny Threads}},
booktitle = {14th {USENIX} Symposium on Networked Systems Design and Implementation ({NSDI} 17)},
year = {2017},
isbn = {978-1-931971-37-9},
address = {Boston, MA},
pages = {363--376},
url = {https://www.usenix.org/conference/nsdi17/technical-sessions/presentation/fouladi},

}

@INPROCEEDINGS{2018-fpl-multi-kv,  
author={ Zsolt {István} and Gustavo Alonso and Ankit Singla},  
booktitle={2018 28th International Conference on Field Programmable Logic and Applications (FPL)},   
title={{Providing Multi-tenant Services with FPGAs: Case Study on a Key-Value Store}},   
year={2018},  
pages={119-1195},
}

@inproceedings {2018-atc-sock,
author = {Edward Oakes and Leon Yang and Dennis Zhou and Kevin Houck and Tyler Harter and Andrea Arpaci-Dusseau and Remzi Arpaci-Dusseau},
title = {{{SOCK}: Rapid Task Provisioning with Serverless-Optimized Containers}},
booktitle = {2018 {USENIX} Annual Technical Conference ({USENIX} {ATC} 18)},
year = {2018},
isbn = {978-1-931971-44-7},
address = {Boston, MA},
pages = {57--70},
url = {https://www.usenix.org/conference/atc18/presentation/oakes},

month = jul,
}

@inproceedings {2018-osdi-graviton,
author = {Stavros Volos and Kapil Vaswani and Rodrigo Bruno},
title = {{Graviton: Trusted Execution Environments on GPUs}},
booktitle = {13th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 18)},
year = {2018},
isbn = {978-1-939133-08-3},
address = {Carlsbad, CA},
pages = {681--696},
url = {https://www.usenix.org/conference/osdi18/presentation/volos},

month = oct,
}

@inproceedings{2018-atc-peeking-serverless,
 author = {Wang, Liang and Li, Mengyuan and Zhang, Yinqian and Ristenpart, Thomas and Swift, Michael},
 title = {{Peeking Behind the Curtains of Serverless Platforms}},
 booktitle = {Proceedings of the 2018 USENIX Conference on Usenix Annual Technical Conference},
 series = {USENIX ATC '18},
 year = {2018},
 isbn = {978-1-931971-44-7},
 address = {Boston, MA, USA},
 pages = {133--145},
 numpages = {13},
 url = {http://dl.acm.org/citation.cfm?id=3277355.3277369},
 acmid = {3277369},
 
 
} 

@inproceedings {2018-osdi-amorphos,
author = {Ahmed Khawaja and Joshua Landgraf and Rohith Prakash and Michael Wei and Eric Schkufza and Christopher J. Rossbach},
title = {Sharing, Protection, and Compatibility for Reconfigurable Fabric with {AmorphOS}},
booktitle = {13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18)},
year = {2018},
isbn = {978-1-939133-08-3},
address = {Carlsbad, CA},
pages = {107--127},
url = {http://www.usenix.org/conference/osdi18/presentation/khawaja},
publisher = {USENIX Association},
month = oct,
}

@inproceedings {2019-atc-multit-gpus,
author = {Myeongjae Jeon and Shivaram Venkataraman and Amar Phanishayee and Junjie Qian and Wencong Xiao and Fan Yang},
title = {{Analysis of Large-Scale Multi-Tenant {GPU} Clusters for {DNN} Training Workloads}},
booktitle = {2019 {USENIX} Annual Technical Conference ({USENIX} {ATC} 19)},
year = {2019},
isbn = {978-1-939133-03-8},
address = {Renton, WA},
pages = {947--960},
url = {https://www.usenix.org/conference/atc19/presentation/jeon},

month = jul,
}

@inproceedings {2019-hotcloud-happiness,
author = {Vojislav Dukic and Ankit Singla},
title = {{Happiness index: Right-sizing the cloud{\textquoteright}s tenant-provider interface}},
booktitle = {11th {USENIX} Workshop on Hot Topics in Cloud Computing (HotCloud 19)},
year = {2019},
address = {Renton, WA},
url = {https://www.usenix.org/conference/hotcloud19/presentation/dukic},

month = jul,
}

@article{2019-cacm-golden-age,
 author = {Hennessy, John L. and Patterson, David A.},
 title = {{A New Golden Age for Computer Architecture}},
 journal = {Commun. ACM},
 issue_date = {February 2019},
 volume = {62},
 number = {2},
 month = jan,
 year = {2019},
 issn = {0001-0782},
 pages = {48--60},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/3282307},
 doi = {10.1145/3282307},
 acmid = {3282307},
 publisher = {ACM},
 
} 

@inproceedings{2015-pact-nvmmu,
author = {Zhang, Jie and Donofrio, David and Shalf, John and Kandemir, Mahmut T. and Jung, Myoungsoo},
title = {NVMMU: A Non-Volatile Memory Management Unit for Heterogeneous GPU-SSD Architectures},
year = {2015},
isbn = {9781467395243},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi-org.vu-nl.idm.oclc.org/10.1109/PACT.2015.43},
doi = {10.1109/PACT.2015.43},
abstract = {Thanks to massive parallelism in modern Graphics Processing Units (GPUs), emerging data processing applications in GPU computing exhibit ten-fold speedups compared to CPU-only systems. However, this GPU-based acceleration is limited in many cases by the significant data movement overheads and inefficient memory management for host-side storage accesses. To address these shortcomings, this paper proposes a non-volatile memory management unit (NVMMU) that reduces the file datamovement overheads by directly connecting the Solid State Disk (SSD) to the GPU. We implemented our proposed NVMMU on a real hardware with commercially available GPU and SSD devices by considering different types of storage interfaces and configurations. In this work, NVMMU unifies two discrete software stacks (one for the SSD and other for the GPU) in two major ways. While a new interface provided by our NVMMU directly forwards file data between the GPU runtime library and the I/O runtime library, it supports non-volatile direct memory access (NDMA) that pairs those GPU and SSD devices via physically shared system memory blocks. This unification in turn can eliminate unnecessary user/kernel-mode switching, improve memory management, and remove data copy overheads. Our evaluation results demonstrate that NVMMU can reduce the overheads of file data movement by 95% on average, improving overall system performance by 78% compared to a conventional IOMMU approach.},
booktitle = {Proceedings of the 2015 International Conference on Parallel Architecture and Compilation (PACT)},
pages = {13–24},
numpages = {12},
series = {PACT '15}
}

@ARTICLE{2022-ieee-ml-energy,  author={Patterson, David and Gonzalez, Joseph and Hölzle, Urs and Le, Quoc and Liang, Chen and Munguia, Lluis-Miquel and Rothchild, Daniel and So, David R. and Texier, Maud and Dean, Jeff},  journal={Computer},   title={The Carbon Footprint of Machine Learning Training Will Plateau, Then Shrink},   year={2022},  volume={55},  number={7},  pages={18-28},  doi={10.1109/MC.2022.3148714}}

@article{2020-cacm-dsa-supercompuer-ml,
  author    = {Norman P. Jouppi and
               Doe Hyun Yoon and
               George Kurian and
               Sheng Li and
               Nishant Patil and
               James Laudon and
               Cliff Young and
               David A. Patterson},
  title     = {A domain-specific supercomputer for training deep neural networks},
  journal   = {Commun. {ACM}},
  volume    = {63},
  number    = {7},
  pages     = {67--78},
  year      = {2020},
  url       = {https://doi.org/10.1145/3360307},
  doi       = {10.1145/3360307},
  timestamp = {Tue, 14 Jul 2020 08:50:56 +0200},
  biburl    = {https://dblp.org/rec/journals/cacm/JouppiYKLPLYP20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{2019-eurosys-timeos,
 author = {Ge, Qian and Yarom, Yuval and Chothia, Tom and Heiser, Gernot},
 title = {{Time Protection: The Missing OS Abstraction}},
 booktitle = {Proceedings of the Fourteenth EuroSys Conference 2019},
 series = {EuroSys '19},
 year = {2019},
 isbn = {978-1-4503-6281-8},
 address = {Dresden, Germany},
 pages = {1:1--1:17},
 articleno = {1},
 numpages = {17},
 url = {http://doi.acm.org/10.1145/3302424.3303976},
 doi = {10.1145/3302424.3303976},
 acmid = {3303976},
 publisher = {ACM},
 
 keywords = {confidentiality, covert channels, microkernels, seL4, security, temporal isolation, time protection, timing channels},
} 

@inproceedings{2019-sosp-netcache,
 author = {Jin, Xin and Li, Xiaozhou and Zhang, Haoyu and Soul{\'e}, Robert and Lee, Jeongkeun and Foster, Nate and Kim, Changhoon and Stoica, Ion},
 title = {{NetCache: Balancing Key-Value Stores with Fast In-Network Caching}},
 booktitle = {Proceedings of the 26th Symposium on Operating Systems Principles},
 series = {SOSP '17},
 year = {2017},
 isbn = {978-1-4503-5085-3},
 address = {Shanghai, China},
 pages = {121--136},
 numpages = {16},
 url = {http://doi.acm.org/10.1145/3132747.3132764},
 doi = {10.1145/3132747.3132764},
 acmid = {3132764},
 publisher = {ACM},
 
 keywords = {Caching, Key-value stores, Programmable switches},
} 

@inproceedings{2017-hotnets-in-net-dumb,
 author = {Sapio, Amedeo and Abdelaziz, Ibrahim and Aldilaijan, Abdulla and Canini, Marco and Kalnis, Panos},
 title = {{In-Network Computation is a Dumb Idea Whose Time Has Come}},
 booktitle = {Proceedings of the 16th ACM Workshop on Hot Topics in Networks},
 series = {HotNets-XVI},
 year = {2017},
 isbn = {978-1-4503-5569-8},
 address = {Palo Alto, CA, USA},
 pages = {150--156},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/3152434.3152461},
 doi = {10.1145/3152434.3152461},
 acmid = {3152461},
 publisher = {ACM},
 
} 

@inproceedings{2017-sosp-eris,
 author = {Li, Jialin and Michael, Ellis and Ports, Dan R. K.},
 title = {Eris: Coordination-Free Consistent Transactions Using In-Network Concurrency Control},
 booktitle = {Proceedings of the 26th Symposium on Operating Systems Principles},
 series = {SOSP '17},
 year = {2017},
 isbn = {978-1-4503-5085-3},
 address = {Shanghai, China},
 pages = {104--120},
 numpages = {17},
 url = {http://doi.acm.org/10.1145/3132747.3132751},
 doi = {10.1145/3132747.3132751},
 acmid = {3132751},
 publisher = {ACM},
 
 keywords = {distributed transactions, in-network concurrency control, network multi-sequencing},
} 

@inproceedings{2017-sosp-netcache,
 author = {Jin, Xin and Li, Xiaozhou and Zhang, Haoyu and Soul{\'e}, Robert and Lee, Jeongkeun and Foster, Nate and Kim, Changhoon and Stoica, Ion},
 title = {NetCache: Balancing Key-Value Stores with Fast In-Network Caching},
 booktitle = {Proceedings of the 26th Symposium on Operating Systems Principles},
 series = {SOSP '17},
 year = {2017},
 isbn = {978-1-4503-5085-3},
 address = {Shanghai, China},
 pages = {121--136},
 numpages = {16},
 url = {http://doi.acm.org/10.1145/3132747.3132764},
 doi = {10.1145/3132747.3132764},
 acmid = {3132764},
 publisher = {ACM},
 
 keywords = {Caching, Key-value stores, Programmable switches},
} 

@inproceedings{2017-sosp-kvdirect,
 author = {Li, Bojie and Ruan, Zhenyuan and Xiao, Wencong and Lu, Yuanwei and Xiong, Yongqiang and Putnam, Andrew and Chen, Enhong and Zhang, Lintao},
 title = {KV-Direct: High-Performance In-Memory Key-Value Store with Programmable NIC},
 booktitle = {Proceedings of the 26th Symposium on Operating Systems Principles},
 series = {SOSP '17},
 year = {2017},
 isbn = {978-1-4503-5085-3},
 address = {Shanghai, China},
 pages = {137--152},
 numpages = {16},
 url = {http://doi.acm.org/10.1145/3132747.3132756},
 doi = {10.1145/3132747.3132756},
 acmid = {3132756},
 publisher = {ACM},
 
 keywords = {Key-Value Store, Performance, Programmable Hardware},
} 

@inproceedings{2017-gpgpu-attacks,
 author = {Zhu, Zhiting and Kim, Sangman and Rozhanski, Yuri and Hu, Yige and Witchel, Emmett and Silberstein, Mark},
 title = {{Understanding The Security of Discrete GPUs}},
 booktitle = {Proceedings of the General Purpose GPUs},
 series = {GPGPU-10},
 year = {2017},
 isbn = {978-1-4503-4915-4},
 address = {Austin, TX, USA},
 pages = {1--11},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/3038228.3038233},
 doi = {10.1145/3038228.3038233},
 acmid = {3038233},
 publisher = {ACM},
 
} 

@inproceedings {2017-woot-flashhammer,
author = {Anil Kurmus and Nikolas Ioannou and Matthias Neugschwandtner and Nikolaos Papandreou and Thomas Parnell},
title = {{From random block corruption to privilege escalation: A filesystem attack vector for rowhammer-like attacks}},
booktitle = {11th {USENIX} Workshop on Offensive Technologies ({WOOT} 17)},
year = {2017},
address = {Vancouver, BC},
url = {https://www.usenix.org/conference/woot17/workshop-program/presentation/kurmus},

}

@inproceedings{2017-sosp-vmcontainer,
 author = {Manco, Filipe and Lupu, Costin and Schmidt, Florian and Mendes, Jose and Kuenzer, Simon and Sati, Sumit and Yasukata, Kenichi and Raiciu, Costin and Huici, Felipe},
 title = {My VM is Lighter (and Safer) Than Your Container},
 booktitle = {Proceedings of the 26th Symposium on Operating Systems Principles},
 series = {SOSP '17},
 year = {2017},
 isbn = {978-1-4503-5085-3},
 address = {Shanghai, China},
 pages = {218--233},
 numpages = {16},
 url = {http://doi.acm.org/10.1145/3132747.3132763},
 doi = {10.1145/3132747.3132763},
 acmid = {3132763},
 publisher = {ACM},
 
 keywords = {Virtualization, Xen, containers, hypervisor, operating systems, specialization, unikernels, virtual machine},
} 

@inproceedings{2017-atc-container,
 author = {Chen, Wei and Rao, Jia and Zhou, Xiaobo},
 title = {Preemptive, Low Latency Datacenter Scheduling via Lightweight Virtualization},
 booktitle = {Proceedings of the 2017 USENIX Conference on Usenix Annual Technical Conference},
 series = {USENIX ATC '17},
 year = {2017},
 isbn = {978-1-931971-38-6},
 address = {Santa Clara, CA, USA},
 pages = {251--263},
 numpages = {13},
 url = {http://dl.acm.org/citation.cfm?id=3154690.3154714},
 acmid = {3154714},
 
 
} 

@inproceedings{2017-atc-tlb,
 author = {Amit, Nadav},
 title = {{Optimizing the TLB Shootdown Algorithm with Page Access Tracking}},
 booktitle = {Proceedings of the 2017 USENIX Conference on Usenix Annual Technical Conference},
 series = {USENIX ATC '17},
 year = {2017},
 isbn = {978-1-931971-38-6},
 address = {Santa Clara, CA, USA},
 pages = {27--39},
 numpages = {13},
 url = {http://dl.acm.org/citation.cfm?id=3154690.3154694},
 acmid = {3154694},
  
} 

@inproceedings {2018-hotstorage-upi,
author = {Bryan S. Kim},
title = {Utilitarian Performance Isolation in Shared SSDs},
booktitle = {10th {USENIX} Workshop on Hot Topics in Storage and File Systems (HotStorage 18)},
year = {2018},
address = {Boston, MA},
url = {https://www.usenix.org/conference/hotstorage18/presentation/kim-bryan},

month = jul,
}

@inproceedings{2018-asplos-mask,
 author = {Ausavarungnirun, Rachata and Miller, Vance and Landgraf, Joshua and Ghose, Saugata and Gandhi, Jayneel and Jog, Adwait and Rossbach, Christopher J. and Mutlu, Onur},
 title = {{MASK: Redesigning the GPU Memory Hierarchy to Support Multi-Application Concurrency}},
 booktitle = {Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems},
 series = {ASPLOS '18},
 year = {2018},
 isbn = {978-1-4503-4911-6},
 address = {Williamsburg, VA, USA},
 pages = {503--518},
 numpages = {16},
 url = {http://doi.acm.org/10.1145/3173162.3173169},
 doi = {10.1145/3173162.3173169},
 acmid = {3173169},
 publisher = {ACM},
 
 keywords = {GPGPU applications, address translation, graphics processing units, memory interference, memory protection, memory systems, performance, virtual memory management},
} 

@Inbook{2018-springer-yarn-enh,
author="Karanasos, Konstantinos
and Suresh, Arun
and Douglas, Chris",
editor="Sakr, Sherif
and Zomaya, Albert",
title="Advancements in YARN Resource Manager",
bookTitle="Encyclopedia of Big Data Technologies",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="1--9",
isbn="978-3-319-63962-8",
doi="10.1007/978-3-319-63962-8_207-1",
url="https://doi.org/10.1007/978-3-319-63962-8_207-1"
}

@inproceedings{2018-sigcomm-hyperloop,
 author = {Kim, Daehyeok and Memaripour, Amirsaman and Badam, Anirudh and Zhu, Yibo and Liu, Hongqiang Harry and Padhye, Jitu and Raindel, Shachar and Swanson, Steven and Sekar, Vyas and Seshan, Srinivasan},
 title = {{Hyperloop: Group-based NIC-offloading to Accelerate Replicated Transactions in Multi-tenant Storage Systems}},
 booktitle = {Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication},
 series = {SIGCOMM '18},
 year = {2018},
 isbn = {978-1-4503-5567-4},
 address = {Budapest, Hungary},
 pages = {297--312},
 numpages = {16},
 url = {http://doi.acm.org/10.1145/3230543.3230572},
 doi = {10.1145/3230543.3230572},
 acmid = {3230572},
 publisher = {ACM},
 
 keywords = {NIC-offloading, RDMA, distributed storage systems, replicated transactions},
} 

@inproceedings {2018-atc-tailwind,
author = {Yacine Taleb and Ryan Stutsman and Gabriel Antoniu and Toni Cortes},
title = {{Tailwind: Fast and Atomic RDMA-based Replication}},
booktitle = {2018 {USENIX} Annual Technical Conference ({USENIX} {ATC} 18)},
year = {2018},
isbn = {978-1-931971-44-7},
address = {Boston, MA},
pages = {851--863},
url = {https://www.usenix.org/conference/atc18/presentation/taleb},

}

@article{2018-nethammer,
  author    = {Moritz Lipp and
               Misiker Tadesse Aga and
               Michael Schwarz and
               Daniel Gruss and
               Cl{\'{e}}mentine Maurice and
               Lukas Raab and
               Lukas Lamster},
  title     = {Nethammer: Inducing Rowhammer Faults through Network Requests},
  journal   = {CoRR},
  volume    = {abs/1805.04956},
  year      = {2018},
  url       = {http://arxiv.org/abs/1805.04956},
  archivePrefix = {arXiv},
  eprint    = {1805.04956},
  timestamp = {Mon, 13 Aug 2018 16:46:18 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1805-04956},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings {2018-meltdown,
author = {Moritz Lipp and Michael Schwarz and Daniel Gruss and Thomas Prescher and Werner Haas and Anders Fogh and Jann Horn and Stefan Mangard and Paul Kocher and Daniel Genkin and Yuval Yarom and Mike Hamburg},
title = {{Meltdown: Reading Kernel Memory from User Space}},
booktitle = {27th {USENIX} Security Symposium ({USENIX} Security 18)},
year = {2018},
isbn = {978-1-939133-04-5},
address = {Baltimore, MD},
pages = {973--990},
url = {https://www.usenix.org/conference/usenixsecurity18/presentation/lipp},

month = aug,
}

@INPROCEEDINGS{2018-spectre,
  author    = {Paul Kocher and
               Daniel Genkin and
               Daniel Gruss and
               Werner Haas and
               Mike Hamburg and
               Moritz Lipp and
               Stefan Mangard and
               Thomas Prescher and
               Michael Schwarz and
               Yuval Yarom},
booktitle={2019 IEEE Symposium on Security and Privacy (S\&P)},   
title={{Spectre Attacks: Exploiting Speculative Execution}},
year={2019},
pages={1-19},}

@inproceedings {2018-atc-throwhammer,
author = {Andrei Tatar and Radhesh Krishnan Konoth and Elias Athanasopoulos and Cristiano Giuffrida and Herbert Bos and Kaveh Razavi},
title = {{Throwhammer: Rowhammer Attacks over the Network and Defenses}},
booktitle = {2018 {USENIX} Annual Technical Conference ({USENIX} {ATC} 18)},
year = {2018},
isbn = {978-1-931971-44-7},
address = {Boston, MA},
pages = {213--226},
url = {https://www.usenix.org/conference/atc18/presentation/tatar},

}

@inproceedings{2018-sosr-shieldbox,
 author = {Trach, Bohdan and Krohmer, Alfred and Gregor, Franz and Arnautov, Sergei and Bhatotia, Pramod and Fetzer, Christof},
 title = {ShieldBox: Secure Middleboxes Using Shielded Execution},
 booktitle = {Proceedings of the Symposium on SDN Research},
 series = {SOSR '18},
 year = {2018},
 isbn = {978-1-4503-5664-0},
 address = {Los Angeles, CA, USA},
 pages = {2:1--2:14},
 articleno = {2},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/3185467.3185469},
 doi = {10.1145/3185467.3185469},
 acmid = {3185469},
 publisher = {ACM},
 
} 

@inproceedings {2018-osdi-floem,
author = {Phitchaya Mangpo Phothilimthana and Ming Liu and Antoine Kaufmann and Simon Peter and Rastislav Bodik and Thomas Anderson},
title = {{Floem: A Programming System for NIC-Accelerated Network Applications}},
booktitle = {13th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 18)},
year = {2018},
isbn = {978-1-939133-08-3},
address = {Carlsbad, CA},
pages = {663--679},
url = {https://www.usenix.org/conference/osdi18/presentation/phothilimthana},

month = oct,
}

@inproceedings {2018-osdi-legoos,
author = {Yizhou Shan and Yutong Huang and Yilun Chen and Yiying Zhang},
title = {{LegoOS: A Disseminated, Distributed {OS} for Hardware Resource Disaggregation}},
booktitle = {13th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 18)},
year = {2018},
isbn = {978-1-931971-47-8},
address = {Carlsbad, CA},
pages = {69--87},
url = {https://www.usenix.org/conference/osdi18/presentation/shan},

}

@inproceedings {2018-usenix-sec-tlb,
author = {Ben Gras and Kaveh Razavi and Herbert Bos and Cristiano Giuffrida},
title = {Translation Leak-aside Buffer: Defeating Cache Side-channel Protections with {TLB} Attacks},
booktitle = {27th {USENIX} Security Symposium ({USENIX} Security 18)},
year = {2018},
isbn = {978-1-931971-46-1},
address = {Baltimore, MD},
pages = {955--972},
url = {https://www.usenix.org/conference/usenixsecurity18/presentation/gras},

}

@inproceedings {2018-nsdi-mp-rdma,
author = {Yuanwei Lu and Guo Chen and Bojie Li and Kun Tan and Yongqiang Xiong and Peng Cheng and Jiansong Zhang and Enhong Chen and Thomas Moscibroda},
title = {Multi-Path Transport for {RDMA} in Datacenters},
booktitle = {15th {USENIX} Symposium on Networked Systems Design and Implementation ({NSDI} 18)},
year = {2018},
isbn = {978-1-939133-01-4},
address = {Renton, WA},
pages = {357--371},
url = {https://www.usenix.org/conference/nsdi18/presentation/lu},

}

@inproceedings {2018-nsdi-smartnic,
author = {Daniel Firestone and Andrew Putnam and Sambhrama Mundkur and Derek Chiou and Alireza Dabagh and Mike Andrewartha and Hari Angepat and Vivek Bhanu and Adrian Caulfield and Eric Chung and Harish Kumar Chandrappa and Somesh Chaturmohta and Matt Humphrey and Jack Lavier and Norman Lam and Fengfen Liu and Kalin Ovtcharov and Jitu Padhye and Gautham Popuri and Shachar Raindel and Tejas Sapre and Mark Shaw and Gabriel Silva and Madhan Sivakumar and Nisheeth Srivastava and Anshuman Verma and Qasim Zuhair and Deepak Bansal and Doug Burger and Kushagra Vaid and David A. Maltz and Albert Greenberg},
title = {{Azure Accelerated Networking: SmartNICs in the Public Cloud}},
booktitle = {15th {USENIX} Symposium on Networked Systems Design and Implementation ({NSDI} 18)},
year = {2018},
isbn = {978-1-939133-01-4},
address = {Renton, WA},
pages = {51--66},
url = {https://www.usenix.org/conference/nsdi18/presentation/firestone},

}

@inproceedings{2018-nsdi-clocksync,
 author = {Geng, Yilong and Liu, Shiyu and Yin, Zi and Naik, Ashish and Prabhakar, Balaji and Rosunblum, Mendel and Vahdat, Amin},
 title = {Exploiting a Natural Network Effect for Scalable, Fine-grained Clock Synchronization},
 booktitle = {Proceedings of the 15th USENIX Conference on Networked Systems Design and Implementation},
 series = {NSDI'18},
 year = {2018},
 isbn = {978-1-931971-43-0},
 address = {Renton, WA, USA},
 pages = {81--94},
 numpages = {14},
 url = {http://dl.acm.org/citation.cfm?id=3307441.3307449},
 acmid = {3307449},
  
} 

@inproceedings {2018-hotcloud-cachecloud,
author = {Shelby Thomas and Geoffrey M. Voelker and George Porter},
title = {CacheCloud: Towards Speed-of-light Datacenter Communication},
booktitle = {10th {USENIX} Workshop on Hot Topics in Cloud Computing (HotCloud 18)},
year = {2018},
address = {Boston, MA},
url = {https://www.usenix.org/conference/hotcloud18/presentation/thomas},

}

@article{2018-iacr-fpga,
  author    = {Jonas Krautter and
               Dennis R. E. Gnad and
               Mehdi Baradaran Tahoori},
  title     = {{FPGAhammer: Remote Voltage Fault Attacks on Shared FPGAs, suitable
               for {DFA} on {AES}}},
  journal   = {{IACR} Trans. Cryptogr. Hardw. Embed. Syst.},
  volume    = {2018},
  number    = {3},
  pages     = {44--68},
  year      = {2018},
  url       = {https://doi.org/10.13154/tches.v2018.i3.44-68},
  doi       = {10.13154/tches.v2018.i3.44-68},
  timestamp = {Tue, 30 Oct 2018 16:37:48 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/tches/KrautterGT18},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{2018-eurosys-dcat,
 author = {Xu, Cong and Rajamani, Karthick and Ferreira, Alexandre and Felter, Wesley and Rubio, Juan and Li, Yang},
 title = {{dCat: Dynamic Cache Management for Efficient, Performance-sensitive Infrastructure-as-a-service}},
 booktitle = {Proceedings of the Thirteenth EuroSys Conference},
 series = {EuroSys '18},
 year = {2018},
 isbn = {978-1-4503-5584-1},
 address = {Porto, Portugal},
 pages = {14:1--14:13},
 articleno = {14},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/3190508.3190555},
 doi = {10.1145/3190508.3190555},
 acmid = {3190555},
 publisher = {ACM},
 
} 

@article{2019-tos-venice,
 author = {Zhao, Boyan and Hou, Rui and Dong, Jianbo and Huang, Michael and Mckee, Sally A. and Zhang, Qianlong and Liu, Yueji and Li, Ye and Zhang, Lixin and Meng, Dan},
 title = {Venice: An Effective Resource Sharing Architecture for Data Center Servers},
 journal = {ACM Trans. Comput. Syst.},
 issue_date = {March 2019},
 volume = {36},
 number = {1},
 month = mar,
 year = {2019},
 issn = {0734-2071},
 pages = {2:1--2:26},
 articleno = {2},
 numpages = {26},
 url = {http://doi.acm.org/10.1145/3310360},
 doi = {10.1145/3310360},
 acmid = {3310360},
 publisher = {ACM},
 
 keywords = {Interconnect fabric, data center architecture, resource sharing, scheduling},
} 

@article{2019-cacm-arch,
 author = {Markettos, A. T. and Watson, R. N. M. and Moore, S. W. and Sewell, P. and Neumann, P. G.},
 title = {{Through Computer Architecture, Darkly}},
 journal = {Commun. ACM},
 issue_date = {June 2019},
 volume = {62},
 number = {6},
 month = may,
 year = {2019},
 issn = {0001-0782},
 pages = {25--27},
 numpages = {3},
 url = {http://doi.acm.org/10.1145/3325284},
 doi = {10.1145/3325284},
 acmid = {3325284},
 publisher = {ACM},
 
} 

@article{2019-cacm-prog-storage,
 author = {Do, Jaeyoung and Sengupta, Sudipta and Swanson, Steven},
 title = {{Programmable Solid-state Storage in Future Cloud Datacenters}},
 journal = {Commun. ACM},
 issue_date = {June 2019},
 volume = {62},
 number = {6},
 month = may,
 year = {2019},
 issn = {0001-0782},
 pages = {54--62},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/3286588},
 doi = {10.1145/3286588},
 acmid = {3286588},
 publisher = {ACM},
 
} 

@inproceedings{2019-asplos-xcontainer,
 author = {Shen, Zhiming and Sun, Zhen and Sela, Gur-Eyal and Bagdasaryan, Eugene and Delimitrou, Christina and Van Renesse, Robbert and Weatherspoon, Hakim},
 title = {X-Containers: Breaking Down Barriers to Improve Performance and Isolation of Cloud-Native Containers},
 booktitle = {Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems},
 series = {ASPLOS '19},
 year = {2019},
 isbn = {978-1-4503-6240-5},
 address = {Providence, RI, USA},
 pages = {121--135},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/3297858.3304016},
 doi = {10.1145/3297858.3304016},
 acmid = {3304016},
 publisher = {ACM},
 
 keywords = {cloud-native, containers, exokernel, library os, x-containers},
} 

@inproceedings{2019-asplos-parties,
 author = {Chen, Shuang and Delimitrou, Christina and Mart\'{\i}nez, Jos{\'e} F.},
 title = {{PARTIES: QoS-Aware Resource Partitioning for Multiple Interactive Services}},
 booktitle = {Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems},
 series = {ASPLOS '19},
 year = {2019},
 isbn = {978-1-4503-6240-5},
 address = {Providence, RI, USA},
 pages = {107--120},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/3297858.3304005},
 doi = {10.1145/3297858.3304005},
 acmid = {3304005},
 publisher = {ACM},
 
 keywords = {cloud computing, datacenters, interference, isolation, quality of service, resource management, resource partitioning},
} 

@inproceedings {2019-usenix-sec-pythia,
title = {{Pythia: Remote Oracles for the Masses}},
author = {Shin-Yeh Tsai and Mathias Payer and Yiying Zhang}, 
booktitle = {28th {USENIX} Security Symposium ({USENIX} Security 19)},
year = {2019},
address = {Santa Clara, CA},
url = {https://www.usenix.org/conference/usenixsecurity19/presentation/tsai},

}

@inproceedings{2019-nsdi-freeflow,
author = {Kim, Daehyeok and Yu, Tianlong and Liu, Hongqiang Harry and Zhu, Yibo and Padhye, Jitu and Raindel, Shachar and Guo, Chuanxiong and Sekar, Vyas and Seshan, Srinivasan},
title = {{Freeflow: Software-Based Virtual RDMA Networking for Containerized Clouds}},
year = {2019},
isbn = {9781931971492},

booktitle = {Proceedings of the 16th USENIX Conference on Networked Systems Design and Implementation},
pages = {113–125},
numpages = {13},
address = {Boston, MA, USA},
series = {NSDI’19}
}
  
@inproceedings {2019-nsdi-shoal,
author = {Vishal Shrivastav and Asaf Valadarsky and Hitesh Ballani and Paolo Costa and Ki Suh Lee and Han Wang and Rachit Agarwal and Hakim Weatherspoon},
title = {{Shoal: A Network Architecture for Disaggregated Racks}},
booktitle = {16th {USENIX} Symposium on Networked Systems Design and Implementation ({NSDI} 19)},
year = {2019},
isbn = {978-1-931971-49-2},
address = {Boston, MA},
pages = {255--270},
url = {https://www.usenix.org/conference/nsdi19/presentation/shrivastav},

}

@inproceedings {2019-hotcloud-rdma-sec,
author = {Shin-Yeh Tsai and Yiying Zhang},
title = {A Double-Edged Sword: Security Threats and Opportunities in One-Sided Network Communication},
booktitle = {11th {USENIX} Workshop on Hot Topics in Cloud Computing (HotCloud 19)},
year = {2019},
address = {Renton, WA},
url = {https://www.usenix.org/conference/hotcloud19/presentation/tsai},

}

@inproceedings{2019-hotos-isolation,
author = {Hunt, Tyler and Jia, Zhipeng and Miller, Vance and Rossbach, Christopher J. and Witchel, Emmett},
title = {{Isolation and Beyond: Challenges for System Security}},
year = {2019},
isbn = {9781450367271},
publisher = {ACM},
url = {https://doi.org/10.1145/3317550.3321427},
doi = {10.1145/3317550.3321427},
booktitle = {Proceedings of the Workshop on Hot Topics in Operating Systems},
pages = {96–104},
numpages = {9},
address = {Bertinoro, Italy},
series = {HotOS ’19}
}
  
@inproceedings{2019-hotos-prog-switch,
 author = {Ports, Dan R. K. and Nelson, Jacob},
 title = {{When Should The Network Be The Computer?}},
 booktitle = {Proceedings of the Workshop on Hot Topics in Operating Systems},
 series = {HotOS '19},
 year = {2019},
 isbn = {978-1-4503-6727-1},
 address = {Bertinoro, Italy},
 pages = {209--215},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/3317550.3321439},
 doi = {10.1145/3317550.3321439},
 acmid = {3321439},
 publisher = {ACM},
 
 keywords = {in-network computation, programmable switches, reconfigurable devices, smart NICs},
} 

@article{2019-xxx-gpu-interconnects,
  author    = {Ang Li and
               Shuaiwen Leon Song and
               Jieyang Chen and
               Jiajia Li and
               Xu Liu and
               Nathan R. Tallent and
               Kevin J. Barker},
  title     = {Evaluating Modern {GPU} Interconnect: PCIe, NVLink, NV-SLI, NVSwitch
               and GPUDirect},
  journal   = {CoRR},
  volume    = {abs/1903.04611},
  year      = {2019},
  url       = {http://arxiv.org/abs/1903.04611},
  archivePrefix = {arXiv},
  eprint    = {1903.04611},
  timestamp = {Thu, 13 Jun 2019 07:16:05 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1903-04611},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{2019-hotos-syncdc,
 author = {Yang, Tian and Gifford, Robert and Haeberlen, Andreas and Phan, Linh Thi Xuan},
 title = {{The Synchronous Data Center}},
 booktitle = {Proceedings of the Workshop on Hot Topics in Operating Systems},
 series = {HotOS '19},
 year = {2019},
 isbn = {978-1-4503-6727-1},
 address = {Bertinoro, Italy},
 pages = {142--148},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/3317550.3321442},
 doi = {10.1145/3317550.3321442},
 acmid = {3321442},
 publisher = {ACM},
 
} 

@inproceedings {2019-atc-insider,
author = {Zhenyuan Ruan and Tong He and Jason Cong},
title = {{{INSIDER}: Designing In-Storage Computing System for Emerging High-Performance Drive}},
booktitle = {2019 {USENIX} Annual Technical Conference ({USENIX} {ATC} 19)},
year = {2019},
isbn = {978-1-939133-03-8},
address = {Renton, WA},
pages = {379--394},
url = {https://www.usenix.org/conference/atc19/presentation/ruan},

}

@inproceedings {2019-atc-gg,
author = {Sadjad Fouladi and Francisco Romero and Dan Iter and Qian Li and Shuvo Chatterjee and Christos Kozyrakis and Matei Zaharia and Keith Winstein},
title = {{From Laptop to Lambda: Outsourcing Everyday Jobs to Thousands of Transient Functional Containers}},
booktitle = {2019 {USENIX} Annual Technical Conference ({USENIX} {ATC} 19)},
year = {2019},
isbn = {978-1-939133-03-8},
address = {Renton, WA},
pages = {475--488},
url = {https://www.usenix.org/conference/atc19/presentation/fouladi},

}

@inproceedings{2019-eurosys-most-llc,
 author = {Farshin, Alireza and Roozbeh, Amir and Maguire,Jr., Gerald Q. and Kosti\'{c}, Dejan},
 title = {{Make the Most out of Last Level Cache in Intel Processors}},
 booktitle = {Proceedings of the Fourteenth EuroSys Conference 2019},
 series = {EuroSys '19},
 year = {2019},
 isbn = {978-1-4503-6281-8},
 address = {Dresden, Germany},
 pages = {8:1--8:17},
 articleno = {8},
 numpages = {17},
 url = {http://doi.acm.org/10.1145/3302424.3303977},
 doi = {10.1145/3302424.3303977},
 acmid = {3303977},
 publisher = {ACM},
 
 keywords = {Cache Allocation Technology, Cache Partitioning, CacheDirector, DDIO, DPDK, Key-Value Store, Last Level Cache, Network Function Virtualization, Non-Uniform Cache Architecture, Slice-aware Memory Management},
} 

@inproceedings{2019-hotos-granular,
 author = {Lee, Collin and Ousterhout, John},
 title = {{Granular Computing}},
 booktitle = {Proceedings of the Workshop on Hot Topics in Operating Systems},
 series = {HotOS '19},
 year = {2019},
 isbn = {978-1-4503-6727-1},
 address = {Bertinoro, Italy},
 pages = {149--154},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/3317550.3321447},
 doi = {10.1145/3317550.3321447},
 acmid = {3321447},
 publisher = {ACM},
 
 keywords = {abstraction, burst, computing, datacenter, efficiency, granular, infrastructure, latency, software, task},
} 

@article{2019-berkeley-serverless,
  author    = {Eric Jonas and
               Johann Schleier{-}Smith and
               Vikram Sreekanti and
               Chia{-}che Tsai and
               Anurag Khandelwal and
               Qifan Pu and
               Vaishaal Shankar and
               Joao Carreira and
               Karl Krauth and
               Neeraja Jayant Yadwadkar and
               Joseph E. Gonzalez and
               Raluca Ada Popa and
               Ion Stoica and
               David A. Patterson},
  title     = {{Cloud Programming Simplified: {A} Berkeley View on Serverless Computing}},
  journal   = {CoRR},
  volume    = {abs/1902.03383},
  year      = {2019},
  url       = {http://arxiv.org/abs/1902.03383},
  archivePrefix = {arXiv},
  eprint    = {1902.03383},
  timestamp = {Tue, 21 May 2019 18:03:40 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1902-03383},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{2016-ieee-ibm-net-attached-fpga,  
author={Weerasinghe, Jagath and Polig, Raphael and Abel, Francois and Hagleitner, Christoph},  booktitle={2016 International Conference on Field-Programmable Technology (FPT)},   title={Network-attached FPGAs for data center applications},   year={2016},  volume={},  number={},  pages={36-43},  doi={10.1109/FPT.2016.7929186}}

@misc{2019-gpudirect-storage,
  author = {{NVIDIA}}, 
  title = {{GPUDirect Storage: A Direct Path Between Storage and GPU Memory}},
  howpublished = {\nolinkurl{https://developer.nvidia.com/blog/gpudirect-storage/}},
  note = {Accessed: 2022-Feb-02}
}

@conference {2020-zns-appends,
author = {Matias Bj{\o}rling},
title = {Zone Append: A New Way of Writing to Zoned Storage},
year = {2020},
address = {Santa Clara, CA},
publisher = {USENIX Association},
month = feb,
}

@INPROCEEDINGS{2011-hpdc-atomic-writes,  author={Ouyang, Xiangyong and Nellans, David and Wipfel, Robert and Flynn, David and Panda, Dhabaleswar K.},  booktitle={2011 IEEE 17th International Symposium on High Performance Computer Architecture},   title={Beyond block I/O: Rethinking traditional storage primitives},   year={2011},  volume={},  number={},  pages={301-311},  doi={10.1109/HPCA.2011.5749738}}

@misc{2022-xilinx-u280,
  authors = {Xilinx}, 
  title = {{Alveo U280 Data Center Accelerator Card}},
  howpublished = {\nolinkurl{https://www.xilinx.com/products/boards-and-kits/alveo/u280.html#specifications}},
  note = {Accessed: 2022-Feb-20}
}

@misc{2019-rise,
  title = {{Real-time intelligent secure explanable (RISE) systems, UC Berkeley}},
  howpublished = {\nolinkurl{https://rise.cs.berkeley.edu/}},
  note = {Accessed: 2019-05-20}
}

@misc{2022-dfg-cards,
  title = {{DFC Open Source Community}},
  howpublished = {\nolinkurl{https://github.com/DFC-OpenSource}},
  note = {Accessed: 2022-Feb-01}
}

@inproceedings {2021-atc-homa,
author = {John Ousterhout},
title = {A Linux Kernel Implementation of the Homa Transport Protocol},
booktitle = {2021 USENIX Annual Technical Conference (USENIX ATC 21)},
year = {2021},
isbn = {978-1-939133-23-6},
pages = {99--115},
url = {https://www.usenix.org/conference/atc21/presentation/ousterhout},
publisher = {USENIX Association},
month = jul,
}


@article{2019-mutlu-rowhammer,
  author    = {Onur Mutlu and
               Jeremie S. Kim},
  title     = {RowHammer: {A} Retrospective},
  journal   = {CoRR},
  volume    = {abs/1904.09724},
  year      = {2019},
  url       = {http://arxiv.org/abs/1904.09724},
  archivePrefix = {arXiv},
  eprint    = {1904.09724},
  timestamp = {Fri, 26 Apr 2019 13:18:53 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1904-09724},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{2018-cacm-modern-algo-ssd,
author = {Petrov, Alex},
title = {Algorithms Behind Modern Storage Systems: Different Uses for Read-Optimized B-Trees and Write-Optimized LSM-Trees},
year = {2018},
issue_date = {March-April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {2},
issn = {1542-7730},
url = {https://doi-org.vu-nl.idm.oclc.org/10.1145/3212477.3220266},
doi = {10.1145/3212477.3220266},
abstract = {This article takes a closer look at two storage system design approaches used in a majority of modern databases (read-optimized B-trees and write-optimized LSM (log-structured merge)-trees) and describes their use cases and tradeoffs.},
journal = {Queue},
month = {apr},
pages = {31–51},
numpages = {21}
}

@inproceedings{2021-systor-kv-ssd,
author = {Bhimani, Janki and Yang, Jingpei and Mi, Ningfang and Choi, Changho and Saha, Manoj and Maruf, Adnan},
title = {Fine-Grained Control of Concurrency within KV-SSDs},
year = {2021},
isbn = {9781450383981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.vu-nl.idm.oclc.org/10.1145/3456727.3463777},
doi = {10.1145/3456727.3463777},
abstract = {The development of KV-SSDs allows simplifying the I/O stack compared to the traditional block-based SSDs. We propose a novel Key-Value-based Storage infrastructure for Parallel Computing(KV-SiPC)-a framework for multi-thread OpenMP applications to use NVMe-based KV-SSDs. We design a new capability to execute workloads with multiple parallel data threads along with traditional parallel compute threads, that allow us to improve the overall throughput of applications, utilizing the maximum possible storage bandwidth. We implement our KV-SiPC infrastructure in a real system by extending various processing layers (e.g., program, OS, and device layers) and evaluate the performance of KV-SiPC by using block-based NVMe SSDs in the traditional I/O stack as a baseline for comparisons. The experimental results show that KV-SiPC can better utilize the available device bandwidth and significantly increases application I/O throughput.},
booktitle = {Proceedings of the 14th ACM International Conference on Systems and Storage},
articleno = {4},
numpages = {12},
keywords = {solid state drive (SSD), key-value SSD, multi-threading},
location = {Haifa, Israel},
series = {SYSTOR '21}
}

@inproceedings {2020-hotstorage-hayagui,
author = {Shinichi Awamoto and Erich Focht and Michio Honda},
title = {Designing a Storage Software Stack for Accelerators},
booktitle = {12th USENIX Workshop on Hot Topics in Storage and File Systems (HotStorage 20)},
year = {2020},
url = {https://www.usenix.org/conference/hotstorage20/presentation/awamoto},
publisher = {USENIX Association},
month = jul,
}

@inproceedings{2021-ieee-fermat,
  author    = {Yu Zou and
               Mingjie Lin},
  title     = {{FERMAT:} FPGA-Accelerated Heterogeneous Computing Platform Near NVMe
               Storage},
  booktitle = {29th {IEEE} Annual International Symposium on Field-Programmable Custom
               Computing Machines, {FCCM} 2021, Orlando, FL, USA, May 9-12, 2021},
  pages     = {262},
  publisher = {{IEEE}},
  year      = {2021},
  url       = {https://doi.org/10.1109/FCCM51124.2021.00049},
  doi       = {10.1109/FCCM51124.2021.00049},
  timestamp = {Mon, 07 Jun 2021 17:48:25 +0200},
  biburl    = {https://dblp.org/rec/conf/fccm/ZouL21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{2016-isca-biscuit,
author = {Gu, Boncheol and Yoon, Andre S. and Bae, Duck-Ho and Jo, Insoon and Lee, Jinyoung and Yoon, Jonghyun and Kang, Jeong-Uk and Kwon, Moonsang and Yoon, Chanho and Cho, Sangyeun and Jeong, Jaeheon and Chang, Duckhyun},
title = {Biscuit: A Framework for near-Data Processing of Big Data Workloads},
year = {2016},
isbn = {9781467389471},
publisher = {IEEE Press},
url = {https://doi-org.vu-nl.idm.oclc.org/10.1109/ISCA.2016.23},
doi = {10.1109/ISCA.2016.23},
abstract = {Data-intensive queries are common in business intelligence, data warehousing and analytics applications. Typically, processing a query involves full inspection of large in-storage data sets by CPUs. An intuitive way to speed up such queries is to reduce the volume of data transferred over the storage network to a host system. This can be achieved by filtering out extraneous data within the storage, motivating a form of near-data processing. This work presents Biscuit, a novel near-data processing framework designed for modern solid-state drives. It allows programmers to write a data-intensive application to run on the host system and the storage system in a distributed, yet seamless manner. In order to offer a high-level programming model, Biscuit builds on the concept of data flow. Data processing tasks communicate through typed and data-ordered ports. Biscuit does not distinguish tasks that run on the host system and the storage system. As the result, Biscuit has desirable traits like generality and expressiveness, while promoting code reuse and naturally exposing concurrency. We implement Biscuit on a host system that runs the Linux OS and a high-performance solid-state drive. We demonstrate the effectiveness of our approach and implementation with experimental results. When data filtering is done by hardware in the solid-state drive, the average speed-up obtained for the top five queries of TPC-H is over 15\texttimes{}.},
booktitle = {Proceedings of the 43rd International Symposium on Computer Architecture},
pages = {153–165},
numpages = {13},
keywords = {SSD, in-storage computing, near-data processing},
location = {Seoul, Republic of Korea},
series = {ISCA '16}
}


@inproceedings {2019-nsdi-azure-fpga,
author = {Ran Shu and Peng Cheng and Guo Chen and Zhiyuan Guo and Lei Qu and Yongqiang Xiong and Derek Chiou and Thomas Moscibroda},
title = {{Direct Universal Access: Making Data Center Resources Available to {FPGA}}},
booktitle = {16th {USENIX} Symposium on Networked Systems Design and Implementation ({NSDI} 19)},
year = {2019},
isbn = {978-1-931971-49-2},
address = {Boston, MA},
pages = {127--140},
url = {https://www.usenix.org/conference/nsdi19/presentation/shu},

}


@inproceedings{2018-hpc-opencl,
author = {Kobayashi, Ryohei and Oobata, Yuma and Fujita, Norihisa and Yamaguchi, Yoshiki and Boku, Taisuke},
title = {OpenCL-Ready High Speed FPGA Network for Reconfigurable High Performance Computing},
year = {2018},
isbn = {9781450353724},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3149457.3149479},
doi = {10.1145/3149457.3149479},
abstract = {Field programmable gate arrays (FPGAs) have gained attention in high-performance computing (HPC) research because their computation and communication capabilities have dramatically improved in recent years as a result of improvements to semiconductor integration technologies that depend on Moore's Law. In addition to FPGA performance improvements, OpenCL-based FPGA development toolchains have been developed and offered by FPGA vendors, which reduces the programming effort required as compared to the past. These improvements reveal the possibilities of realizing a concept to enable on-the-fly offloading computation at which CPUs/GPUs perform poorly to FPGAs while performing low-latency data movement. We think that this concept is one of the keys to more improve the performance of modern heterogeneous supercomputers using accelerators like GPUs. In this paper, we propose high-performance inter-FPGA Ethernet communication using OpenCL and Verilog HDL mixed programming in order to demonstrate the feasibility of realizing this concept. OpenCL is used to program application algorithms and data movement control when Verilog HDL is used to implement low-level components for Ethernet communication. Experimental results using ping-pong programs showed that our proposed approach achieves a latency of 0.99 μs and as much as 4.97 GB/s between FPGAs over different nodes, thus confirming that the proposed method is effective at realizing this concept.},
booktitle = {Proceedings of the International Conference on High Performance Computing in Asia-Pacific Region},
pages = {192–201},
numpages = {10},
keywords = {Accelerator computing, Heterogeneous cluster, FPGA, Verilog HDL, Interconnect for accelerators, OpenCL, I/O channels, Ethernet communication, BSP, QSFP+},
location = {Chiyoda, Tokyo, Japan},
series = {HPC Asia 2018}
}

@INPROCEEDINGS{2015-icdcs-rstore,  
author={\ulx{Trivedi, Animesh} and Stuedi, Patrick and Metzler, Bernard and Lutz, Clemens and Schmatz, Martin and Gross, Thomas R.},  booktitle={2015 IEEE 35th International Conference on Distributed Computing Systems},   title={RStore: A Direct-Access DRAM-based Data Store},   year={2015},  volume={},  number={},  pages={674-685},  doi={10.1109/ICDCS.2015.74}}

@inproceedings{2004-sc-gpu-cluster,
author = {Fan, Zhe and Qiu, Feng and Kaufman, Arie and Yoakum-Stover, Suzanne},
title = {GPU Cluster for High Performance Computing},
year = {2004},
isbn = {0769521533},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SC.2004.26},
doi = {10.1109/SC.2004.26},
abstract = {Inspired by the attractive Flops/dollar ratio and the incredible growth in the speed of modern graphics processing units (GPUs), we propose to use a cluster of GPUs for high performance scientific computing. As an example application, we have developed a parallel flow simulation using the lattice Boltzmann model (LBM) on a GPU cluster and have simulated the dispersion of airborne contaminants in the Times Square area of New York City. Using 30 GPU nodes, our simulation can compute a 480x400x80 LBM in 0.31second/step, a speed which is 4.6 times faster than that of our CPU cluster implementation. Besides the LBM, we also discuss other potential applications of the GPU cluster, such as cellular automata, PDE solvers, and FEM.},
booktitle = {Proceedings of the 2004 ACM/IEEE Conference on Supercomputing},
pages = {47},
keywords = {urban airborne dispersion, GPU cluster, computational fluid dynamics, data intensive computing, lattice Boltzmann model},
series = {SC '04}
}

@inproceedings{2019-fpga-supercomputer,
author = {Chen, Deming},
title = {FPGAs in Supercomputers: Opportunity or Folly?},
year = {2019},
isbn = {9781450361378},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3289602.3293929},
doi = {10.1145/3289602.3293929},
abstract = {Supercomputers play an important role in the field of computational science, and are used for a wide range of computationally intensive tasks in various fields. So far, the building blocks for supercomputers have been dominated by CPUs and GPUs. Although FPGAs start to play an important role for cloud computing such as those used in AWS and Microsoft Azure, FPGAs haven't been adopted to build top supercomputers yet. In this evening panel, panelists from industry, research institute, and academia will attempt to answer the following questions, propose actions, and present their point of view through lively discussions and debates. 1)Is there a need to bring FPGAs into supercomputers? Why or why not? 2)Are there unique applications that are specifically suitable for FPGAs for supercomputing fields? 3)What are the challenges and/or major issues facing FPGAs for supporting supercomputing? 4)What and where are the opportunities? Who are the stakeholders? 5)Name one thing that the FPGA industry should (or should not) do in the near term to facilitate FPGA's induction into supercomputers.},
booktitle = {Proceedings of the 2019 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
pages = {201},
numpages = {1},
keywords = {fpgas, supercomputer, reconfigurable computing},
location = {Seaside, CA, USA},
series = {FPGA '19}
}

@inproceedings {2022-atc-fpga-nics,
author = {Zeke Wang and Hongjing Huang and Jie Zhang and Fei Wu and Gustavo Alonso},
title = {{FpgaNIC}: An {FPGA-based} Versatile 100Gb {SmartNIC} for {GPUs}},
booktitle = {2022 USENIX Annual Technical Conference (USENIX ATC 22)},
year = {2022},
isbn = {978-1-939133-29-25},
address = {Carlsbad, CA},
pages = {967--986},
url = {https://www.usenix.org/conference/atc22/presentation/wang-zeke},
publisher = {USENIX Association},
month = jul,
}

@article{2014-vldb-ibex,
  author    = {Louis Woods and
               Zsolt Istv{\'{a}}n and
               Gustavo Alonso},
  title     = {Ibex - An Intelligent Storage Engine with Support for Advanced {SQL}
               Off-loading},
  journal   = {Proc. {VLDB} Endow.},
  volume    = {7},
  number    = {11},
  pages     = {963--974},
  year      = {2014},
  url       = {http://www.vldb.org/pvldb/vol7/p963-woods.pdf},
  doi       = {10.14778/2732967.2732972},
  timestamp = {Tue, 16 Aug 2022 23:06:05 +0200},
  biburl    = {https://dblp.org/rec/journals/pvldb/WoodsIA14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings {2019-fast-orion,
author = {Jian Yang and Joseph Izraelevitz and Steven Swanson},
title = {{Orion: A Distributed File System for Non-Volatile Main Memory and RDMA-Capable Networks}},
booktitle = {17th {USENIX} Conference on File and Storage Technologies ({FAST} 19)},
year = {2019},
isbn = {978-1-931971-48-5},
address = {Boston, MA},
pages = {221--234},
url = {https://www.usenix.org/conference/fast19/presentation/yang},

}

@inproceedings {2019-fast-speicher,
author = {Maurice Bailleu and J\'org Thalheim and Pramod Bhatotia and Christof Fetzer and Michio Honda and Kapil Vaswani},
title = {{SPEICHER}: Securing LSM-based Key-Value Stores using Shielded Execution},
booktitle = {17th {USENIX} Conference on File and Storage Technologies ({FAST} 19)},
year = {2019},
isbn = {978-1-931971-48-5},
address = {Boston, MA},
pages = {173--190},
url = {https://www.usenix.org/conference/fast19/presentation/bailleu},

}


@inproceedings{frigo2018,
  title = {Grand {{Pwning Unit}}: {{Accelerating Microarchitectural Attacks}} with the {{GPU}}},  
  booktitle={2018 IEEE Symposium on Security and Privacy (S\&P)},   
  author = {Frigo, Pietro and Giuffrida, Cristiano and Bos, Herbert and Razavi, Kaveh},
  year={2018},
  pages={195-210},
}

@inproceedings{netcat,
  title = {{NetCAT: Practical Cache Attacks from the Network}},
  booktitle={2020 IEEE Symposium on Security and Privacy (S\&P)},
  year={2020},
  author = {Kurth, Michael and Gras, Ben and Andriesse, Dennis and Giuffrida, Cristiano and Bos, Herbert and Razavi, Kaveh},
}

@inproceedings{zebram,
author = {Konoth, Radhesh Krishnan and Oliverio, Marco and Tatar, Andrei and Andriesse, Dennis and Bos, Herbert and Giuffrida, Cristiano and Razavi, Kaveh},
title = {{{ZebRAM}}: {{Comprehensive}} and {{Compatible Software Protection Against Rowhammer Attacks}}},
year = {2018},
isbn = {9781931971478},

booktitle = {Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation},
pages = {697–710},
numpages = {14},
address = {Carlsbad, CA, USA},
series = {OSDI’18}
}

@inproceedings{vusion,
author = {Oliverio, Marco and Razavi, Kaveh and Bos, Herbert and Giuffrida, Cristiano},
title = {{Secure Page Fusion with VUsion}},
year = {2017},
isbn = {9781450350853},
publisher = {ACM},

url = {https://doi.org/10.1145/3132747.3132781},
doi = {10.1145/3132747.3132781},
booktitle = {Proceedings of the 26th Symposium on Operating Systems Principles},
pages = {531–545},
numpages = {15},
keywords = {Rowhammer, Side channels, Memory Management, Page Fusion},
address = {Shanghai, China},
series = {SOSP ’17}
}

@inproceedings{cloak,
author = {Gruss, Daniel and Lettner, Julian and Schuster, Felix and Ohrimenko, Olga and Haller, Istvan and Costa, Manuel},
title = {{Strong and Efficient Cache Side-Channel Protection Using Hardware Transactional Memory}},
year = {2017},
isbn = {9781931971409},

booktitle = {Proceedings of the 26th USENIX Conference on Security Symposium},
pages = {217–233},
numpages = {17},
address = {Vancouver, BC, Canada},
series = {SEC’17}
}

@inproceedings{liu2016catalyst,
  title={{CATalyst: Defeating Last-Level Cache Side Channel Attacks in Cloud Computing}},
  author={Liu, Fangfei and Ge, Qian and Yarom, Yuval and Mckeen, Frank and Rozas, Carlos and Heiser, Gernot and Lee, Ruby B},
  booktitle={IEEE International Symposium on High Performance Computer Architecture (HPCA)},  
  series={HPCA'16},
  pages={406--418},
   year={2016},
}

% commit to the bib template from here 

@inproceedings{2021-hotos-ebpf-exo-os,
author = {Zhong, Yuhong and Wang, Hongyi and Wu, Yu Jian and Cidon, Asaf and Stutsman, Ryan and Tai, Amy and Yang, Junfeng},
title = {BPF for Storage: An Exokernel-Inspired Approach},
year = {2021},
isbn = {9781450384384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458336.3465290},
doi = {10.1145/3458336.3465290},
abstract = {The overhead of the kernel storage path accounts for half of the access latency for new NVMe storage devices. We explore using BPF to reduce this overhead, by injecting user-defined functions deep in the kernel's I/O processing stack. When issuing a series of dependent I/O requests, this approach can increase IOPS by over 2.5X and cut latency by half, by bypassing kernel layers and avoiding user-kernel boundary crossings. However, we must avoid losing important properties when bypassing the file system and block layer such as the safety guarantees of the file system and translation between physical blocks addresses and file offsets. We sketch potential solutions to these problems, inspired by exokernel file systems from the late 90s, whose time, we believe, has finally come!"As a dog returns to his vomit, so a fool repeats his folly."Attributed to King Solomon},
booktitle = {Proceedings of the Workshop on Hot Topics in Operating Systems},
pages = {128–135},
numpages = {8},
keywords = {BPF, non-volatile memory, eBPF, storage, NVM},
location = {Ann Arbor, Michigan},
series = {HotOS '21}
}

@misc{bpf-seccomp,
 title = {Using seccomp to limit the kernel attack surface},
 author = {Michael Kerrisk},
 howpublished={Linux Plumbers Conference},
 note={Accessed: 2022-Feb-02, \nolinkurl{https://man7.org/conf/lpc2015/limiting_kernel_attack_surface_with_seccomp-LPC_2015-Kerrisk.pdf}},
 year={2015},
}

@inproceedings{2020-middleware-blockNDP,
author = {Barbalace, Antonio and Decky, Martin and Picorel, Javier and Bhatotia, Pramod},
title = {BlockNDP: Block-Storage Near Data Processing},
year = {2020},
isbn = {9781450382014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.vu-nl.idm.oclc.org/10.1145/3429357.3430519},
doi = {10.1145/3429357.3430519},
abstract = {We introduce blockNDP, a set of hardware and software protocol and interface extensions to enable block-oriented near data processing (NDP) within emerging solid-state drives (SSDs) integrating processing units. blockNDP is an end-to-end architecture, including a programming model and framework for host and in-storage compute units. blockNDP is the first proposal to be generic, backward compatible - supports existent standards and file systems, portable, and language agnostic. B@We have implemented and evaluated the effectiveness of blockNDP using two full-system prototypes; one on a new NDP emulation platform - developed atop QEMU, and another on an actual development board - the OpenSSD. Our experimental evaluation demonstrates that blockNDP is not only feasible with a modest effort, &lt; 10K LoC, but it is also able to achieve up to 4\texttimes{} speedups, despite wimpy CPU cores, on a real-world data-driven application such as MySQL.},
booktitle = {Proceedings of the 21st International Middleware Conference Industrial Track},
pages = {8–15},
numpages = {8},
location = {Delft, Netherlands},
series = {Middleware '20}
}

@inproceedings{2019-atc-ebpf-cache,
	title={Extension framework for file systems in user space},
	author={Bijlani, Ashish and Ramachandran, Umakishore},
	booktitle={2019 $\{$USENIX$\}$ Annual Technical Conference ($\{$USENIX$\}$$\{$ATC$\}$ 19)},
	pages={121--134},
	year={2019}
}

@misc{2022-bpf-tracing,
 title = {{Linux Enhanced BPF (eBPF) Tracing Tools}},
 author={Brendan D. Gregg},
 note={Accessed: 2022-Feb-02, \nolinkurl{http://www.brendangregg.com/ebpf.html}},
}

@misc{2022-pcie-xover,
 title = {{PCIe x16 Lanes Crossover adapter board for NVMe-IP evaluation}},
 author={{Design Gateway}},
 note={Accessed: 2022-Feb-02, \nolinkurl{https://eu.mouser.com/datasheet/2/854/AB18-PCIEx16-MAN-E-1594818.pdf}},
}

@inproceedings{2018-conext-xdp,
  author    = {H\o{}iland-J\o{}rgensen, Toke and Brouer, Jesper Dangaard and Borkmann, Daniel and Fastabend, John and Herbert, Tom and Ahern, David and Miller, David},
  title     = {The EXpress Data Path: Fast Programmable Packet Processing in the Operating System Kernel},
  year      = {2018},
  isbn      = {9781450360807},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3281411.3281443},
  doi       = {10.1145/3281411.3281443},
  booktitle = {Proceedings of the 14th International Conference on Emerging Networking EXperiments and Technologies},
  pages     = {54–66},
  numpages  = {13},
  keywords  = {programmable networking, XDP, DPDK, BPF},
  location  = {Heraklion, Greece},
  series    = {CoNEXT '18}
}

@inproceedings{1967-cc-amdahl,
author = {Amdahl, Gene M.},
title = {Validity of the Single Processor Approach to Achieving Large Scale Computing Capabilities},
year = {1967},
isbn = {9781450378956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1465482.1465560},
doi = {10.1145/1465482.1465560},
booktitle = {Proceedings of the April 18-20, 1967, Spring Joint Computer Conference},
pages = {483–485},
numpages = {3},
location = {Atlantic City, New Jersey},
series = {AFIPS '67 (Spring)}
}

@inproceedings{10.1145/285237.285283,
author = {Lakshman, T. V. and Stiliadis, D.},
title = {High-Speed Policy-Based Packet Forwarding Using Efficient Multi-Dimensional Range Matching},
year = {1998},
isbn = {1581130031},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/285237.285283},
doi = {10.1145/285237.285283},
abstract = {The ability to provide differentiated services to users with widely varying requirements is becoming increasingly important, and Internet Service Providers would like to provide these differentiated services using the same shared network infrastructure. The key mechanism, that enables differentiation in a connectionless network, is the packet classification function that parses the headers of the packets, and after determining their context, classifies them based on administrative policies or real-time reservation decisions. Packet classification, however, is a complex operation that can become the bottleneck in routers that try to support gigabit link capacities. Hence, many proposals for differentiated services only require classification at lower speed edge routers and also avoid classification based on multiple fields in the packet header even if it might be advantageous to service providers. In this paper, we present new packet classification schemes that, with a worst-case and traffic-independent performance metric, can classify packets, by checking amongst a few thousand filtering rules, at rates of a million packets per second using range matches on more than 4 packet header fields. For a special case of classification in two dimensions, we present an algorithm that can handle more than 128K rules at these speeds in a traffic independent manner. We emphasize worst-case performance over average case performance because providing differentiated services requires intelligent queueing and scheduling of packets that precludes any significant queueing before the differentiating step (i.e., before packet classification). The presented filtering or classification schemes can be used to classify packets for security policy enforcement, applying resource management decisions, flow identification for RSVP reservations, multicast look-ups, and for source-destination and policy based routing. The scalability and performance of the algorithms have been demonstrated by implementation and testing in a prototype system.},
booktitle = {Proceedings of the ACM SIGCOMM '98 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication},
pages = {203–214},
numpages = {12},
location = {Vancouver, British Columbia, Canada},
series = {SIGCOMM '98}
}

@article{1998-LBVS,
author = {Lakshman, T. V. and Stiliadis, D.},
title = {High-Speed Policy-Based Packet Forwarding Using Efficient Multi-Dimensional Range Matching},
year = {1998},
issue_date = {Oct. 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {4},
issn = {0146-4833},
url = {https://doi.org/10.1145/285243.285283},
doi = {10.1145/285243.285283},
abstract = {The ability to provide differentiated services to users with widely varying requirements is becoming increasingly important, and Internet Service Providers would like to provide these differentiated services using the same shared network infrastructure. The key mechanism, that enables differentiation in a connectionless network, is the packet classification function that parses the headers of the packets, and after determining their context, classifies them based on administrative policies or real-time reservation decisions. Packet classification, however, is a complex operation that can become the bottleneck in routers that try to support gigabit link capacities. Hence, many proposals for differentiated services only require classification at lower speed edge routers and also avoid classification based on multiple fields in the packet header even if it might be advantageous to service providers. In this paper, we present new packet classification schemes that, with a worst-case and traffic-independent performance metric, can classify packets, by checking amongst a few thousand filtering rules, at rates of a million packets per second using range matches on more than 4 packet header fields. For a special case of classification in two dimensions, we present an algorithm that can handle more than 128K rules at these speeds in a traffic independent manner. We emphasize worst-case performance over average case performance because providing differentiated services requires intelligent queueing and scheduling of packets that precludes any significant queueing before the differentiating step (i.e., before packet classification). The presented filtering or classification schemes can be used to classify packets for security policy enforcement, applying resource management decisions, flow identification for RSVP reservations, multicast look-ups, and for source-destination and policy based routing. The scalability and performance of the algorithms have been demonstrated by implementation and testing in a prototype system.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = {oct},
pages = {203–214},
numpages = {12}
}

@inproceedings{2020-sigmod-pump-up,
author = {Lutz, Clemens and Bre\ss{}, Sebastian and Zeuch, Steffen and Rabl, Tilmann and Markl, Volker},
title = {Pump Up the Volume: Processing Large Data on GPUs with Fast Interconnects},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.vu-nl.idm.oclc.org/10.1145/3318464.3389705},
doi = {10.1145/3318464.3389705},
abstract = {GPUs have long been discussed as accelerators for database query processing because of their high processing power and memory bandwidth. However, two main challenges limit the utility of GPUs for large-scale data processing: (1) the on-board memory capacity is too small to store large data sets, yet (2) the interconnect bandwidth to CPU main-memory is insufficient for ad hoc data transfers. As a result, GPU-based systems and algorithms run into a transfer bottleneck and do not scale to large data sets. In practice, CPUs process large-scale data faster than GPUs with current technology. In this paper, we investigate how a fast interconnect can resolve these scalability limitations using the example of NVLink 2.0. NVLink 2.0 is a new interconnect technology that links dedicated GPUs to a CPU@. The high bandwidth of NVLink 2.0 enables us to overcome the transfer bottleneck and to efficiently process large data sets stored in main-memory on GPUs. We perform an in-depth analysis of NVLink 2.0 and show how we can scale a no-partitioning hash join beyond the limits of GPU memory. Our evaluation shows speed-ups of up to 18x over PCI-e 3.0 and up to 7.3x over an optimized CPU implementation. Fast GPU interconnects thus enable GPUs to efficiently accelerate query processing.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1633–1649},
numpages = {17},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings {2020-osdi-os-on-fpga,
author = {Dario Korolija and Timothy Roscoe and Gustavo Alonso},
title = {Do {OS} abstractions make sense on {FPGAs}?},
booktitle = {14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20)},
year = {2020},
isbn = {978-1-939133-19-9},
pages = {991--1010},
url = {https://www.usenix.org/conference/osdi20/presentation/roscoe},
publisher = {USENIX Association},
month = nov,
}


@inproceedings {2021-atc-fcsv-virt-sto-fpga,
author = {Dongup Kwon and Dongryeong Kim and Junehyuk Boo and Wonsik Lee and Jangwoo Kim},
title = {A Fast and Flexible Hardware-based Virtualization Mechanism for Computational Storage Devices},
booktitle = {2021 USENIX Annual Technical Conference (USENIX ATC 21)},
year = {2021},
isbn = {978-1-939133-23-6},
pages = {729--743},
url = {https://www.usenix.org/conference/atc21/presentation/kwon},
publisher = {USENIX Association},
month = jul,
}

@inproceedings {2020-hotcloud-stratus,
author = {Kaveh Razavi and \ulx{Animesh Trivedi}},
title = {Stratus: Clouds with Microarchitectural Resource Management},
booktitle = {12th USENIX Workshop on Hot Topics in Cloud Computing (HotCloud 20)},
year = {2020},
url = {https://www.usenix.org/conference/hotcloud20/presentation/razavi},
publisher = {USENIX Association},
month = jul,
}

@inbook{2021-asplos-compiler-fpga-ctx,
author = {Landgraf, Joshua and Yang, Tiffany and Lin, Will and Rossbach, Christopher J. and Schkufza, Eric},
title = {Compiler-Driven FPGA Virtualization with SYNERGY},
year = {2021},
isbn = {9781450383172},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3445814.3446755},
booktitle = {Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {818–831},
numpages = {14}
}

@inproceedings{2012-fast-Shredder,
author = {Bhatotia, Pramod and Rodrigues, Rodrigo and Verma, Akshat},
title = {Shredder: GPU-Accelerated Incremental Storage and Computation},
year = {2012},
publisher = {USENIX Association},
address = {USA},
abstract = {Redundancy elimination using data deduplication and incremental data processing has emerged as an important technique to minimize storage and computation requirements in data center computing. In this paper, we present the design, implementation and evaluation of Shredder, a high performance content-based chunking framework for supporting incremental storage and computation systems. Shredder exploits the massively parallel processing power of GPUs to overcome the CPU bottlenecks of content-based chunking in a cost-effective manner. Unlike previous uses of GPUs, which have focused on applications where computation costs are dominant, Shredder is designed to operate in both compute-and dataintensive environments. To allow this, Shredder provides several novel optimizations aimed at reducing the cost of transferring data between host (CPU) and GPU, fully utilizing the multicore architecture at the host, and reducing GPU memory access latencies. With our optimizations, Shredder achieves a speedup of over 5X for chunking bandwidth compared to our optimized parallel implementation without a GPU on the same host system. Furthermore, we present two real world applications of Shredder: an extension to HDFS, which serves as a basis for incremental MapReduce computations, and an incremental cloud backup system. In both contexts, Shredder detects redundancies in the input data across successive runs, leading to significant savings in storage, computation, and end-to-end completion times.},
booktitle = {Proceedings of the 10th USENIX Conference on File and Storage Technologies},
pages = {14},
numpages = {1},
location = {San Jose, CA},
series = {FAST'12}
}

@inproceedings{2013-asplos-gpufs,
author = {Silberstein, Mark and Ford, Bryan and Keidar, Idit and Witchel, Emmett},
title = {GPUfs: Integrating a File System with GPUs},
year = {2013},
isbn = {9781450318709},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.vu-nl.idm.oclc.org/10.1145/2451116.2451169},
doi = {10.1145/2451116.2451169},
abstract = {PU hardware is becoming increasingly general purpose, quickly outgrowing the traditional but constrained GPU-as-coprocessor programming model. To make GPUs easier to program and easier to integrate with existing systems, we propose making the host's file system directly accessible from GPU code. GPUfs provides a POSIX-like API for GPU programs, exploits GPU parallelism for efficiency, and optimizes GPU file access by extending the buffer cache into GPU memory. Our experiments, based on a set of real benchmarks adopted to use our file system, demonstrate the feasibility and benefits of our approach. For example, we demonstrate a simple self-contained GPU program which searches for a set of strings in the entire tree of Linux kernel source files over seven times faster than an eight-core CPU run.},
booktitle = {Proceedings of the Eighteenth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {485–498},
numpages = {14},
keywords = {operating systems design, gpgpus, accelerators, file systems},
location = {Houston, Texas, USA},
series = {ASPLOS '13}
}

@inproceedings{2021-hotnets-persistent-packets,
author = {Honda, Michio},
title = {Packets as Persistent In-Memory Data Structures},
year = {2021},
isbn = {9781450390873},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3484266.3487386},
doi = {10.1145/3484266.3487386},
abstract = {Networked storage applications cannot fully benefit from fast persistent memory (PM), because of data management overheads incurred to implement storage properties, such as integrity, consistency, search efficiency and flexibility. To address this problem, we explore a new approach that turns networking overheads into assets, repurposing the transport protocol and network stack features, some of which can be offloaded to the NIC hardware, for implementing the storage properties particularly for the PM devices.},
booktitle = {Proceedings of the Twentieth ACM Workshop on Hot Topics in Networks},
pages = {31–37},
numpages = {7},
keywords = {transport protocols, Persistent memory},
location = {Virtual Event, United Kingdom},
series = {HotNets '21}
}

@inproceedings{2021-hotos-zone,
author = {Stavrinos, Theano and Berger, Daniel S. and Katz-Bassett, Ethan and Lloyd, Wyatt},
title = {Don't Be a Blockhead: Zoned Namespaces Make Work on Conventional SSDs Obsolete},
year = {2021},
isbn = {9781450384384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458336.3465300},
doi = {10.1145/3458336.3465300},
abstract = {Research on flash devices almost exclusively focuses on conventional SSDs, which expose a block interface. Industry, however, has standardized and is adopting Zoned Namespaces (ZNS) SSDs, which offer a new storage interface that dominates conventional SSDs. Continued research on conventional SSDs is thus a missed opportunity to unlock a step-change improvement in system performance by building on ZNS SSDs. We argue for an immediate and complete shift in research to ZNS SSDs and discuss research directions.},
booktitle = {Proceedings of the Workshop on Hot Topics in Operating Systems},
pages = {144–151},
numpages = {8},
location = {Ann Arbor, Michigan},
series = {HotOS '21}
}

@inproceedings{2021-hotos-last-cpu,
author = {Nider, Joel and Fedorova, Alexandra (Sasha)},
title = {The Last CPU},
year = {2021},
isbn = {9781450384384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458336.3465291},
doi = {10.1145/3458336.3465291},
abstract = {Since the end of Dennard scaling and Moore's Law have been foreseen, specialized hardware has become the focus for continued scaling of application performance. Programmable accelerators such as smart memory, smart disks, and smart NICs are now being integrated into our systems. Many accelerators can be programmed to process their data autonomously and require little or no intervention during normal operation. In this way, entire applications are offloaded, leaving the CPU with the minimal responsibilities of initialization, coordination and error handling.We claim that these responsibilities can also be handled in simple hardware other than the CPU and that it is wasteful to use a CPU for these purposes. We explore the role and the structure of the OS in a system that has no CPU and demonstrate that all necessary functionality can be moved to other hardware. We show that almost all of the pieces for such a system design are already available today. The responsibilities of the operating system must be split between self-managing devices and a system bus that handles privileged operations.},
booktitle = {Proceedings of the Workshop on Hot Topics in Operating Systems},
pages = {1–8},
numpages = {8},
keywords = {decentralized, operating system, CPU},
location = {Ann Arbor, Michigan},
series = {HotOS '21}
}

@inproceedings{2013-systor-corfu-hardware,
author = {Wei, Michael and Davis, John D. and Wobber, Ted and Balakrishnan, Mahesh and Malkhi, Dahlia},
title = {Beyond Block I/O: Implementing a Distributed Shared Log in Hardware},
year = {2013},
isbn = {9781450321167},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2485732.2485739},
doi = {10.1145/2485732.2485739},
abstract = {The basic block I/O interface used for interacting with storage devices hasn't changed much in 30 years. With the advent of very fast I/O devices based on solid-state memory, it becomes increasingly attractive to make many devices directly and concurrently available to many clients. However, when multiple clients share media at fine grain, retaining data consistency is problematic: SCSI, IDE, and their descendants don't offer much help. We propose an interface to networked storage that reduces an existing software implementation of a distributed shared log to hardware. Our system achieves both scalable throughput and strong consistency, while obtaining significant benefits in cost and power over the software implementation.},
booktitle = {Proceedings of the 6th International Systems and Storage Conference},
articleno = {21},
numpages = {11},
location = {Haifa, Israel},
series = {SYSTOR '13}
}

@inproceedings {2022-atc-ebpf-wrapping,
author = {Marco Bonola and Giacomo Belocchi and Angelo Tulumello and Marco Spaziani Brunella and Giuseppe Siracusano and Giuseppe Bianchi and Roberto Bifulco},
title = {Faster Software Packet Processing on {FPGA} {NICs} with {eBPF} Program Warping},
booktitle = {2022 USENIX Annual Technical Conference (USENIX ATC 22)},
year = {2022},
isbn = {978-1-939133-29-58},
address = {Carlsbad, CA},
pages = {987--1004},
url = {https://www.usenix.org/conference/atc22/presentation/bonola},
publisher = {USENIX Association},
month = jul,
}

@inproceedings{2019-asplos-lightstore,
author = {Chung, Chanwoo and Koo, Jinhyung and Im, Junsu and Arvind and Lee, Sungjin},
title = {LightStore: Software-Defined Network-Attached Key-Value Drives},
year = {2019},
isbn = {9781450362405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297858.3304022},
doi = {10.1145/3297858.3304022},
booktitle = {Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {939–953},
numpages = {15},
keywords = {hardware ftl, key-value flash drive, lsm-tree},
location = {Providence, RI, USA},
series = {ASPLOS '19}
}

@inproceedings {2020-atc-twizzler,
author = {Daniel Bittman and Peter Alvaro and Pankaj Mehra and Darrell D. E. Long and Ethan L. Miller},
title = {Twizzler: a {Data-Centric} {OS} for {Non-Volatile} Memory},
booktitle = {2020 USENIX Annual Technical Conference (USENIX ATC 20)},
year = {2020},
isbn = {978-1-939133-14-4},
pages = {65--80},
url = {https://www.usenix.org/conference/atc20/presentation/bittman},
publisher = {USENIX Association},
month = jul,
}

@inproceedings{2019-asplos-flatflash,
author = {Abulila, Ahmed and Mailthody, Vikram Sharma and Qureshi, Zaid and Huang, Jian and Kim, Nam Sung and Xiong, Jinjun and Hwu, Wen-mei},
title = {FlatFlash: Exploiting the Byte-Accessibility of SSDs within a Unified Memory-Storage Hierarchy},
year = {2019},
isbn = {9781450362405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.vu-nl.idm.oclc.org/10.1145/3297858.3304061},
doi = {10.1145/3297858.3304061},
abstract = {Using flash-based solid state drives (SSDs) as main memory has been proposed as a practical solution towards scaling memory capacity for data-intensive applications. However, almost all existing approaches rely on the paging mechanism to move data between SSDs and host DRAM. This inevitably incurs significant performance overhead and extra I/O traffic. Thanks to the byte-addressability supported by the PCIe interconnect and the internal memory in SSD controllers, it is feasible to access SSDs in both byte and block granularity today. Exploiting the benefits of SSD's byte-accessibility in today's memory-storage hierarchy is, however, challenging as it lacks systems support and abstractions for programs. In this paper, we present FlatFlash, an optimized unified memory-storage hierarchy, to efficiently use byte-addressable SSD as part of the main memory. We extend the virtual memory management to provide a unified memory interface so that programs can access data across SSD and DRAM in byte granularity seamlessly. We propose a lightweight, adaptive page promotion mechanism between SSD and DRAM to gain benefits from both the byte-addressable large SSD and fast DRAM concurrently and transparently, while avoiding unnecessary page movements. Furthermore, we propose an abstraction of byte-granular data persistence to exploit the persistence nature of SSDs, upon which we rethink the design primitives of crash consistency of several representative software systems that require data persistence, such as file systems and databases. Our evaluation with a variety of applications demonstrates that, compared to the current unified memory-storage systems, FlatFlash improves the performance for memory-intensive applications by up to 2.3x, reduces the tail latency for latency-critical applications by up to 2.8x, scales the throughput for transactional database by up to 3.0x, and decreases the meta-data persistence overhead for file systems by up to 18.9x. FlatFlash also improves the cost-effectiveness by up to 3.8x compared to DRAM-only systems, while enhancing the SSD lifetime significantly.},
booktitle = {Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {971–985},
numpages = {15},
keywords = {unified memory management, byte-addressable ssd, page promotion, data persistence},
location = {Providence, RI, USA},
series = {ASPLOS '19}
}

@inproceedings{2009-sc-prefetch-ml,
author = {Liao, Shih-wei and Hung, Tzu-Han and Nguyen, Donald and Chou, Chinyen and Tu, Chiaheng and Zhou, Hucheng},
title = {Machine Learning-Based Prefetch Optimization for Data Center Applications},
year = {2009},
isbn = {9781605587448},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1654059.1654116},
doi = {10.1145/1654059.1654116},
abstract = {Performance tuning for data centers is essential and complicated. It is important since a data center comprises thousands of machines and thus a single-digit performance improvement can significantly reduce cost and power consumption. Unfortunately, it is extremely difficult as data centers are dynamic environments where applications are frequently released and servers are continually upgraded.In this paper, we study the effectiveness of different processor prefetch configurations, which can greatly influence the performance of memory system and the overall data center. We observe a wide performance gap when comparing the worst and best configurations, from 1.4% to 75.1%, for 11 important data center applications. We then develop a tuning framework which attempts to predict the optimal configuration based on hardware performance counters. The framework achieves performance within 1% of the best performance of any single configuration for the same set of applications.},
booktitle = {Proceedings of the Conference on High Performance Computing Networking, Storage and Analysis},
articleno = {56},
numpages = {10},
location = {Portland, Oregon},
series = {SC '09}
}

@inproceedings {2020-osdi-linnos,
author = {Mingzhe Hao and Levent Toksoz and Nanqinqin Li and Edward Edberg Halim and Henry Hoffmann and Haryadi S. Gunawi},
title = {{LinnOS}: Predictability on Unpredictable Flash Storage with a Light Neural Network},
booktitle = {14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20)},
year = {2020},
isbn = {978-1-939133-19-9},
pages = {173--190},
url = {https://www.usenix.org/conference/osdi20/presentation/hao},
publisher = {USENIX Association},
month = nov,
}

@ARTICLE{2020-ieee-deep-prefetch,  
author={Ganfure, Gaddisa Olani and Wu, Chun-Feng and Chang, Yuan-Hao and Shih, Wei-Kuan},  
journal={IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},   
title={DeepPrefetcher: A Deep Learning Framework for Data Prefetching in Flash Storage Devices},   
year={2020},  volume={39},  number={11},  pages={3311-3322},  doi={10.1109/TCAD.2020.3012173}}

@INPROCEEDINGS{2016-ieee-lynx-prefetcher,  
author={Laga, Arezki and Boukhobza, Jalil and Koskas, Michel and Singhoff, Frank},  
booktitle={2016 5th Non-Volatile Memory Systems and Applications Symposium (NVMSA)},   
title={Lynx: a learning linux prefetching mechanism for SSD performance model},   
year={2016},  
volume={},  
number={},  
pages={1-6},  
doi={10.1109/NVMSA.2016.7547186}
}

@article{2008-sigops-readahead-prefetcher,
author = {Fengguang, WU and Hongsheng, XI and Chenfeng, XU},
title = {On the Design of a New Linux Readahead Framework},
year = {2008},
issue_date = {July 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {5},
issn = {0163-5980},
url = {https://doi.org/10.1145/1400097.1400106},
doi = {10.1145/1400097.1400106},
abstract = {As Linux runs an increasing variety of workloads, its in-kernel readahead algorithm has been challenged by many unexpected and subtle problems. To name a few: readahead thrashings arise when readahead pages are evicted prematurely under memory pressure; readahead attempts on already cached pages are undesirable; interrupted-then-retried reads and locally disordered NFS reads that can easily fool the sequential detection logic. In this paper, we present a new Linux readahead framework with flexible and robust heuristics that can cover varied sequential I/O patterns. It also enjoys great simplicity by handling most abnormal cases in an implicit way. We demonstrate its advantages by a host of case studies. Network throughput is 3 times better in the case of thrashing and 1.8 times better for large NFS files. On serving large files with lighttpd, the disk utilization is decreased by 26% while providing 17% more network throughput.},
journal = {SIGOPS Oper. Syst. Rev.},
month = {jul},
pages = {75–84},
numpages = {10},
keywords = {thrashing, sequentiality, readahead, Linux, I/O performance, prefetching, caching, operating systems, access pattern}
}

@InProceedings{2021-spinger-lstm-model-io-prefetching,
author="Chakraborttii, Chandranil
and Litz, Heiner",
editor="Dong, Yuxiao
and Mladeni{\'{c}}, Dunja
and Saunders, Craig",
title="Learning I/O Access Patterns to Improve Prefetching in SSDs",
booktitle="Machine Learning and Knowledge Discovery in Databases: Applied Data Science Track",
year="2021",
publisher="Springer International Publishing",
address="Cham",
pages="427--443",
abstract="Flash based solid state drives (SSDs) have established themselves as a higher-performance alternative to hard disk drives in cloud and mobile environments. Nevertheless, SSDs remain a performance bottleneck of computer systems due to their high I/O access latency. A common approach for improving the access latency is prefetching. Prefetching predicts future block accesses and preloads them into main memory ahead of time. In this paper, we discuss the challenges of prefetching in SSDs, explain why prior approaches fail to achieve high accuracy, and present a neural network based prefetching approach that significantly outperforms the state-of the-art. To achieve high performance, we address the challenges of prefetching in very large sparse address spaces, as well as prefetching in a timely manner by predicting ahead of time. We collect I/O trace files from several real-world applications running on cloud servers and show that our proposed approach consistently outperforms the existing stride prefetchers by up to 800{\$}{\$}{\backslash}times {\$}{\$}{\texttimes}and prior prefetching approaches based on Markov chains by up to 8{\$}{\$}{\backslash}times {\$}{\$}{\texttimes}. Furthermore, we propose an address mapping learning technique to demonstrate the applicability of our approach to previously unseen SSD workloads and perform a hyperparameter sensitivity study.",
isbn="978-3-030-67667-4"
}

@inbook{2020-asplos-leapio,
author = {Li, Huaicheng and Hao, Mingzhe and Novakovic, Stanko and Gogte, Vaibhav and Govindan, Sriram and Ports, Dan R. K. and Zhang, Irene and Bianchini, Ricardo and Gunawi, Haryadi S. and Badam, Anirudh},
title = {LeapIO: Efficient and Portable Virtual NVMe Storage on ARM SoCs},
year = {2020},
isbn = {9781450371025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3373376.3378531},
booktitle = {Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {591–605},
numpages = {15}
}

@inproceedings{2017-isca-summarizer,
author = {Koo, Gunjae and Matam, Kiran Kumar and I, Te and Narra, H. V. Krishna Giri and Li, Jing and Tseng, Hung-Wei and Swanson, Steven and Annavaram, Murali},
title = {Summarizer: Trading Communication with Computing near Storage},
year = {2017},
isbn = {9781450349529},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3123939.3124553},
doi = {10.1145/3123939.3124553},
booktitle = {Proceedings of the 50th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {219–231},
numpages = {13},
keywords = {near data processing, storage systems, SSD, dynamic workload offloading},
location = {Cambridge, Massachusetts},
series = {MICRO-50 '17}
}

@inproceedings {2020-nsdi-i10,
author = {Jaehyun Hwang and Qizhe Cai and Ao Tang and Rachit Agarwal},
title = {{TCP} {==} {RDMA}: {CPU-efficient} Remote Storage Access with i10 },
booktitle = {17th USENIX Symposium on Networked Systems Design and Implementation (NSDI 20)},
year = {2020},
isbn = {978-1-939133-13-7},
address = {Santa Clara, CA},
pages = {127--140},
url = {https://www.usenix.org/conference/nsdi20/presentation/hwang},
publisher = {USENIX Association},
month = feb,
}

@inproceedings {2019-atc-m3x,
author = {Nils Asmussen and Michael Roitzsch and Hermann H{\"a}rtig},
title = {{M{\textthreesuperior}x}: Autonomous Accelerators via {Context-Enabled} {Fast-Path} Communication},
booktitle = {2019 USENIX Annual Technical Conference (USENIX ATC 19)},
year = {2019},
isbn = {978-1-939133-03-8},
address = {Renton, WA},
pages = {617--632},
url = {https://www.usenix.org/conference/atc19/presentation/asmussen},
publisher = {USENIX Association},
month = jul,
}

@inproceedings{2019-asplos-jit-verilog,
author = {Schkufza, Eric and Wei, Michael and Rossbach, Christopher J.},
title = {Just-In-Time Compilation for Verilog: A New Technique for Improving the FPGA Programming Experience},
year = {2019},
isbn = {9781450362405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297858.3304010},
doi = {10.1145/3297858.3304010},
abstract = {FPGAs offer compelling acceleration opportunities for modern applications. However compilation for FPGAs is painfully slow, potentially requiring hours or longer. We approach this problem with a solution from the software domain: the use of a JIT. Code is executed immediately in a software simulator, and compilation is performed in the background. When finished, the code is moved into hardware, and from the user's perspective it simply gets faster. We have embodied these ideas in Cascade: the first JIT compiler for Verilog. Cascade reduces the time between initiating compilation and running code to less than a second, and enables generic printf debugging from hardware. Cascade preserves program performance to within 3\texttimes{} in a debugging environment, and has minimal effect on a finalized design. Crucially, these properties hold even for programs that perform side effects on connected IO devices. A user study demonstrates the value to experts and non-experts alike: Cascade encourages more frequent compilation, and reduces the time to produce working hardware designs.},
booktitle = {Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {271–286},
numpages = {16},
keywords = {cascade, jit, compiler, verilog, just-in-time, fpga},
location = {Providence, RI, USA},
series = {ASPLOS '19}
}
@inproceedings{2018-pldi-spatial,
author = {Koeplinger, David and Feldman, Matthew and Prabhakar, Raghu and Zhang, Yaqi and Hadjis, Stefan and Fiszel, Ruben and Zhao, Tian and Nardi, Luigi and Pedram, Ardavan and Kozyrakis, Christos and Olukotun, Kunle},
title = {Spatial: A Language and Compiler for Application Accelerators},
year = {2018},
isbn = {9781450356985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3192366.3192379},
doi = {10.1145/3192366.3192379},
abstract = {Industry is increasingly turning to reconfigurable architectures like FPGAs and CGRAs for improved performance and energy efficiency. Unfortunately, adoption of these architectures has been limited by their programming models. HDLs lack abstractions for productivity and are difficult to target from higher level languages. HLS tools are more productive, but offer an ad-hoc mix of software and hardware abstractions which make performance optimizations difficult.  In this work, we describe a new domain-specific language and compiler called Spatial for higher level descriptions of application accelerators. We describe Spatial's hardware-centric abstractions for both programmer productivity and design performance, and summarize the compiler passes required to support these abstractions, including pipeline scheduling, automatic memory banking, and automated design tuning driven by active machine learning. We demonstrate the language's ability to target FPGAs and CGRAs from common source code. We show that applications written in Spatial are, on average, 42% shorter and achieve a mean speedup of 2.9x over SDAccel HLS when targeting a Xilinx UltraScale+ VU9P FPGA on an Amazon EC2 F1 instance.},
booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {296–311},
numpages = {16},
keywords = {FPGAs, domain-specific languages, CGRAs, high-level synthesis, compilers, reconfigurable architectures, hardware accelerators},
location = {Philadelphia, PA, USA},
series = {PLDI 2018}
}

@INPROCEEDINGS{2012-dac-chisel,  author={Bachrach, Jonathan and Vo, Huy and Richards, Brian and Lee, Yunsup and Waterman, Andrew and Avižienis, Rimas and Wawrzynek, John and Asanović, Krste},  booktitle={DAC Design Automation Conference 2012},   title={Chisel: Constructing hardware in a Scala embedded language},   year={2012},  volume={},  number={},  pages={1212-1221},  doi={10.1145/2228360.2228584}}

@inproceedings{1993-atc-bpf,
author = {McCanne, Steven and Jacobson, Van},
title = {The BSD Packet Filter: A New Architecture for User-Level Packet Capture},
year = {1993},
publisher = {USENIX Association},
address = {USA},
abstract = {Many versions of Unix provide facilities for user-level packet capture, making possible the use of general purpose workstations for network monitoring. Because network monitors run as user-level processes, packets must be copied across the kernel/user-space protection boundary. This copying can be minimized by deploying a kernel agent called a packet filter, which discards unwanted packets as early as possible. The original Unix packet filter was designed around a stack-based filter evaluator that performs sub-optimally on current RISC CPUs. The BSD Packet Filter (BPF) uses a new, register-based filter evaluator that is up to 20 times faster than the original design. BPF alson uses a straighforward buffering strategy that makes its overall performance up to 100 times faster than Sun's NIT running on the same hardware.},
booktitle = {Proceedings of the USENIX Winter 1993 Conference Proceedings on USENIX Winter 1993 Conference Proceedings},
pages = {2},
numpages = {1},
location = {San Diego, California},
series = {USENIX'93}
}

@misc{2021-ebpf-endace,  
  author       = {{Endace}},
  title = {Endace DAG Packet Capture Cards: Part 1}, 
  howpublished = {\nolinkurl{https://tryingtokeepitsecure.bz/index.php/8-network-engineering/14-endace-dag-packet-capture-cards}},
  note         = {Accessed: 2022-Feb-02}
}

@article{2004-xxx-combo6,
author = {Markatos, Evangelos and Y, Ji and Polychronakis, Michalis and Smotlacha, Vladimir and Ubik, Sven},
year = {2004},
month = {05},
pages = {},
title = {SCAMPI - A Scaleable Monitoring Platform for the Internet}
}

@misc{2021-ebpf-netronome,  
  author       = {{Jakub Kicinski, Nicolaas Viljoen}},
  title = {Netronome Systems, eBPF Hardware Offload to SmartNICs: cls bpf and XDP}, 
  howpublished = {\nolinkurl{https://www.netronome.com/media/documents/eBPF_HW_OFFLOAD_HNiMne8_2_.pdf}},
  note         = {Accessed: 2022-Feb-02}
}


@misc{2021-ebpf-io,  
  author       = {{Cilium}},
  howpublished = {\nolinkurl{https://ebpf.io/}},
  note         = {Accessed: 2022-Feb-02}
}

@misc{xdp,
 title = {{XDP: eXpress Data Path}},
 howpublished = {\nolinkurl{https://www.iovisor.org/technology/xdp}},
 key = {xdp},
}


@article{2017-vldb-caribou,
author = {Istv\'{a}n, Zsolt and Sidler, David and Alonso, Gustavo},
title = {Caribou: Intelligent Distributed Storage},
year = {2017},
issue_date = {August 2017},
publisher = {VLDB Endowment},
volume = {10},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3137628.3137632},
doi = {10.14778/3137628.3137632},
abstract = {The ever increasing amount of data being handled in data centers causes an intrinsic inefficiency: moving data around is expensive in terms of bandwidth, latency, and power consumption, especially given the low computational complexity of many database operations.In this paper we explore near-data processing in database engines, i.e., the option of offloading part of the computation directly to the storage nodes. We implement our ideas in Caribou, an intelligent distributed storage layer incorporating many of the lessons learned while building systems with specialized hardware. Caribou provides access to DRAM/NVRAM storage over the network through a simple key-value store interface, with each storage node providing high-bandwidth near-data processing at line rate and fault tolerance through replication. The result is a highly efficient, distributed, intelligent data storage that can be used to both boost performance and reduce power consumption and real estate usage in the data center thanks to the micro-server architecture adopted.},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1202–1213},
numpages = {12}
}

@inbook{2018-asplos-darwin,
author = {Turakhia, Yatish and Bejerano, Gill and Dally, William J.},
title = {Darwin: A Genomics Co-Processor Provides up to 15,000X Acceleration on Long Read Assembly},
year = {2018},
isbn = {9781450349116},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173162.3173193},
abstract = {Genomics is transforming medicine and our understanding of life in fundamental ways. Genomics data, however, is far outpacing Moore»s Law. Third-generation sequencing technologies produce 100X longer reads than second generation technologies and reveal a much broader mutation spectrum of disease and evolution. However, these technologies incur prohibitively high computational costs. Over 1,300 CPU hours are required for reference-guided assembly of the human genome, and over 15,600 CPU hours are required for de novo assembly. This paper describes "Darwin" --- a co-processor for genomic sequence alignment that, without sacrificing sensitivity, provides up to $15,000X speedup over the state-of-the-art software for reference-guided assembly of third-generation reads. Darwin achieves this speedup through hardware/algorithm co-design, trading more easily accelerated alignment for less memory-intensive filtering, and by optimizing the memory system for filtering. Darwin combines a hardware-accelerated version of D-SOFT, a novel filtering algorithm, alignment at high speed, and with a hardware-accelerated version of GACT, a novel alignment algorithm. GACT generates near-optimal alignments of arbitrarily long genomic sequences using constant memory for the compute-intensive step. Darwin is adaptable, with tunable speed and sensitivity to match emerging sequencing technologies and to meet the requirements of genomic applications beyond read assembly.},
booktitle = {Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {199–213},
numpages = {15}
}

@inproceedings{2020-apsys-p2p-dma,
author = {Nakamura, Ryo and Kuga, Yohei and Akashi, Kunio},
title = {How Beneficial is Peer-to-Peer DMA?},
year = {2020},
isbn = {9781450380690},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3409963.3410491},
doi = {10.1145/3409963.3410491},
abstract = {This paper revisits Peer-to-Peer DMA (P2P DMA) and investigates its potential for exploitation on Ethernet NICs and NVMe SSDs. The slowing performance improvement of CPUs has led to emergence of peripheral accelerators such as Smart NICs and TPU. P2P DMA presents potential for the efficient integration of multiple peripherals by avoiding data bouncing on main memory. However, P2P DMA has been studied mainly around GPUs, and its improvement has been measured for specific applications. In this paper, we perform experiments to clarify the benefits of using P2P DMA on individual devices, i.e., an Ethernet NIC and an NVMe SSD, from an I/O throughput perspective. We developed a library, called Libpop, for manipulating memory on devices for invoking P2P DMA. Additionally, we integrated Libpop into pcie-bench, which is an FPGA-based benchmark device, netmap for Ethernet NICs, and UNVMe for NVMe SSDs. Experiments with these implementations show that (1) memory writes degrade the throughput of DMA write by 70%, (2) the degradation affects I/O throughput on the devices, and (3) P2P DMA can avoid degradation, but device queues affect throughput on the Ethernet NIC.},
booktitle = {Proceedings of the 11th ACM SIGOPS Asia-Pacific Workshop on Systems},
pages = {25–32},
numpages = {8},
keywords = {NVMe SSD, peer-to-peer DMA, PCIe, ethernet NIC},
location = {Tsukuba, Japan},
series = {APSys '20}
}

@misc{2022-cerebras,
  author = {},
  title        = {{Cerebras Systems: Achieving Industry Best AI Performance Through A Systems Approach}},
  howpublished = {\nolinkurl{https://f.hubspotusercontent30.net/hubfs/8968533/Cerebras-CS-2-Whitepaper.pdf}},
  note         = {Accessed: 2022-Feb-02}
 }


@misc{2022-telsa-dojo,
  author = {},
  title        = {{Tesla details Dojo supercomputer, reveals Dojo D1 chip and training tile module}},
  howpublished = {\nolinkurl{https://www.datacenterdynamics.com/en/news/tesla-details-dojo-supercomputer-reveals-dojo-d1-chip-and-training-tile-module/}},
  note         = {Accessed: 2022-Feb-02}
 }
  
  
@misc{2022-fail2ban,
  author = {},
  title        = {{Fail2ban}},
  howpublished = {\nolinkurl{https://www.fail2ban.org/wiki/index.php/Main_Page}},
  note         = {Accessed: 2022-Feb-02}
 }
  
 
@inbook{2020-asplos-virt-fpga,
author = {Zha, Yue and Li, Jing},
title = {Virtualizing FPGAs in the Cloud},
year = {2020},
isbn = {9781450371025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3373376.3378491},
abstract = {Field-Programmable Gate Arrays (FPGAs) have been integrated into the cloud infrastructure to enhance its computing performance by supporting on-demand acceleration. However, system support for FPGAs in the context of the cloud environment is still in its infancy with two major limitations, i.e., the inefficient runtime management due to the tight coupling between compilation and resource allocation, and the high programming complexity when exploiting scale-out acceleration. The root cause is that FPGA resources are not virtualized. In this paper, we propose a full-stack solution, namely ViTAL, to address the aforementioned limitations by virtualizing FPGA resources. Specifically, ViTAL provides a homogeneous abstraction to decouple the compilation and resource allocation. Applications are offline compiled onto the abstraction, while the resource allocation is dynamically determined at runtime. Enabled by a latency-insensitive communication interface, applications can be mapped flexibly onto either one FPGA or multiple FPGAs to maximize the resource utilization and the aggregated system throughput. Meanwhile, ViTAL creates an illusion of a single and large FPGA to users, thereby reducing the programming complexity and supporting scale-out acceleration. Moreover, ViTAL also provides virtualization support for peripheral components (e.g., on-board DRAM and Ethernet), as well as protection and isolation support to ensure a secure execution in the multi-user cloud environment. We evaluate ViTAL on a real system - an FPGA cluster composed of the latest Xilinx UltraScale+ FPGAs (XCVU37P). The results show that, compared with the existing management method, ViTAL enables fine-grained resource sharing and reduces the response time by 82% on average (improving Quality-of-Service) with a marginal virtualization overhead. Moreover, ViTAL also reduces the response time by 25% compared to AmorphOS (operating in high-throughput mode), a recently proposed FPGA virtualization method.},
booktitle = {Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {845–858},
numpages = {14}
}

@techreport{doekemeijer_2022, 
title={{TropoDB: Design, Implementation and Evaluation of an Optimised KV-Store for NVMe Zoned Namespace Devices, Msc. thesis}}, 
author={Doekemeijer, Krijn}, 
institution = {VU Amsterdam}, 
year={2022}
} 

@article{2022-arxiv-zns-study,
  author    = {Nick Tehrany and
               \ulx{Animesh Trivedi}},
  title     = {Understanding NVMe Zoned Namespace {(ZNS)} Flash {SSD} Storage Devices},
  journal   = {CoRR},
  volume    = {abs/2206.01547},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2206.01547},
  doi       = {10.48550/arXiv.2206.01547},
  eprinttype = {arXiv},
  eprint    = {2206.01547},
  timestamp = {Mon, 13 Jun 2022 01:00:00 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2206-01547.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings {2004-osdi-boxwood,
author = {John MacCormick and Nick Murphy and Marc Najork and Chandramohan A. Thekkath and Lidong Zhou},
title = {Boxwood: Abstractions as the Foundation for Storage Infrastructure},
booktitle = {6th Symposium on Operating Systems Design \& Implementation (OSDI 04)},
year = {2004},
address = {San Francisco, CA},
url = {https://www.usenix.org/conference/osdi-04/boxwood-abstractions-foundation-storage-infrastructure},
publisher = {USENIX Association},
month = dec,
}

@inproceedings {2012-nsdi-corfu,
author = {Mahesh Balakrishnan and Dahlia Malkhi and Vijayan Prabhakaran and Ted Wobbler and Michael Wei and John D. Davis},
title = { {CORFU}: A Shared Log Design for Flash Clusters},
booktitle = {9th USENIX Symposium on Networked Systems Design and Implementation (NSDI 12)},
year = {2012},
isbn = {978-931971-92-8},
address = {San Jose, CA},
pages = {1--14},
url = {https://www.usenix.org/conference/nsdi12/technical-sessions/presentation/balakrishnan},
publisher = {USENIX Association},
month = apr,
}

@techreport{2009-msr-bee3,
author = {Davis, John and Thacker, Chuck and Chang, Chen},
title = {BEE3: Revitalizing Computer Architecture Research},
year = {2009},
month = {April},
abstract = {In recent years, advances in computer architecture have slowed dramatically with most simulation results demonstrating only incremental architectural innovation. This is further exacerbated by increased processor and system complexity spurred by a seemingly unlimited number of transistors at computer architect’s disposal. Computer architects produce a myopic view of their systems through the lens of slow, highly-detailed software simulation or fast, coarse-grained software simulation, with fidelity always in question.

By leveraging silicon technology scaling in Field Programmable Gate Arrays (FPGAs), hardware can be used to accelerate simulation, emulation, or prototyping of systems. Furthermore, because the base components are reconfigurable, the same system can be used for a variety of research projects, amortizing the cost, both in dollars and in learning time. In this paper, we present the third generation of the Berkeley Emulation Engine or BEE3 system. We demonstrate a new collaboration methodology between academia and industry and compare the industrial and academic system design process. The BEE3 is a production multi-FPGA system with up to 64 GB of DRAM and several I/O subsystems that can be used to enable faster, larger and higher fidelity computer architecture or other systems research. Using a widely available hardware platform also facilitates a software community that can generate and share software modules, thereby enabling rapid system development for computer architecture research.},
institution = {Microsoft},
url = {https://www.microsoft.com/en-us/research/publication/bee3-revitalizing-computer-architecture-research/},
number = {MSR-TR-2009-45},
}

@misc{2022-nvme-cmb,
  author = {Stephen Bates},
  title        = {{Enabling the NVMe™ CMB and PMR Ecosystem}},
  howpublished = {\nolinkurl{https://nvmexpress.org/wp-content/uploads/Session-2-Enabling-the-NVMe-CMB-and-PMR-Ecosystem-Eideticom-and-Mell....pdf}},
  note         = {Accessed: 2022-Feb-02}
 }
 
@inproceedings {2015-hotstorage-xilinx-kv-40tb-flash,
author = {Michaela Blott and Ling Liu and Kimon Karras and Kees Vissers},
title = {Scaling Out to a {Single-Node} 80Gbps Memcached Server with 40Terabytes of Memory},
booktitle = {7th USENIX Workshop on Hot Topics in Storage and File Systems (HotStorage 15)},
year = {2015},
address = {Santa Clara, CA},
url = {https://www.usenix.org/conference/hotstorage15/workshop-program/presentation/blott},
publisher = {USENIX Association},
month = jul,
}

@inproceedings{2015-isca-bluedbm,
author = {Jun, Sang-Woo and Liu, Ming and Lee, Sungjin and Hicks, Jamey and Ankcorn, John and King, Myron and Xu, Shuotao and Arvind},
title = {BlueDBM: An Appliance for Big Data Analytics},
year = {2015},
isbn = {9781450334020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2749469.2750412},
doi = {10.1145/2749469.2750412},
abstract = {Complex data queries, because of their need for random accesses, have proven to be slow unless all the data can be accommodated in DRAM. There are many domains, such as genomics, geological data and daily twitter feeds where the datasets of interest are 5TB to 20 TB. For such a dataset, one would need a cluster with 100 servers, each with 128GB to 256GBs of DRAM, to accommodate all the data in DRAM. On the other hand, such datasets could be stored easily in the flash memory of a rack-sized cluster. Flash storage has much better random access performance than hard disks, which makes it desirable for analytics workloads. In this paper we present BlueDBM, a new system architecture which has flash-based storage with in-store processing capability and a low-latency high-throughput inter-controller network. We show that BlueDBM outperforms a flash-based system without these features by a factor of 10 for some important applications. While the performance of a ram-cloud system falls sharply even if only 5%~10% of the references are to the secondary storage, this sharp performance degradation is not an issue in BlueDBM. BlueDBM presents an attractive point in the cost-performance trade-off for Big Data analytics.},
booktitle = {Proceedings of the 42nd Annual International Symposium on Computer Architecture},
pages = {1–13},
numpages = {13},
location = {Portland, Oregon},
series = {ISCA '15}
}


@article{2016-vldb-bluedb-cache,
author = {Xu, Shuotao and Lee, Sungjin and Jun, Sang-Woo and Liu, Ming and Hicks, Jamey and Arvind},
title = {Bluecache: A Scalable Distributed Flash-Based Key-Value Store},
year = {2016},
issue_date = {November 2016},
publisher = {VLDB Endowment},
volume = {10},
number = {4},
issn = {2150-8097},
url = {https://doi.org/10.14778/3025111.3025113},
doi = {10.14778/3025111.3025113},
abstract = {A key-value store (KVS), such as memcached and Redis, is widely used as a caching layer to augment the slower persistent backend storage in data centers. DRAM-based KVS provides fast key-value access, but its scalability is limited by the cost, power and space needed by the machine cluster to support a large amount of DRAM. This paper offers a 10X to 100X cheaper solution based on flash storage and hardware accelerators. In BlueCache key-value pairs are stored in flash storage and all KVS operations, including the flash controller are directly implemented in hardware. Furthermore, BlueCache includes a fast interconnect between flash controllers to provide a scalable solution. We show that BlueCache has 4.18X higher throughput and consumes 25X less power than a flash-backed KVS software implementation on x86 servers. We further show that BlueCache can outperform DRAM-based KVS when the latter has more than 7.4% misses for a read-intensive aplication. BlueCache is an attractive solution for both rack-level appliances and data-center-scale key-value cache.},
journal = {Proc. VLDB Endow.},
month = {nov},
pages = {301–312},
numpages = {12}
}

@inproceedings{2013-fpga-mc,
author = {Chalamalasetti, Sai Rahul and Lim, Kevin and Wright, Mitch and AuYoung, Alvin and Ranganathan, Parthasarathy and Margala, Martin},
title = {An FPGA Memcached Appliance},
year = {2013},
isbn = {9781450318877},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2435264.2435306},
doi = {10.1145/2435264.2435306},
abstract = {Providing low-latency access to large amounts of data is one of the foremost requirements for many web services. To address these needs, systems such as Memcached have been created which provide a distributed, all in-memory key-value store. These systems are critical and often deployed across hundreds or thousands of servers. However, these systems are not well matched for commodity servers, as they require significant CPU resources to achieve reasonable network bandwidth, yet the core Memcached functions do not benefit from the high performance of standard server CPUs. In this paper, we demonstrate the design of an FPGA-based Memcached appliance. We take Memcached, a complex software system, and implement its core functionality on an FPGA. By leveraging the FPGA's design and utilizing its customizable logic to create a specialized appliance we are able to tightly integrate networking, compute, and memory. This integration allows us to overcome many of the bottlenecks found in standard servers. Our design provides performance on-par with baseline servers, but consumes only 9% of the power of the baseline. Scaled out, we see benefits at the data center level, substantially improving the performance-per-dollar while improving energy efficiency by 3.2X to 10.9X.},
booktitle = {Proceedings of the ACM/SIGDA International Symposium on Field Programmable Gate Arrays},
pages = {245–254},
numpages = {10},
keywords = {energy efficiency, data centers, FPGA, memcached appliance, low power},
location = {Monterey, California, USA},
series = {FPGA '13}
}

@inproceedings {2013-hotcloud-fpga-mc-10gbps,
author = {Michaela Blott and Kimon Karras and Ling Liu and Kees Vissers and Jeremia B{\"a}r and Zsolt Istv{\'a}n},
title = {Achieving 10Gbps Line-rate Key-value Stores with {FPGAs}},
booktitle = {5th USENIX Workshop on Hot Topics in Cloud Computing (HotCloud 13)},
year = {2013},
address = {San Jose, CA},
url = {https://www.usenix.org/conference/hotcloud13/workshop-program/presentations/blott},
publisher = {USENIX Association},
month = jun,
}

@inproceedings {2016-nsdi-consensus-box,
author = {Zsolt Istv{\'a}n and David Sidler and Gustavo Alonso and Marko Vukolic},
title = {Consensus in a Box: Inexpensive Coordination in Hardware},
booktitle = {13th USENIX Symposium on Networked Systems Design and Implementation (NSDI 16)},
year = {2016},
isbn = {978-1-931971-29-4},
address = {Santa Clara, CA},
pages = {425--438},
url = {https://www.usenix.org/conference/nsdi16/technical-sessions/presentation/istvan},
publisher = {USENIX Association},
month = mar,
}

@inproceedings{2016-eurosys-flash-disaggregation,
author = {Klimovic, Ana and Kozyrakis, Christos and Thereska, Eno and John, Binu and Kumar, Sanjeev},
title = {Flash Storage Disaggregation},
year = {2016},
isbn = {9781450342407},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2901318.2901337},
doi = {10.1145/2901318.2901337},
abstract = {PCIe-based Flash is commonly deployed to provide datacenter applications with high IO rates. However, its capacity and bandwidth are often underutilized as it is difficult to design servers with the right balance of CPU, memory and Flash resources over time and for multiple applications. This work examines Flash disaggregation as a way to deal with Flash overprovisioning. We tune remote access to Flash over commodity networks and analyze its impact on workloads sampled from real datacenter applications. We show that, while remote Flash access introduces a 20% throughput drop at the application level, disaggregation allows us to make up for these overheads through resource-efficient scale-out. Hence, we show that Flash disaggregation allows scaling CPU and Flash resources independently in a cost effective manner. We use our analysis to draw conclusions about data and control plane issues in remote storage.},
booktitle = {Proceedings of the Eleventh European Conference on Computer Systems},
articleno = {29},
numpages = {15},
keywords = {network storage, flash, datacenter},
location = {London, United Kingdom},
series = {EuroSys '16}
}

@inproceedings{2021-asplos-nic-offloading,
author = {Pismenny, Boris and Eran, Haggai and Yehezkel, Aviad and Liss, Liran and Morrison, Adam and Tsafrir, Dan},
title = {Autonomous NIC Offloads},
year = {2021},
isbn = {9781450383172},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3445814.3446732},
doi = {10.1145/3445814.3446732},
booktitle = {Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {18–35},
numpages = {18},
keywords = {NIC, operating system, hardware/software co-design},
location = {Virtual, USA},
series = {ASPLOS 2021}
}

@inproceedings{2021-sosp-linefs,
author = {Kim, Jongyul and Jang, Insu and Reda, Waleed and Im, Jaeseong and Canini, Marco and Kosti\'{c}, Dejan and Kwon, Youngjin and Peter, Simon and Witchel, Emmett},
title = {LineFS: Efficient SmartNIC Offload of a Distributed File System with Pipeline Parallelism},
year = {2021},
isbn = {9781450387095},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477132.3483565},
doi = {10.1145/3477132.3483565},
abstract = {In multi-tenant systems, the CPU overhead of distributed file systems (DFSes) is increasingly a burden to application performance. CPU and memory interference cause degraded and unstable application and storage performance, in particular for operation latency. Recent client-local DFSes for persistent memory (PM) accelerate this trend. DFS offload to SmartNICs is a promising solution to these problems, but it is challenging to fit the complex demands of a DFS onto simple SmartNIC processors located across PCIe.We present LineFS, a SmartNIC-offloaded, high-performance DFS with support for client-local PM. To fully leverage the SmartNIC architecture, we decompose DFS operations into execution stages that can be offloaded to a parallel datapath execution pipeline on the SmartNIC. LineFS offloads CPU-intensive DFS tasks, like replication, compression, data publication, index and consistency management to a Smart-NIC. We implement LineFS on the Mellanox BlueField Smart-NIC and compare it to Assise, a state-of-the-art PM DFS. LineFS improves latency in LevelDB up to 80% and throughput in Filebench up to 79%, while providing extended DFS availability during host system failures.},
booktitle = {Proceedings of the ACM SIGOPS 28th Symposium on Operating Systems Principles},
pages = {756–771},
numpages = {16},
keywords = {Distributed file system, SmartNIC offload},
location = {Virtual Event, Germany},
series = {SOSP '21}
}

@inproceedings{2022-asplos-enzian,
author = {Cock, David and Ramdas, Abishek and Schwyn, Daniel and Giardino, Michael and Turowski, Adam and He, Zhenhao and Hossle, Nora and Korolija, Dario and Licciardello, Melissa and Martsenko, Kristina and Achermann, Reto and Alonso, Gustavo and Roscoe, Timothy},
title = {Enzian: An Open, General, CPU/FPGA Platform for Systems Software Research},
year = {2022},
isbn = {9781450392051},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503222.3507742},
doi = {10.1145/3503222.3507742},
abstract = {Hybrid computing platforms, comprising CPU cores and FPGA logic, are increasingly used for accelerating data-intensive workloads in cloud deployments, and are a growing topic of interest in systems research. However, from a research perspective, existing hardware platforms are limited: they are often optimized for concrete, narrow use-cases and, therefore lack the flexibility needed to explore other applications and configurations.  We show that a research group can design and build a more general, open, and affordable hardware platform for hybrid systems research. The platform, Enzian, is capable of duplicating the functionality of existing CPU/FPGA systems with comparable performance but in an open, flexible system. It couples a large FPGA with a server-class CPU in an asymmetric cache-coherent NUMA system. Enzian also enables research not possible with existing hybrid platforms, through explicit access to coherence messages, extensive thermal and power instrumentation, and an open, programmable baseboard management processor.  Enzian is already being used in multiple projects, is open source (both hardware and software), and available for remote use. We present the design principles of Enzian, the challenges in building it, and evaluate it with a range of existing research use-cases alongside other, more specialized platforms, as well as demonstrating research not possible on existing platforms.},
booktitle = {Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {434–451},
numpages = {18},
keywords = {cache coherence, FPGAs, heterogeneous systems},
location = {Lausanne, Switzerland},
series = {ASPLOS 2022}
}

@inproceedings{2022-asplos-fpga-fast-compilation,
author = {Xiao, Yuanlong and Micallef, Eric and Butt, Andrew and Hofmann, Matthew and Alston, Marc and Goldsmith, Matthew and Merczynski-Hait, Andrew and DeHon, Andr\'{e}},
title = {PLD: Fast FPGA Compilation to Make Reconfigurable Acceleration Compatible with Modern Incremental Refinement Software Development},
year = {2022},
isbn = {9781450392051},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503222.3507740},
doi = {10.1145/3503222.3507740},
abstract = {FPGA-based accelerators are demonstrating significant absolute performance and energy efficiency compared with general-purpose CPUs. While FPGA computations can now be described in standard, programming languages, like C, development for FPGAs accelerators remains tedious and inaccessible to modern software engineers. Slow compiles (potentially taking tens of hours) inhibit the rapid, incremental refinement of designs that is the hallmark of modern software engineering. To address this issue, we introduce separate compilation and linkage into the FPGA design flow, providing faster design turns more familiar to software development. To realize this flow, we provide abstractions, compiler options, and compiler flow that allow the same C source code to be compiled to processor cores in seconds and to FPGA regions in minutes, providing the missing -O0 and -O1 options familiar in software development. This raises the FPGA programming level and standardizes the programming experience, bringing FPGA-based accelerators into a more familiar software platform ecosystem for software engineers.},
booktitle = {Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {933–945},
numpages = {13},
keywords = {FPGA, DFX, Data Center, Compilation, Partial Reconfiguration},
location = {Lausanne, Switzerland},
series = {ASPLOS 2022}
}

@inproceedings{2022-asplos-fpga-debug,
author = {Ma, Jiacheng and Zuo, Gefei and Loughlin, Kevin and Zhang, Haoyang and Quinn, Andrew and Kasikci, Baris},
title = {Debugging in the Brave New World of Reconfigurable Hardware},
year = {2022},
isbn = {9781450392051},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503222.3507701},
doi = {10.1145/3503222.3507701},
abstract = {Software and hardware development cycles have traditionally been quite distinct. Software allows post-deployment patches, which leads to a rapid development cycle. In contrast, hardware bugs that are found after fabrication are extremely costly to fix (and sometimes even unfixable), so the traditional hardware development cycle involves massive investment in extensive simulation and formal verification. Reconfigurable hardware, such as a Field Programmable Gate Array (FPGA), promises to propel hardware development towards an agile software-like development approach, since it enables a hardware developer to patch bugs that are detected during on-chip testing or in production. Unfortunately, FPGA programmers lack bug localization tools amenable to this rapid development cycle, since past tools mainly find bugs via simulation and verification. To develop hardware bug localization tools for a rapid development cycle, a thorough understanding of the symptoms, root causes, and fixes of hardware bugs is needed.  In this paper, we first study bugs in existing FPGA designs and produce a testbed of reliably-reproducible bugs. We classify the bugs according to their intrinsic properties, symptoms, and root causes. We demonstrate that many hardware bugs are comparable to software bug counterparts, and would benefit from similar techniques for bug diagnosis and repair. Based upon our findings, we build a novel collection of hybrid static/dynamic program analysis and monitoring tools for debugging FPGA designs, showing that our tools enable a software-like development cycle by effectively reducing developers' manual efforts for bug localization.},
booktitle = {Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {946–962},
numpages = {17},
keywords = {Bug Study, Debugging, FPGA, Reconfigurable Hardware},
location = {Lausanne, Switzerland},
series = {ASPLOS 2022}
}

@inproceedings{2022-asplos-fpga-enclaves,
author = {Zhao, Mark and Gao, Mingyu and Kozyrakis, Christos},
title = {ShEF: Shielded Enclaves for Cloud FPGAs},
year = {2022},
isbn = {9781450392051},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503222.3507733},
doi = {10.1145/3503222.3507733},
abstract = {FPGAs are now used in public clouds to accelerate a wide range of applications, including many that operate on sensitive data such as financial and medical records. We present ShEF, a trusted execution environment (TEE) for cloud-based reconfigurable accelerators. ShEF is independent from CPU-based TEEs and allows secure execution under a threat model where the adversary can control all software running on the CPU connected to the FPGA, has physical access to the FPGA, and can compromise the FPGA interface logic of the cloud provider. ShEF provides a secure boot and remote attestation process that relies solely on existing FPGA mechanisms for root of trust. It also includes a Shield component that provides secure access to data while the accelerator is in use. The Shield is highly customizable and extensible, allowing users to craft a bespoke security solution that fits their accelerator's memory access patterns, bandwidth, and security requirements at minimum performance and area overheads. We describe a prototype implementation of ShEF for existing cloud FPGAs, map ShEF to a performant and secure storage application, and measure the performance benefits of customizable security using five additional accelerators.},
booktitle = {Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {1070–1085},
numpages = {16},
keywords = {enclaves, FPGAs, cloud computing, reconfigurable computing, trusted execution},
location = {Lausanne, Switzerland},
series = {ASPLOS 2022}
}

@inproceedings{2011-isca-dark-silicon,
author = {Esmaeilzadeh, Hadi and Blem, Emily and St. Amant, Renee and Sankaralingam, Karthikeyan and Burger, Doug},
title = {Dark Silicon and the End of Multicore Scaling},
year = {2011},
isbn = {9781450304726},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2000064.2000108},
doi = {10.1145/2000064.2000108},
abstract = {Since 2005, processor designers have increased core counts to exploit Moore's Law scaling, rather than focusing on single-core performance. The failure of Dennard scaling, to which the shift to multicore parts is partially a response, may soon limit multicore scaling just as single-core scaling has been curtailed. This paper models multicore scaling limits by combining device scaling, single-core scaling, and multicore scaling to measure the speedup potential for a set of parallel workloads for the next five technology generations. For device scaling, we use both the ITRS projections and a set of more conservative device scaling parameters. To model single-core scaling, we combine measurements from over 150 processors to derive Pareto-optimal frontiers for area/performance and power/performance. Finally, to model multicore scaling, we build a detailed performance model of upper-bound performance and lower-bound core power. The multicore designs we study include single-threaded CPU-like and massively threaded GPU-like multicore chip organizations with symmetric, asymmetric, dynamic, and composed topologies. The study shows that regardless of chip organization and topology, multicore scaling is power limited to a degree not widely appreciated by the computing community. Even at 22 nm (just one year from now), 21% of a fixed-size chip must be powered off, and at 8 nm, this number grows to more than 50%. Through 2024, only 7.9x average speedup is possible across commonly used parallel workloads, leaving a nearly 24-fold gap from a target of doubled performance per generation.},
booktitle = {Proceedings of the 38th Annual International Symposium on Computer Architecture},
pages = {365–376},
numpages = {12},
keywords = {modeling, power, dark silicon, multicore, technology scaling},
location = {San Jose, California, USA},
series = {ISCA '11}
}

@ARTICLE{2011-micro-dark-silicon,  
author={Hardavellas, Nikos and Ferdman, Michael and Falsafi, Babak and Ailamaki, Anastasia},  
journal={IEEE Micro},   
title={Toward Dark Silicon in Servers},   
year={2011},  
volume={31},  
number={4},  
pages={6-15},  
doi={10.1109/MM.2011.77}
}

@article{2019-acm-in-depth-cpu-fgpa,
author = {Choi, Young-Kyu and Cong, Jason and Fang, Zhenman and Hao, Yuchen and Reinman, Glenn and Wei, Peng},
title = {In-Depth Analysis on Microarchitectures of Modern Heterogeneous CPU-FPGA Platforms},
year = {2019},
issue_date = {March 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {1},
issn = {1936-7406},
url = {https://doi.org/10.1145/3294054},
doi = {10.1145/3294054},
abstract = {Conventional homogeneous multicore processors are not able to provide the continued performance and energy improvement that we have expected from past endeavors. Heterogeneous architectures that feature specialized hardware accelerators are widely considered a promising paradigm for resolving this issue. Among different heterogeneous devices, FPGAs that can be reconfigured to accelerate a broad class of applications with orders-of-magnitude performance/watt gains, are attracting increased attention from both academia and industry. As a consequence, a variety of CPU-FPGA acceleration platforms with diversified microarchitectural features have been supplied by industry vendors. Such diversity, however, poses a serious challenge to application developers in selecting the appropriate platform for a specific application or application domain.This article aims to address this challenge by determining which microarchitectural characteristics affect performance, and in what ways. Specifically, we conduct a quantitative comparison and an in-depth analysis on five state-of-the-art CPU-FPGA acceleration platforms: (1) the Alpha Data board and (2) the Amazon F1 instance that represent the traditional PCIe-based platform with private device memory; (3) the IBM CAPI that represents the PCIe-based system with coherent shared memory; (4) the first generation of the Intel Xeon+FPGA Accelerator Platform that represents the QPI-based system with coherent shared memory; and (5) the second generation of the Intel Xeon+FPGA Accelerator Platform that represents a hybrid PCIe-based (non-coherent) and QPI-based (coherent) system with shared memory. Based on the analysis of their CPU-FPGA communication latency and bandwidth characteristics, we provide a series of insights for both application developers and platform designers. Furthermore, we conduct two case studies to demonstrate how these insights can be leveraged to optimize accelerator designs. The microbenchmarks used for evaluation have been released for public use.},
journal = {ACM Trans. Reconfigurable Technol. Syst.},
month = {feb},
articleno = {4},
numpages = {20},
keywords = {Heterogeneous computing, AWS F1, Xeon+FPGA, CAPI, CPU-FPGA platform}
}


@misc{2022-dpu-fungible,
  author = {Fungible},
  title        = {{Fungible F1 Data Processing Unit}},
  howpublished = {\nolinkurl{https://www.fungible.com/wp-content/uploads/2021/09/PB0028.02.12020914-Fungible-F1-Data-Processing-Unit.pdf}},
  note         = {Accessed: 2022-Feb-02}
 }

@misc{2022-dpu-pensando,
  author = {Pensando},
  title        = {{The Pensando Distributed Services Card (DSC)}},
  howpublished = {\nolinkurl{https://pensando.io/products/dsc/}},
  note         = {Accessed: 2022-Feb-02}
 }

@misc{2022-knuth-interview,
  author = {Andrew Binstock and Donald Knuth},
  title        = {{Interview with Donald Knuth}},
  howpublished = {\nolinkurl{https://www.informit.com/articles/article.aspx?p=1193856}},
  year = {2008}, 
  note         = {Accessed: 2022-Feb-02}
 } 
 
 
@misc{2022-openroad,
  author = {},
  title        = {{The OpenRoad Project, Democratizing Hardware Design}},
  howpublished = {\nolinkurl{https://theopenroadproject.org/}},
  year = {2022}, 
  note         = {Accessed: 2022-Feb-02}
 }
 
 
@misc{2022-chips,
  author = {},
  title        = {{CHIPS (Common Hardware for Interfaces, Processors and Systems) Alliance}},
  howpublished = {\nolinkurl{https://chipsalliance.org/}},
  year = {2022}, 
  note         = {Accessed: 2022-Feb-02}
 }  


@misc{2022-true-fabric,
  author = {Fungible},
  title        = {{TrueFabric: A Fundamental Advance to the State of the Art in Data Center Networks}},
  howpublished = {\nolinkurl{https://www.fungible.com/wp-content/uploads/2020/08/WP0033.00.02020818-TrueFabric-A-Fundamental-Advance-to-the-State-of-the-Art-in-Data-Center-Networks.pdf}},
  year = {2022}, 
  note         = {Accessed: 2022-Feb-02}
 }   
 
 @article{1970-cacm-virtual-memory,
author = {Denning, Peter J.},
title = {Virtual Memory},
year = {1970},
issue_date = {Sept. 1970},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/356571.356573},
doi = {10.1145/356571.356573},
journal = {ACM Comput. Surv.},
month = {sep},
pages = {153–189},
numpages = {37}
}

@inbook{2020-asplos-optimus,
author = {Ma, Jiacheng and Zuo, Gefei and Loughlin, Kevin and Cheng, Xiaohe and Liu, Yanqiang and Eneyew, Abel Mulugeta and Qi, Zhengwei and Kasikci, Baris},
title = {A Hypervisor for Shared-Memory FPGA Platforms},
year = {2020},
isbn = {9781450371025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3373376.3378482},
abstract = {Cloud providers widely deploy FPGAs as application-specific accelerators for customer use. These providers seek to multiplex their FPGAs among customers via virtualization, thereby reducing running costs. Unfortunately, most virtualization support is confined to FPGAs that expose a restrictive, host-centric programming model in which accelerators cannot issue direct memory accesses (DMAs). The host-centric model incurs high runtime overhead for workloads that exhibit pointer chasing. Thus, FPGAs are beginning to support a shared-memory programming model in which accelerators can issue DMAs. However, virtualization support for shared-memory FPGAs is limited. This paper presents Optimus, the first hypervisor that supports scalable shared-memory FPGA virtualization. Optimus offers both spatial multiplexing and temporal multiplexing to provide efficient and flexible sharing of each accelerator on an FPGA. To share the FPGA-CPU interconnect at a high clock frequency, Optimus implements a multiplexer tree. To isolate each guest's address space, Optimus introduces the technique of page table slicing as a hardware-software co-design. To support preemptive temporal multiplexing, Optimus provides an accelerator preemption interface. We show that Optimus supports eight physical accelerators on a single FPGA and improves the aggregate throughput of twelve real-world benchmarks by 1.98x-7x.},
booktitle = {Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {827–844},
numpages = {18}
}

@book{2014-os-vu-book,
author = {Tanenbaum, Andrew S. and Bos, Herbert},
title = {Modern Operating Systems},
year = {2014},
isbn = {013359162X},
publisher = {Prentice Hall Press},
address = {USA},
edition = {4th},
abstract = {Modern Operating Systems, Fourth Edition, is intended for introductory courses in Operating Systems in Computer Science, Computer Engineering, and Electrical Engineering programs. It also serves as a useful reference for OS professionals The widely anticipated revision of this worldwide best-seller incorporates the latest developments in operating systems (OS) technologies. The Fourth Edition includes up-to-date materials on relevantOS. Tanenbaum also provides information on current research based on his experience as an operating systems researcher. Modern Operating Systems, Third Editionwas the recipient of the 2010 McGuffey Longevity Award. The McGuffey Longevity Award recognizes textbooks whose excellence has been demonstrated over time.http://taaonline.net/index.html Teaching and Learning Experience This program will provide a better teaching and learning experiencefor you and your students. It will help: Provide Practical Detail on the Big Picture Concepts: A clear and entertaining writing style outlines the concepts every OS designer needs to master. Keep Your Course Current: This edition includes information on the latest OS technologies and developments Enhance Learning with Student and Instructor Resources: Students will gain hands-on experience using the simulation exercises and lab experiments.}
}

@book{2022-ostep,
author = {Arpaci-Dusseau, Remzi H. and Arpaci-Dusseau, Andrea C.},
title = {Operating Systems: Three Easy Pieces},
year = {2018},
isbn = {198508659X},
publisher = {CreateSpace Independent Publishing Platform},
address = {North Charleston, SC, USA},
abstract = {OSTEP ("oh step"), or the "the comet book", represents the culmination of years of teaching intro to operating systems to both undergraduates and graduates at the University of Wisconsin-Madison Computer Sciences department for nearly 20 years. The book is organized around three concepts fundamental to OS construction: virtualization (of CPU and memory), concurrency (locks and condition variables), and persistence (disks, RAIDS, and file systems). The material, if combined with serious project work and homeworks, will lead students to a deeper understanding and appreciation of modern OSes. The authors, Remzi and Andrea Arpaci-Dusseau, are both professors of Computer Sciences at the University of Wisconsin-Madison. They have been doing research in computer systems for over 20 years, working together since their first graduate operating systems class at U.C. Berkeley in 1993. Since that time, they have published over 100 papers on the performance and reliability of many aspects of modern computer systems, with a special focus on file and storage systems. Their work has been recognized with numerous best-paper awards, and some of their innovations can be found in the Linux and BSD operating systems today.}
}


@phdthesis{2018-phd-Istvan,
    title    = {Building Distributed Storage with Specialized Hardware},
    school   = {ETH Zurich},
    author   = {Istvan, Zsolt},
    year     = {2018},
    type     = {{PhD} dissertation},
}

@phdthesis{2020-phd-achermann,
    title    = {On Memory Addressing},
    school   = {ETH Zurich},
    author   = {Reto Achermann},
    year     = {2020},
    type     = {{PhD} dissertation},
}

@phdthesis{2016-phd-trivedi,
    title    = {End-to-End Considerations in the Unification of High-Performance I/O},
    school   = {ETH Zurich},
    author   = {\ulx{Animesh Trivedi}},
    year     = {2016},
    type     = {{PhD} dissertation},
}

@ARTICLE{1960-atlas-single-level,  author={Kilburn, T. and Edwards, D. B. G. and Lanigan, M. J. and Sumner, F. H.},  journal={IRE Transactions on Electronic Computers},   title={One-Level Storage System},   year={1962},  volume={EC-11},  number={2},  pages={223-235},  doi={10.1109/TEC.1962.5219356}}

@ARTICLE {2007-energy-proportional-computing,
author = {U. Hölzle and L. Barroso},
journal = {Computer},
title = {The Case for Energy-Proportional Computing},
year = {2007},
volume = {40},
number = {12},
issn = {1558-0814},
pages = {33-37},
keywords = {green computing;energy-proportional computing},
doi = {10.1109/MC.2007.443},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {dec}
}

@article{2018-xxx-dvfs,
title = {Power-performance tradeoffs in data center servers: DVFS, CPU pinning, horizontal, and vertical scaling},
journal = {Future Generation Computer Systems},
volume = {81},
pages = {114-128},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2017.10.044},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17304910},
author = {Jakub Krzywda and Ahmed Ali-Eldin and Trevor E. Carlson and Per-Olov Östberg and Erik Elmroth},
keywords = {Power-performance tradeoffs, Dynamic Voltage and Frequency Scaling (DVFS), CPU pinning, Horizontal scaling, Vertical scaling},
abstract = {Dynamic Voltage and Frequency Scaling (DVFS), CPU pinning, horizontal, and vertical scaling, are four techniques that have been proposed as actuators to control the performance and energy consumption on data center servers. This work investigates the utility of these four actuators, and quantifies the power-performance tradeoffs associated with them. Using replicas of the German Wikipedia running on our local testbed, we perform a set of experiments to quantify the influence of DVFS, vertical and horizontal scaling, and CPU pinning on end-to-end response time (average and tail), throughput, and power consumption with different workloads. Results of the experiments show that DVFS rarely reduces the power consumption of underloaded servers by more than 5%, but it can be used to limit the maximal power consumption of a saturated server by up to 20% (at a cost of performance degradation). CPU pinning reduces the power consumption of underloaded server (by up to 7%) at the cost of performance degradation, which can be limited by choosing an appropriate CPU pinning scheme. Horizontal and vertical scaling improves both the average and tail response time, but the improvement is not proportional to the amount of resources added. The load balancing strategy has a big impact on the tail response time of horizontally scaled applications.}
}

@inproceedings{2011-isca-online-energy,
author = {Meisner, David and Sadler, Christopher M. and Barroso, Luiz Andr\'{e} and Weber, Wolf-Dietrich and Wenisch, Thomas F.},
title = {Power Management of Online Data-Intensive Services},
year = {2011},
isbn = {9781450304726},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2000064.2000103},
doi = {10.1145/2000064.2000103},
abstract = {Much of the success of the Internet services model can be attributed to the popularity of a class of workloads that we call Online Data-Intensive (OLDI) services. These workloads perform significant computing over massive data sets per user request but, unlike their offline counterparts (such as MapReduce computations), they require responsiveness in the sub-second time scale at high request rates. Large search products, online advertising, and machine translation are examples of workloads in this class. Although the load in OLDI services can vary widely during the day, their energy consumption sees little variance due to the lack of energy proportionality of the underlying machinery. The scale and latency sensitivity of OLDI workloads also make them a challenging target for power management techniques.We investigate what, if anything, can be done to make OLDI systems more energy-proportional. Specifically, we evaluate the applicability of active and idle low-power modes to reduce the power consumed by the primary server components (processor, memory, and disk), while maintaining tight response time constraints, particularly on 95th-percentile latency. Using Web search as a representative example of this workload class, we first characterize a production Web search workload at cluster-wide scale. We provide a fine-grain characterization and expose the opportunity for power savings using low-power modes of each primary server component. Second, we develop and validate a performance model to evaluate the impact of processor- and memory-based low-power modes on the search latency distribution and consider the benefit of current and foreseeable low-power modes. Our results highlight the challenges of power management for this class of workloads. In contrast to other server workloads, for which idle low-power modes have shown great promise, for OLDI workloads we find that energy-proportionality with acceptable query latency can only be achieved using coordinated, full-system active low-power modes.},
booktitle = {Proceedings of the 38th Annual International Symposium on Computer Architecture},
pages = {319–330},
numpages = {12},
keywords = {servers, power management},
location = {San Jose, California, USA},
series = {ISCA '11}
}

@inproceedings{2022-hotstorage-cxl,
author = {Jung, Myoungsoo},
title = {Hello Bytes, Bye Blocks: PCIe Storage Meets Compute Express Link for Memory Expansion (CXL-SSD)},
year = {2022},
isbn = {9781450393997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.vu-nl.idm.oclc.org/10.1145/3538643.3539745},
doi = {10.1145/3538643.3539745},
booktitle = {Proceedings of the 14th ACM Workshop on Hot Topics in Storage and File Systems},
pages = {45–51},
numpages = {7},
location = {Virtual Event},
series = {HotStorage '22}
}

@inproceedings{2012-isca-ovc-tlb-energy,
author = {Basu, Arkaprava and Hill, Mark D. and Swift, Michael M.},
title = {Reducing Memory Reference Energy with Opportunistic Virtual Caching},
year = {2012},
isbn = {9781450316422},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Most modern cores perform a highly-associative transaction look aside buffer (TLB) lookup on every memory access. These designs often hide the TLB lookup latency by overlapping it with L1 cache access, but this overlap does not hide the power dissipated by TLB lookups. It can even exacerbate the power dissipation by requiring higher associativity L1 cache. With today's concern for power dissipation, designs could instead adopt a virtual L1 cache, wherein TLB access power is dissipated only after L1 cache misses. Unfortunately, virtual caches have compatibility issues, such as supporting writeable synonyms and x86's physical page table walker.This work proposes an Opportunistic Virtual Cache (OVC) that exposes virtual caching as a dynamic optimization by allowing some memory blocks to be cached with virtual addresses and others with physical addresses. OVC relies on small OS changes to signal which pages can use virtual caching (e.g., no writeable synonyms), but defaults to physical caching for compatibility. We show OVC's promise with analysis that finds virtual cache problems exist, but are dynamically rare. We change 240 lines in Linux 2.6.28 to enable OVC. On experiments with Parsec and commercial workloads, the resulting system saves 94-99% of TLB lookup energy and nearly 23% of L1 cache dynamic lookup energy.},
booktitle = {Proceedings of the 39th Annual International Symposium on Computer Architecture},
pages = {297–308},
numpages = {12},
location = {Portland, Oregon},
series = {ISCA '12}
}

@inproceedings{2002-mico-itlb-energy,
author = {Kadayif, I. and Sivasubramaniam, A. and Kandemir, M. and Kandiraju, G. and Chen, G.},
title = {Generating Physical Addresses Directly for Saving Instruction TLB Energy},
year = {2002},
isbn = {0769518591},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
abstract = {Power consumption and power density for the Translation Lookaside Buffer (TLB) are important considerations not only in its design, but can have a consequence on cache design as well. This paper embarks on a new philosophy for reducing the number of accesses to the instruction TLB (iTLB) for power and performance optimizations. The overall idea is to keep a translation currently being used in a register and avoid going to the iTLB as far as possible --- until there is a page change. We propose four different approaches for achieving this, and experimentally demonstrate that one of these schemes that uses a combination of compiler and hardware enhancements can reduce iTLB dynamic power by over 85% in most cases.These mechanisms can work with different instructioncache (iL1) lookup mechanisms and achieve significant iTLB power savings without compromising on performance. Their importance grows with higher iL1 miss rates and larger page sizes. They can work very well with large iTLB structures, that can possibly consume more power and take longer to lookup, without the iTLB getting into the common case. Further, we also experimentally demonstrate that they can provide performance savings for virtually-indexed, virtually-tagged iL1 caches, and can even make physically indexed, physically-tagged iL1 caches a possible choice for implementation.},
booktitle = {Proceedings of the 35th Annual ACM/IEEE International Symposium on Microarchitecture},
pages = {185–196},
numpages = {12},
location = {Istanbul, Turkey},
series = {MICRO 35}
}

@inproceedings{2005-codes-energy-tlb,
author = {Zhou, Xiangrong and Petrov, Peter},
title = {Energy-Efficient Address Translation for Virtual Memory Support in Low-Power and Real-Time Embedded Processors},
year = {2005},
isbn = {1595931619},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.vu-nl.idm.oclc.org/10.1145/1084834.1084848},
doi = {10.1145/1084834.1084848},
abstract = {In this paper we present an application-driven address translation scheme for low-power and real-time embedded processors with virtual memory support. The power inefficiency and nondeterministic execution times of address-translation mechanisms have been major barriers in adopting and utilizing the benefits of virtual memory in embedded processors with low-power and real-time constraints. To address this problem, we propose a novel, Customizable Translation Table (CTT) organization, where application knowledge regarding the virtual memory footprint is used in order to eliminate conflicts in the hardware translation buffer and, thus, achieve tag-free address translation lookups. The set of virtual pages is partitioned into groups, such that for each group only a few of the least significant bits are used as an index to obtain the physical page number. We outline an efficient compile-time algorithm for identifying these groups and allocate their translation entries optimally into the CTT. The proposed methodology relies on the combined efforts of compiler, operating system, and hardware architecture to achieve a significant power reduction. The experiments that we have performed on a set of embedded applications show power reductions in the range of 55% to 80% compared to a general- purpose Translation Lookaside Buffer (TLB).},
booktitle = {Proceedings of the 3rd IEEE/ACM/IFIP International Conference on Hardware/Software Codesign and System Synthesis},
pages = {33–38},
numpages = {6},
keywords = {adaptable systems, multi-mode synthesis, reconfigurability},
location = {Jersey City, NJ, USA},
series = {CODES+ISSS '05}
}
@inproceedings{2017-asplos-energy-tlb,
author = {Cox, Guilherme and Bhattacharjee, Abhishek},
title = {Efficient Address Translation for Architectures with Multiple Page Sizes},
year = {2017},
isbn = {9781450344654},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.vu-nl.idm.oclc.org/10.1145/3037697.3037704},
doi = {10.1145/3037697.3037704},
abstract = {Processors and operating systems (OSes) support multiple memory page sizes. Superpages increase Translation Lookaside Buffer (TLB) hits, while small pages provide fine-grained memory protection. Ideally, TLBs should perform well for any distribution of page sizes. In reality, set-associative TLBs -- used frequently for their energy efficiency compared to fully-associative TLBs -- cannot (easily) support multiple page sizes concurrently. Instead, commercial systems typically implement separate set-associative TLBs for different page sizes. This means that when superpages are allocated aggressively, TLB misses may, counter intuitively, increase even if entries for small pages remain unused (and vice-versa). We invent MIX TLBs, energy-frugal set-associative structures that concurrently support all page sizes by exploiting superpage allocation patterns. MIX TLBs boost the performance (often by 10-30%) of big-memory applications on native CPUs, virtualized CPUs, and GPUs. MIX TLBs are simple and require no OS or program changes.},
booktitle = {Proceedings of the Twenty-Second International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {435–448},
numpages = {14},
keywords = {virtual memory, coalescing, superpages, tlb},
location = {Xi'an, China},
series = {ASPLOS '17}
}

@article{1968-multics,
author = {Daley, Robert C. and Dennis, Jack B.},
title = {Virtual Memory, Processes, and Sharing in MULTICS},
year = {1968},
issue_date = {May 1968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {5},
issn = {0001-0782},
url = {https://doi.org/10.1145/363095.363139},
doi = {10.1145/363095.363139},
journal = {Commun. ACM},
month = {may},
pages = {306–312},
numpages = {7},
keywords = {virtual memory, multiprogramming, data sharing, segmentation, dynamic linking, shared procedures, information sharing, file maintenance, paging, storage management, storage hierarchies}
}

@inproceedings{2022-asplos-caratcake,
author = {Suchy, Brian and Ghosh, Souradip and Kersnar, Drew and Chai, Siyuan and Huang, Zhen and Nelson, Aaron and Cuevas, Michael and Bernat, Alex and Chaudhary, Gaurav and Hardavellas, Nikos and Campanoni, Simone and Dinda, Peter},
title = {CARAT CAKE: Replacing Paging via Compiler/Kernel Cooperation},
year = {2022},
isbn = {9781450392051},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503222.3507771},
doi = {10.1145/3503222.3507771},
abstract = {Virtual memory, specifically paging, is undergoing significant innovation due to being challenged by new demands from modern workloads. Recent work has demonstrated an alternative software only design that can result in simplified hardware requirements, even supporting purely physical addressing. While we have made the case for this Compiler- And Runtime-based Address Translation (CARAT) concept, its evaluation was based on a user-level prototype. We now report on incorporating CARAT into a kernel, forming Compiler- And Runtime-based Address Translation for CollAborative Kernel Environments (CARAT CAKE). In our implementation, a Linux-compatible x64 process abstraction can be based either on CARAT CAKE, or on a sophisticated paging implementation. Implementing CARAT CAKE involves kernel changes and compiler optimizations/transformations that must work on all code in the system, including kernel code. We evaluate CARAT CAKE in comparison with paging and find that CARAT CAKE is able to achieve the functionality of paging (protection, mapping, and movement properties) with minimal overhead. In turn, CARAT CAKE allows significant new benefits for systems including energy savings, larger L1 caches, and arbitrary granularity memory management.},
booktitle = {Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {98–114},
numpages = {17},
keywords = {runtime, virtual memory, kernel, memory management},
location = {Lausanne, Switzerland},
series = {ASPLOS 2022}
}

@inproceedings{1993-sosp-sfi,
author = {Wahbe, Robert and Lucco, Steven and Anderson, Thomas E. and Graham, Susan L.},
title = {Efficient Software-Based Fault Isolation},
year = {1993},
isbn = {0897916328},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/168619.168635},
doi = {10.1145/168619.168635},
abstract = {One way to provide fault isolation among cooperating software modules is to place each in its own address space. However, for tightly-coupled modules, this solution incurs prohibitive context switch overhead. In this paper, we present a software approach to implementing fault isolation within a single address space.Our approach has two parts. First, we load the code and data for a distrusted module into its own fault do main, a logically separate portion of the application's address space. Second, we modify the object code of a distrusted module to prevent it from writing or jumping to an address outside its fault domain. Both these software operations are portable and programming language independent.Our approach poses a tradeoff relative to hardware fault isolation: substantially faster communication between fault domains, at a cost of slightly increased execution time for distrusted modules. We demonstrate that for frequently communicating modules, implementing fault isolation in software rather than hardware can substantially improve end-to-end application performance.},
booktitle = {Proceedings of the Fourteenth ACM Symposium on Operating Systems Principles},
pages = {203–216},
numpages = {14},
location = {Asheville, North Carolina, USA},
series = {SOSP '93}
}

@article{2013-tos-compiler-ctx-switch,
author = {Dolan, Stephen and Muralidharan, Servesh and Gregg, David},
title = {Compiler Support for Lightweight Context Switching},
year = {2013},
issue_date = {January 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
issn = {1544-3566},
url = {https://doi.org/10.1145/2400682.2400695},
doi = {10.1145/2400682.2400695},
abstract = {We propose a new language-neutral primitive for the LLVM compiler, which provides efficient context switching and message passing between lightweight threads of control. The primitive, called Swapstack, can be used by any language implementation based on LLVM to build higher-level language structures such as continuations, coroutines, and lightweight threads. As part of adding the primitives to LLVM, we have also added compiler support for passing parameters across context switches. Our modified LLVM compiler produces highly efficient code through a combination of exposing the context switching code to existing compiler optimizations, and adding novel compiler optimizations to further reduce the cost of context switches. To demonstrate the generality and efficiency of our primitives, we add one-shot continuations to C++, and provide a simple fiber library that allows millions of fibers to run on multiple cores, with a work-stealing scheduler and fast inter-fiber sychronization. We argue that compiler-supported lightweight context switching can be significantly faster than using a library to switch between contexts, and provide experimental evidence to support the position.},
journal = {ACM Trans. Archit. Code Optim.},
month = {jan},
articleno = {36},
numpages = {25},
keywords = {continuation, synchronization, Compiler, fiber}
}

@inproceedings {2018-osdi-dynamic-checkpoints,
author = {Kiwan Maeng and Brandon Lucia},
title = {Adaptive Dynamic Checkpointing for Safe Efficient Intermittent Computing},
booktitle = {13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18)},
year = {2018},
isbn = {978-1-939133-08-3},
address = {Carlsbad, CA},
pages = {129--144},
url = {https://www.usenix.org/conference/osdi18/presentation/maeng},
publisher = {USENIX Association},
month = oct,
}

@inproceedings {2019-atc-darwin-talk,
title = {Darwin: A Genomics Co-processor Provides up to 15,000X Acceleration on Long Read Assembly},
author = {Yatish Turakhia and Gill Bejerano and William J. Dally}, 
booktitle = {Invited talk at the 2019 USENIX Annual Technical Conference (USENIX ATC 19)},
year = {2019},
address = {Renton, WA},
url = {https://www.usenix.org/conference/atc19/presentation/turakhia},
publisher = {USENIX Association},
month = jul,
}

@article{2011-cacm-future-microprocessor,
author = {Borkar, Shekhar and Chien, Andrew A.},
title = {The Future of Microprocessors},
year = {2011},
issue_date = {May 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {5},
issn = {0001-0782},
url = {https://doi.org/10.1145/1941487.1941507},
doi = {10.1145/1941487.1941507},
abstract = {Energy efficiency is the new fundamental limiter of processor performance, way beyond numbers of processors.},
journal = {Commun. ACM},
month = {may},
pages = {67–77},
numpages = {11}
}

@inproceedings{2013-hotnets-disaggregation,
author = {Han, Sangjin and Egi, Norbert and Panda, Aurojit and Ratnasamy, Sylvia and Shi, Guangyu and Shenker, Scott},
title = {Network Support for Resource Disaggregation in Next-Generation Datacenters},
year = {2013},
isbn = {9781450325967},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2535771.2535778},
doi = {10.1145/2535771.2535778},
abstract = {Datacenters have traditionally been architected as a collection of servers wherein each server aggregates a fixed amount of computing, memory, storage, and communication resources. In this paper, we advocate an alternative construction in which the resources within a server are disaggregated and the datacenter is instead architected as a collection of standalone resources.Disaggregation brings greater modularity to datacenter infrastructure, allowing operators to optimize their deployments for improved efficiency and performance. However, the key enabling or blocking factor for disaggregation will be the network since communication that was previously contained within a single server now traverses the datacenter fabric. This paper thus explores the question of whether we can build networks that enable disaggregation at datacenter scales.},
booktitle = {Proceedings of the Twelfth ACM Workshop on Hot Topics in Networks},
articleno = {10},
numpages = {7},
location = {College Park, Maryland},
series = {HotNets-XII}
}

@inproceedings {2016-osdi-disaggregation,
author = {Peter X. Gao and Akshay Narayan and Sagar Karandikar and Joao Carreira and Sangjin Han and Rachit Agarwal and Sylvia Ratnasamy and Scott Shenker},
title = {Network Requirements for Resource Disaggregation},
booktitle = {12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16)},
year = {2016},
isbn = {978-1-931971-33-1},
address = {Savannah, GA},
pages = {249--264},
url = {https://www.usenix.org/conference/osdi16/technical-sessions/presentation/gao},
publisher = {USENIX Association},
month = nov,
}

@inproceedings{2019-sigcomm-ipipe,
author = {Liu, Ming and Cui, Tianyi and Schuh, Henry and Krishnamurthy, Arvind and Peter, Simon and Gupta, Karan},
title = {Offloading Distributed Applications onto SmartNICs Using IPipe},
year = {2019},
isbn = {9781450359566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341302.3342079},
doi = {10.1145/3341302.3342079},
abstract = {Emerging Multicore SoC SmartNICs, enclosing rich computing resources (e.g., a multicore processor, onboard DRAM, accelerators, programmable DMA engines), hold the potential to offload generic datacenter server tasks. However, it is unclear how to use a SmartNIC efficiently and maximize the offloading benefits, especially for distributed applications. Towards this end, we characterize four commodity SmartNICs and summarize the offloading performance implications from four perspectives: traffic control, computing capability, onboard memory, and host communication.Based on our characterization, we build iPipe, an actor-based framework for offloading distributed applications onto SmartNICs. At the core of iPipe is a hybrid scheduler, combining FCFS and DRR-based processor sharing, which can tolerate tasks with variable execution costs and maximize NIC compute utilization. Using iPipe, we build a real-time data analytics engine, a distributed transaction system, and a replicated key-value store, and evaluate them on commodity SmartNICs. Our evaluations show that when processing 10/25Gbps of application bandwidth, NIC-side offloading can save up to 3.1/2.2 beefy Intel cores and lower application latencies by 23.0/28.0 μs.},
booktitle = {Proceedings of the ACM Special Interest Group on Data Communication},
pages = {318–333},
numpages = {16},
keywords = {SmartNIC, distributed applications},
location = {Beijing, China},
series = {SIGCOMM '19}
}

@inproceedings{2022-asplos-clio,
author = {Guo, Zhiyuan and Shan, Yizhou and Luo, Xuhao and Huang, Yutong and Zhang, Yiying},
title = {Clio: A Hardware-Software Co-Designed Disaggregated Memory System},
year = {2022},
isbn = {9781450392051},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503222.3507762},
doi = {10.1145/3503222.3507762},
abstract = {Memory disaggregation has attracted great attention recently because of its benefits in efficient memory utilization and ease of management. So far, memory disaggregation research has all taken one of two approaches: building/emulating memory nodes using regular servers or building them using raw memory devices with no processing power. The former incurs higher monetary cost and faces tail latency and scalability limitations, while the latter introduces performance, security, and management problems. Server-based memory nodes and memory nodes with no processing power are two extreme approaches. We seek a sweet spot in the middle by proposing a hardware-based memory disaggregation solution that has the right amount of processing power at memory nodes. Furthermore, we take a clean-slate approach by starting from the requirements of memory disaggregation and designing a memory-disaggregation-native system. We built Clio, a disaggregated memory system that virtualizes, protects, and manages disaggregated memory at hardware-based memory nodes. The Clio hardware includes a new virtual memory system, a customized network system, and a framework for computation offloading. In building Clio, we not only co-design OS functionalities, hardware architecture, and the network system, but also co-design compute nodes and memory nodes. Our FPGA prototype of Clio demonstrates that each memory node can achieve 100&nbsp;Gbps throughput and an end-to-end latency of 2.5&nbsp;µ s at median and 3.2&nbsp;µ s at the 99th percentile. Clio also scales much better and has orders of magnitude lower tail latency than RDMA. It has 1.1\texttimes{} to 3.4\texttimes{} energy saving compared to CPU-based and SmartNIC-based disaggregated memory systems and is 2.7\texttimes{} faster than software-based SmartNIC solutions.},
booktitle = {Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {417–433},
numpages = {17},
keywords = {Virtual Memory, FPGA, Hardware- Software Co-design, Resource Disaggregation},
location = {Lausanne, Switzerland},
series = {ASPLOS 2022}
}

@inproceedings {2020-atc-passive-dissmem,
author = {Shin-Yeh Tsai and Yizhou Shan and Yiying Zhang},
title = {Disaggregating Persistent Memory and Controlling Them Remotely: An Exploration of Passive Disaggregated {Key-Value} Stores},
booktitle = {2020 USENIX Annual Technical Conference (USENIX ATC 20)},
year = {2020},
isbn = {978-1-939133-14-4},
pages = {33--48},
url = {https://www.usenix.org/conference/atc20/presentation/tsai},
publisher = {USENIX Association},
month = jul,
}

@inproceedings {2020-hotcloud-diss-apps,
author = {Sebastian Angel and Mihir Nanavati and Siddhartha Sen},
title = {Disaggregation and the Application},
booktitle = {12th USENIX Workshop on Hot Topics in Cloud Computing (HotCloud 20)},
year = {2020},
url = {https://www.usenix.org/conference/hotcloud20/presentation/angel},
publisher = {USENIX Association},
month = jul,
}

@inbook{2021-asplos-piberry,
author = {Calciu, Irina and Imran, M. Talha and Puddu, Ivan and Kashyap, Sanidhya and Maruf, Hasan Al and Mutlu, Onur and Kolli, Aasheesh},
title = {Rethinking Software Runtimes for Disaggregated Memory},
year = {2021},
isbn = {9781450383172},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3445814.3446713},
abstract = {Disaggregated memory can address resource provisioning inefficiencies in current datacenters. Multiple software runtimes for disaggregated memory have been proposed in an attempt to make disaggregated memory practical. These systems rely on the virtual memory subsystem to transparently offer disaggregated memory to applications using a local memory abstraction. Unfortunately, using virtual memory for disaggregation has multiple limitations, including high overhead that comes from the use of page faults to identify what data to fetch and cache locally, and high dirty data amplification that comes from the use of page-granularity for tracking changes to the cached data (4KB or higher).  In this paper, we propose a fundamentally new approach to designing software runtimes for disaggregated memory that addresses these limitations. Our main observation is that we can use cache coherence instead of virtual memory for tracking applications' memory accesses transparently, at cache-line granularity. This simple idea (1) eliminates page faults from the application critical path when accessing remote data, and (2) decouples the application memory access tracking from the virtual memory page size, enabling cache-line granularity dirty data tracking and eviction. Using this observation, we implemented a new software runtime for disaggregated memory that improves average memory access time by 1.7-5X and reduces dirty data amplification by 2-10X, compared to state-of-the-art systems.},
booktitle = {Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {79–92},
numpages = {14}
}

@inproceedings{2013-conext-rdma,
author = {\ulx{Animesh Trivedi} and Metzler, Bernard and Stuedi, Patrick and Gross, Thomas R.},
title = {On Limitations of Network Acceleration},
year = {2013},
isbn = {9781450321013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2535372.2535412},
doi = {10.1145/2535372.2535412},
abstract = {The performance of large-scale data-intensive applications running on thousands of machines depends considerably on the performance of the network. To deliver better application performance on rapidly evolving high-bandwidth, low-latency interconnects, researchers have proposed the use of network accelerator devices. However, despite the initial enthusiasm, translating network accelerator's capabilities into high application performance remains a challenging issue.In this paper, we describe our experience and discuss issues that we uncover with network acceleration using Remote Direct Memory Access (RDMA) capable network controllers (RNICs). RNICs offload the complete packet processing into network controllers, and provide direct userspace access to the networking hardware. Our analysis shows that multiple (un)related factors significantly influence the performance gains for the end-application. We identify factors that span the whole stack, ranging from low-level architectural issues (cache and DMA interaction, hardware pre-fetching) to the high-level application parameters (buffer size, access pattern). We discuss implications of our findings upon application performance and the future of integration of network acceleration technology within the systems.},
booktitle = {Proceedings of the Ninth ACM Conference on Emerging Networking Experiments and Technologies},
pages = {121–126},
numpages = {6},
keywords = {network acceleration, performance, cache coherence},
location = {Santa Barbara, California, USA},
series = {CoNEXT '13}
}

@INPROCEEDINGS{2019-ieee-fpga-energy, author={Qasaimeh, Murad and Denolf, Kristof and Lo, Jack and Vissers, Kees and Zambreno, Joseph and Jones, Phillip H.},  booktitle={2019 IEEE International Conference on Embedded Software and Systems (ICESS)},   title={Comparing Energy Efficiency of CPU, GPU and FPGA Implementations for Vision Kernels},   year={2019},  volume={},  number={},  pages={1-8},  doi={10.1109/ICESS.2019.8782524}}

@article{2022-the-seattle-report,
author = {Abadi, Daniel and Ailamaki, Anastasia and Andersen, David and Bailis, Peter and Balazinska, Magdalena and Bernstein, Philip A. and Boncz, Peter and Chaudhuri, Surajit and Cheung, Alvin and Doan, Anhai and Dong, Luna and Franklin, Michael J. and Freire, Juliana and Halevy, Alon and Hellerstein, Joseph M. and Idreos, Stratos and Kossmann, Donald and Kraska, Tim and Krishnamurthy, Sailesh and Markl, Volker and Melnik, Sergey and Milo, Tova and Mohan, C. and Neumann, Thomas and Ooi, Beng Chin and Ozcan, Fatma and Patel, Jignesh and Pavlo, Andrew and Popa, Raluca and Ramakrishnan, Raghu and Re, Christopher and Stonebraker, Michael and Suciu, Dan},
title = {The Seattle Report on Database Research},
year = {2022},
issue_date = {August 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {65},
number = {8},
issn = {0001-0782},
url = {https://doi.org/10.1145/3524284},
doi = {10.1145/3524284},
abstract = {Every five years, a group of the leading database researchers meet to reflect on their community's impact on the computing industry as well as examine current research challenges.},
journal = {Commun. ACM},
month = {jul},
pages = {72–79},
numpages = {8}
}

@article{2020-csur-scalable-deep-learning,
author = {Mayer, Ruben and Jacobsen, Hans-Arno},
title = {Scalable Deep Learning on Distributed Infrastructures: Challenges, Techniques, and Tools},
year = {2020},
issue_date = {January 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3363554},
doi = {10.1145/3363554},
journal = {ACM Comput. Surv.},
month = {feb},
articleno = {3},
numpages = {37},
keywords = {Deep-learning systems}
}

@inproceedings {2020-osdi-micro-consensus,
author = {Marcos K. Aguilera and Naama Ben-David and Rachid Guerraoui and Virendra J. Marathe and Athanasios Xygkis and Igor Zablotchi},
title = {Microsecond Consensus for Microsecond Applications},
booktitle = {14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20)},
year = {2020},
isbn = {978-1-939133-19-9},
pages = {599--616},
url = {https://www.usenix.org/conference/osdi20/presentation/aguilera},
publisher = {USENIX Association},
month = nov,
}

@inproceedings {2014-osdi-ix,
author = {Adam Belay and George Prekas and Ana Klimovic and Samuel Grossman and Christos Kozyrakis and Edouard Bugnion},
title = {{IX}: A Protected Dataplane Operating System for High Throughput and Low Latency},
booktitle = {11th USENIX Symposium on Operating Systems Design and Implementation (OSDI 14)},
year = {2014},
isbn = { 978-1-931971-16-4},
address = {Broomfield, CO},
pages = {49--65},
url = {https://www.usenix.org/conference/osdi14/technical-sessions/presentation/belay},
publisher = {USENIX Association},
month = oct,
}

@inproceedings{2022-ics-gpu-alignment,
author = {M\"{u}ller, Andr\'{e} and Schmidt, Bertil and Membarth, Richard and Lei\ss{}a, Roland and Hack, Sebastian},
title = {AnySeq/GPU: A Novel Approach for Faster Sequence Alignment on GPUs},
year = {2022},
isbn = {9781450392815},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524059.3532376},
doi = {10.1145/3524059.3532376},
abstract = {In recent years, the rapidly increasing number of reads produced by next-generation sequencing (NGS) technologies has driven the demand for efficient implementations of sequence alignments in bioinformatics. However, current state-of-the-art approaches are not able to leverage the massively parallel processing capabilities of modern GPUs with close-to-peak performance.We present AnySeq/GPU---a sequence alignment library that augments the AnySeq 1 library with a novel approach for accelerating dynamic programming (DP) alignment on GPUs by minimizing memory accesses using warp shuffles and half-precision arithmetic. Our implementation is based on the AnyDSL compiler framework which allows for convenient zero-cost abstractions through guaranteed partial evaluation. We show that our approach achieves over 80% of the peak performance on both NVIDIA and AMD GPUs thereby outperforming the GPU-based alignment libraries Any-Seq 1, GASAL 2, ADEPT, and NVBIO by a factor of at least 3.6 while achieving a median speedup of 19.2X over these tools across different alignment scenarios and sequence lengths when running on the same hardware.This leads to throughputs of up to 1.7 TCUPS (tera cell updates per second) on an NVIDIA GV100, up to 3.3 TCUPS with half-precision arithmetic on a single NVIDIA A100, and up to 3.8 TCUPS on an AMD MI100. AnySeq/GPU is publicly available at https://github.com/AnyDSL/anyseq.},
booktitle = {Proceedings of the 36th ACM International Conference on Supercomputing},
articleno = {20},
numpages = {11},
location = {Virtual Event},
series = {ICS '22}
}
